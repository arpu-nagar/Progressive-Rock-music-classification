{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHvW34DvirxR"
      },
      "source": [
        "# Download train data and unzip convert back to torch tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPzO8AkfLdqq",
        "outputId": "e58e7846-eb82-4a8f-ff31-ded5e7463358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gdown\n",
            "  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting beautifulsoup4 (from gdown)\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: filelock in /home/arpan/miniconda3/envs/prog/lib/python3.10/site-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /home/arpan/miniconda3/envs/prog/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /home/arpan/miniconda3/envs/prog/lib/python3.10/site-packages (from gdown) (4.66.1)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
            "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/arpan/miniconda3/envs/prog/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/arpan/miniconda3/envs/prog/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arpan/miniconda3/envs/prog/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/arpan/miniconda3/envs/prog/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
            "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: soupsieve, PySocks, beautifulsoup4, gdown\n",
            "Successfully installed PySocks-1.7.1 beautifulsoup4-4.12.3 gdown-5.1.0 soupsieve-2.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hppq96UeMLrm",
        "outputId": "e24a17c6-220b-4f42-8aa4-22d7b04a4a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/arpan/miniconda3/envs/prog/lib/python3.10/site-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1kGWf1Q_bi3SXzOZ822KtA6dh0CzgXvZ8\n",
            "From (redirected): https://drive.google.com/uc?id=1kGWf1Q_bi3SXzOZ822KtA6dh0CzgXvZ8&confirm=t&uuid=71f2bd6e-72dd-47f6-aca1-a4b0a8d46432\n",
            "To: /home/arpan/prog-rock/both_tensors_160_216.zip\n",
            "100%|██████████████████████████████████████| 1.99G/1.99G [00:24<00:00, 82.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1kGWf1Q_bi3SXzOZ822KtA6dh0CzgXvZ8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLyqbSG9MfN2",
        "outputId": "7c7aa879-2c8b-40e2-f026-e402afa25e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  ./content/both_tensors_160_216.zip\n",
            "  inflating: ./content/non_progressive_rock_songs_tensor.pt  \n",
            "  inflating: ./content/progressive_rock_songs_tensor.pt  \n"
          ]
        }
      ],
      "source": [
        "!unzip -j ./content/both_tensors_160_216.zip -d ./content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L07E1_ukNez7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " CUDA Version is  cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "\n",
        "#training tensors\n",
        "prog_tensors = torch.load('./content/progressive_rock_songs_tensor.pt')\n",
        "non_prog_tensors = torch.load('./content/non_progressive_rock_songs_tensor.pt')\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda:0'\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    return device\n",
        "print(\" CUDA Version is \", get_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FJMqG7F3gsQT"
      },
      "outputs": [],
      "source": [
        "# prompt: non_prog_tensors reshaped to [8052, 160, 216])\n",
        "# originally (X, 216, 160)\n",
        "non_prog_tensors = non_prog_tensors.reshape(8052, 160, 216)\n",
        "prog_tensors = prog_tensors.reshape(7455, 160, 216)\n",
        "prog_tensors = prog_tensors.float()\n",
        "non_prog_tensors = non_prog_tensors.float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S_vHwRqi04D"
      },
      "source": [
        "# Download validation data and mess around with it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWMKO0E5Us7m",
        "outputId": "1140a737-9f33-45ec-d2f0-22114d700f33"
      },
      "outputs": [],
      "source": [
        "# https://drive.google.com/file/d/1--QAwB8w8GVWZ1wmpPOPaj6jNkGWDI3c/view?usp=sharing\n",
        "!gdown --id 1--QAwB8w8GVWZ1wmpPOPaj6jNkGWDI3c\n",
        "!unzip ./content/both_tensors_160_216_validation_1.zip\n",
        "import torch\n",
        "prog_tensors_validation = torch.load('./content/progressive_rock_songs_validation_tensor.pt')\n",
        "non_prog_tensors_validation = torch.load('./content/non_progressive_rock_songs_validation_tensor.pt')\n",
        "## NOTE: the above is an array not a tensor!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPGD-qKROjWz"
      },
      "source": [
        "# running some ML and ensemble algorithms from sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDu2fczZOpAS",
        "outputId": "5c58e9c8-1431-471a-c442-7d6fcaca4374"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15507, 34560)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prompt: i have a prog_tensors torch tensor of shape (7455, 160, 216) convert to 2d array of shape (7455, 160*216)\n",
        "## Converting prog tensor to numpy array for sklearn use\n",
        "import numpy as np\n",
        "prog_2d_array = prog_tensors.reshape(7455, 160 * 216)\n",
        "prog_2d_array = prog_2d_array.detach().numpy()\n",
        "\n",
        "non_prog_2d_array = non_prog_tensors.reshape(8052, 160 * 216)\n",
        "non_prog_2d_array = non_prog_2d_array.detach().numpy()\n",
        "labels = torch.cat((torch.ones(prog_tensors.shape[0]), torch.zeros(non_prog_tensors.shape[0])), dim=0)\n",
        "\n",
        "# Convert the labels tensor to a NumPy array\n",
        "y_data = labels.detach().numpy()\n",
        "X_data = np.concatenate((prog_2d_array, non_prog_2d_array), axis=0)\n",
        "X_data.shape\n",
        "\n",
        "# (15507, 34560) -> (instances, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKEe4EGGjXFH"
      },
      "source": [
        "# Random forest classifier on the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "91sFXGPxR1gN",
        "outputId": "3dde8d8c-4c69-47c6-f81f-75365092e7f5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=101)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=101)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier(n_estimators=101)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# xgboost, bagging, decision trees\n",
        "model = RandomForestClassifier(n_estimators=101)\n",
        "model.fit(X_data, y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcNWkaQoT-9Q",
        "outputId": "5dcd6216-e31b-4a41-8b1e-890b4b246643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(n_estimators=101)\n",
            "Feature ranking:\n",
            "1. feature 28321 (0.000670)\n",
            "2. feature 7362 (0.000650)\n",
            "3. feature 7201 (0.000641)\n",
            "4. feature 27361 (0.000624)\n",
            "5. feature 7202 (0.000604)\n",
            "6. feature 19361 (0.000553)\n",
            "7. feature 28481 (0.000545)\n",
            "8. feature 30241 (0.000530)\n",
            "9. feature 12001 (0.000519)\n",
            "10. feature 28322 (0.000487)\n"
          ]
        }
      ],
      "source": [
        "print(model)\n",
        "importances = model.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(10):\n",
        "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAJBrsHGUfEa",
        "outputId": "8208ef2f-6eee-4148-d0ae-f96d0f8d4f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29 1\n",
            "3 56\n",
            "0.9550561797752809 0.9666666666666667 0.90625\n"
          ]
        }
      ],
      "source": [
        "prog_true = 0\n",
        "prog_false = 0\n",
        "non_prog_true = 0\n",
        "non_prog_false = 0\n",
        "for obj in prog_tensors_validation: # obj collection of 10 sec snippets for each song in the validation set 1d array\n",
        "  arr = []\n",
        "  cnt = 0\n",
        "  for i in obj:\n",
        "    # print(i.shape)\n",
        "    j = i.reshape(1, 160 * 216)\n",
        "    j = j.detach().numpy()\n",
        "    prediction = model.predict(j)\n",
        "    # arr.append(j)\n",
        "    # print(prediction[0])\n",
        "    if(prediction[0] == 1.0):\n",
        "      cnt = cnt+1\n",
        "      # 100 snippetts\n",
        "      # cnt = 60\n",
        "  if(2*cnt >= len(obj)):\n",
        "    prog_true+=1\n",
        "  else:\n",
        "    prog_false+=1\n",
        "\n",
        "for obj in non_prog_tensors_validation:\n",
        "  arr = []\n",
        "  cnt = 0\n",
        "  for i in obj:\n",
        "    # print(i.shape)\n",
        "    j = i.reshape(1, 160 * 216)\n",
        "    j = j.detach().numpy()\n",
        "    prediction = model.predict(j)\n",
        "    # arr.append(j)\n",
        "    # print(prediction[0])\n",
        "    if(prediction[0] == 0):\n",
        "      cnt = cnt+1\n",
        "  if(2*cnt >= len(obj)):\n",
        "    non_prog_true+=1\n",
        "  else:\n",
        "    non_prog_false+=1\n",
        "\n",
        "print(prog_true, prog_false)\n",
        "print(non_prog_false,non_prog_true)\n",
        "  # prediction = model.predict([arr])\n",
        "accuracy = (prog_true + non_prog_true) / (prog_true + prog_false + non_prog_true + non_prog_false)\n",
        "precision = prog_true / (prog_true + prog_false)\n",
        "recall = prog_true / (prog_true + non_prog_false)\n",
        "print(accuracy, precision, recall)\n",
        "\n",
        "# pretty good idk how"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Pnzw8Yjjz_"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN5kf_eijmd_"
      },
      "outputs": [],
      "source": [
        "# use X_data, y_data and create an XGBoost classifier\n",
        "## crashed so maybe use PCA and reduce dimensions and then try classifying\n",
        "import xgboost as xgb\n",
        "\n",
        "# Create an XGBoost classifier\n",
        "model_xgb = xgb.XGBClassifier()\n",
        "\n",
        "# Train the model\n",
        "model_xgb.fit(X_data, y_data)\n",
        "\n",
        "# Print the model\n",
        "print(model_xgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amf1ryiJOpm6"
      },
      "source": [
        "# Deep Learning starts here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J1IKd2fNOdxb"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Concatenate the tensors and create labels\n",
        "data = torch.cat((prog_tensors, non_prog_tensors), dim=0)\n",
        "labels = torch.cat((torch.ones(prog_tensors.shape[0]), torch.zeros(non_prog_tensors.shape[0])), dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-15TtZU7P7uh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([15507, 216, 160])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 32  # Define your desired batch size\n",
        "data.shape\n",
        "# (instances, timesteps, features) [15507, 216, 160]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eNMM6ub5VBGo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDataset from your data and labels\n",
        "train_dataset = TensorDataset(data, labels)\n",
        "# val_dataset = TensorDataset(val_data, val_labels)\n",
        "# test_dataset = TensorDataset(test_data, test_labels)\n",
        "\n",
        "# Create DataLoader for each dataset with the specified batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K2UiIFw8VEBf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyConvNet(\n",
            "  (conv1): Conv1d(160, 320, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (batchnorm1): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv1d(320, 640, kernel_size=(5,), stride=(1,), padding=(1,))\n",
            "  (batchnorm2): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv1d(640, 256, kernel_size=(5,), stride=(1,), padding=(1,))\n",
            "  (batchnorm3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (batchnorm4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=27136, out_features=200, bias=True)\n",
            "  (fc2): Linear(in_features=200, out_features=20, bias=True)\n",
            "  (fc3): Linear(in_features=20, out_features=2, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# nn.Conv1d expects a batched 3-dimensional input in the shape [batch_size, in_channels, seq_length] or an unbatched 2-dimensional input in the shape [in_channels, seq_length]\n",
        "class MyConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyConvNet, self).__init__()\n",
        "\n",
        "        # Define convolutional layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=160, out_channels=320, kernel_size=3, stride=1, padding=1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(320)\n",
        "        self.conv2 = nn.Conv1d(in_channels=320, out_channels=640, kernel_size=5, stride=1, padding=1)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(640)\n",
        "        self.conv3 = nn.Conv1d(in_channels=640, out_channels=256, kernel_size=5, stride=1, padding=1)\n",
        "        self.batchnorm3 = nn.BatchNorm1d(256)\n",
        "        self.conv4 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.batchnorm4 = nn.BatchNorm1d(128)\n",
        "\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(212*128, 200)  \n",
        "        self.fc2 = nn.Linear(200, 20)\n",
        "        self.fc3 = nn.Linear(20, 2)\n",
        "\n",
        "        # Define activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers with ReLU activation\n",
        "        x = self.relu(self.batchnorm1(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.batchnorm2(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.batchnorm3(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.batchnorm4(self.conv4(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Flatten the output of the convolutional layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers with ReLU activation\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = MyConvNet()\n",
        "print(model)\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer (e.g., Adam optimizer with learning rate 0.001)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "INOn4Qbzd3Ko"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyConvNet(\n",
            "  (conv1): Conv1d(160, 320, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (batchnorm1): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv1d(320, 640, kernel_size=(5,), stride=(1,), padding=(1,))\n",
            "  (batchnorm2): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv1d(640, 256, kernel_size=(5,), stride=(1,), padding=(1,))\n",
            "  (batchnorm3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (batchnorm4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=27136, out_features=200, bias=True)\n",
            "  (fc2): Linear(in_features=200, out_features=20, bias=True)\n",
            "  (fc3): Linear(in_features=20, out_features=2, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "Epoch: 0, Batch: 1, Loss: 0.6855, Accuracy: 54.69%, Precision: 60.61%\n",
            "Epoch: 0, Batch: 2, Loss: 0.9159, Accuracy: 54.69%, Precision: 63.16%\n",
            "Epoch: 0, Batch: 3, Loss: 2.4419, Accuracy: 55.73%, Precision: 59.80%\n",
            "Epoch: 0, Batch: 4, Loss: 1.1541, Accuracy: 55.08%, Precision: 57.23%\n",
            "Epoch: 0, Batch: 5, Loss: 0.6493, Accuracy: 55.00%, Precision: 56.73%\n",
            "Epoch: 0, Batch: 6, Loss: 0.6436, Accuracy: 55.99%, Precision: 56.73%\n",
            "Epoch: 0, Batch: 7, Loss: 0.7767, Accuracy: 56.47%, Precision: 56.73%\n",
            "Epoch: 0, Batch: 8, Loss: 0.6281, Accuracy: 56.25%, Precision: 56.73%\n",
            "Epoch: 0, Batch: 9, Loss: 0.7430, Accuracy: 56.42%, Precision: 57.14%\n",
            "Epoch: 0, Batch: 10, Loss: 0.6851, Accuracy: 56.09%, Precision: 55.43%\n",
            "Epoch: 0, Batch: 11, Loss: 0.9416, Accuracy: 55.26%, Precision: 53.47%\n",
            "Epoch: 0, Batch: 12, Loss: 0.7757, Accuracy: 55.21%, Precision: 53.70%\n",
            "Epoch: 0, Batch: 13, Loss: 0.7352, Accuracy: 53.85%, Precision: 52.60%\n",
            "Epoch: 0, Batch: 14, Loss: 0.6506, Accuracy: 54.02%, Precision: 52.37%\n",
            "Epoch: 0, Batch: 15, Loss: 0.7546, Accuracy: 53.54%, Precision: 51.75%\n",
            "Epoch: 0, Batch: 16, Loss: 0.6407, Accuracy: 53.81%, Precision: 51.85%\n",
            "Epoch: 0, Batch: 17, Loss: 0.7267, Accuracy: 53.95%, Precision: 52.60%\n",
            "Epoch: 0, Batch: 18, Loss: 0.6497, Accuracy: 54.34%, Precision: 53.01%\n",
            "Epoch: 0, Batch: 19, Loss: 0.6691, Accuracy: 54.11%, Precision: 52.66%\n",
            "Epoch: 0, Batch: 20, Loss: 0.6608, Accuracy: 54.22%, Precision: 52.89%\n",
            "Epoch: 0, Batch: 21, Loss: 0.6812, Accuracy: 54.39%, Precision: 53.03%\n",
            "Epoch: 0, Batch: 22, Loss: 0.6491, Accuracy: 54.62%, Precision: 53.12%\n",
            "Epoch: 0, Batch: 23, Loss: 0.6252, Accuracy: 55.23%, Precision: 53.78%\n",
            "Epoch: 0, Batch: 24, Loss: 0.6848, Accuracy: 55.27%, Precision: 54.10%\n",
            "Epoch: 0, Batch: 25, Loss: 0.6496, Accuracy: 55.25%, Precision: 54.05%\n",
            "Epoch: 0, Batch: 26, Loss: 0.6256, Accuracy: 55.65%, Precision: 54.52%\n",
            "Epoch: 0, Batch: 27, Loss: 0.6605, Accuracy: 55.79%, Precision: 54.63%\n",
            "Epoch: 0, Batch: 28, Loss: 0.6421, Accuracy: 55.86%, Precision: 54.62%\n",
            "Epoch: 0, Batch: 29, Loss: 0.7118, Accuracy: 55.87%, Precision: 54.75%\n",
            "Epoch: 0, Batch: 30, Loss: 0.6730, Accuracy: 55.99%, Precision: 54.97%\n",
            "Epoch: 0, Batch: 31, Loss: 0.6316, Accuracy: 56.30%, Precision: 55.43%\n",
            "Epoch: 0, Batch: 32, Loss: 0.6753, Accuracy: 56.20%, Precision: 55.18%\n",
            "Epoch: 0, Batch: 33, Loss: 0.6704, Accuracy: 56.25%, Precision: 55.07%\n",
            "Epoch: 0, Batch: 34, Loss: 0.6851, Accuracy: 56.02%, Precision: 54.53%\n",
            "Epoch: 0, Batch: 35, Loss: 0.6604, Accuracy: 55.89%, Precision: 54.56%\n",
            "Epoch: 0, Batch: 36, Loss: 0.6734, Accuracy: 55.64%, Precision: 54.56%\n",
            "Epoch: 0, Batch: 37, Loss: 0.6338, Accuracy: 55.79%, Precision: 54.55%\n",
            "Epoch: 0, Batch: 38, Loss: 0.6621, Accuracy: 55.67%, Precision: 54.59%\n",
            "Epoch: 0, Batch: 39, Loss: 0.6817, Accuracy: 55.89%, Precision: 55.03%\n",
            "Epoch: 0, Batch: 40, Loss: 0.6645, Accuracy: 55.82%, Precision: 54.61%\n",
            "Epoch: 0, Batch: 41, Loss: 0.6466, Accuracy: 55.75%, Precision: 54.23%\n",
            "Epoch: 0, Batch: 42, Loss: 0.6623, Accuracy: 55.88%, Precision: 54.25%\n",
            "Epoch: 0, Batch: 43, Loss: 0.6323, Accuracy: 55.78%, Precision: 54.09%\n",
            "Epoch: 0, Batch: 44, Loss: 0.7145, Accuracy: 55.65%, Precision: 54.11%\n",
            "Epoch: 0, Batch: 45, Loss: 0.6340, Accuracy: 55.80%, Precision: 54.20%\n",
            "Epoch: 0, Batch: 46, Loss: 0.5957, Accuracy: 55.81%, Precision: 54.21%\n",
            "Epoch: 0, Batch: 47, Loss: 0.6117, Accuracy: 56.12%, Precision: 54.51%\n",
            "Epoch: 0, Batch: 48, Loss: 0.6854, Accuracy: 55.99%, Precision: 54.37%\n",
            "Epoch: 0, Batch: 49, Loss: 0.6733, Accuracy: 56.12%, Precision: 54.67%\n",
            "Epoch: 0, Batch: 50, Loss: 0.6825, Accuracy: 56.09%, Precision: 54.49%\n",
            "Epoch: 0, Batch: 51, Loss: 0.7113, Accuracy: 55.94%, Precision: 54.21%\n",
            "Epoch: 0, Batch: 52, Loss: 0.6590, Accuracy: 55.92%, Precision: 54.13%\n",
            "Epoch: 0, Batch: 53, Loss: 0.6624, Accuracy: 55.98%, Precision: 54.26%\n",
            "Epoch: 0, Batch: 54, Loss: 0.6735, Accuracy: 55.79%, Precision: 54.02%\n",
            "Epoch: 0, Batch: 55, Loss: 0.6170, Accuracy: 55.74%, Precision: 54.01%\n",
            "Epoch: 0, Batch: 56, Loss: 0.6869, Accuracy: 55.64%, Precision: 53.97%\n",
            "Epoch: 0, Batch: 57, Loss: 0.6237, Accuracy: 55.56%, Precision: 53.97%\n",
            "Epoch: 0, Batch: 58, Loss: 0.6291, Accuracy: 55.60%, Precision: 53.89%\n",
            "Epoch: 0, Batch: 59, Loss: 0.6961, Accuracy: 55.69%, Precision: 53.95%\n",
            "Epoch: 0, Batch: 60, Loss: 0.6522, Accuracy: 55.83%, Precision: 54.18%\n",
            "Epoch: 0, Batch: 61, Loss: 0.6547, Accuracy: 55.87%, Precision: 54.24%\n",
            "Epoch: 0, Batch: 62, Loss: 0.7262, Accuracy: 55.92%, Precision: 54.41%\n",
            "Epoch: 0, Batch: 63, Loss: 0.6838, Accuracy: 55.95%, Precision: 54.41%\n",
            "Epoch: 0, Batch: 64, Loss: 0.6887, Accuracy: 55.98%, Precision: 54.39%\n",
            "Epoch: 0, Batch: 65, Loss: 0.7097, Accuracy: 55.87%, Precision: 54.11%\n",
            "Epoch: 0, Batch: 66, Loss: 0.6091, Accuracy: 55.94%, Precision: 54.17%\n",
            "Epoch: 0, Batch: 67, Loss: 0.6684, Accuracy: 56.02%, Precision: 54.36%\n",
            "Epoch: 0, Batch: 68, Loss: 0.6972, Accuracy: 55.88%, Precision: 54.35%\n",
            "Epoch: 0, Batch: 69, Loss: 0.6272, Accuracy: 55.98%, Precision: 54.40%\n",
            "Epoch: 0, Batch: 70, Loss: 0.6867, Accuracy: 55.83%, Precision: 54.40%\n",
            "Epoch: 0, Batch: 71, Loss: 0.6341, Accuracy: 55.94%, Precision: 54.47%\n",
            "Epoch: 0, Batch: 72, Loss: 0.6239, Accuracy: 55.97%, Precision: 54.41%\n",
            "Epoch: 0, Batch: 73, Loss: 0.6430, Accuracy: 55.97%, Precision: 54.40%\n",
            "Epoch: 0, Batch: 74, Loss: 0.6358, Accuracy: 56.14%, Precision: 54.59%\n",
            "Epoch: 0, Batch: 75, Loss: 0.5963, Accuracy: 56.29%, Precision: 54.79%\n",
            "Epoch: 0, Batch: 76, Loss: 0.6527, Accuracy: 56.25%, Precision: 54.64%\n",
            "Epoch: 0, Batch: 77, Loss: 0.6401, Accuracy: 56.27%, Precision: 54.56%\n",
            "Epoch: 0, Batch: 78, Loss: 0.6319, Accuracy: 56.33%, Precision: 54.60%\n",
            "Epoch: 0, Batch: 79, Loss: 0.6757, Accuracy: 56.51%, Precision: 54.84%\n",
            "Epoch: 0, Batch: 80, Loss: 0.6431, Accuracy: 56.58%, Precision: 54.90%\n",
            "Epoch: 0, Batch: 81, Loss: 0.5835, Accuracy: 56.66%, Precision: 55.02%\n",
            "Epoch: 0, Batch: 82, Loss: 0.6249, Accuracy: 56.78%, Precision: 55.20%\n",
            "Epoch: 0, Batch: 83, Loss: 0.6712, Accuracy: 56.76%, Precision: 55.12%\n",
            "Epoch: 0, Batch: 84, Loss: 0.6176, Accuracy: 56.85%, Precision: 55.15%\n",
            "Epoch: 0, Batch: 85, Loss: 0.6254, Accuracy: 56.93%, Precision: 55.22%\n",
            "Epoch: 0, Batch: 86, Loss: 0.6830, Accuracy: 56.89%, Precision: 55.20%\n",
            "Epoch: 0, Batch: 87, Loss: 0.6038, Accuracy: 56.95%, Precision: 55.30%\n",
            "Epoch: 0, Batch: 88, Loss: 0.6147, Accuracy: 57.07%, Precision: 55.38%\n",
            "Epoch: 0, Batch: 89, Loss: 0.6758, Accuracy: 57.08%, Precision: 55.47%\n",
            "Epoch: 0, Batch: 90, Loss: 0.7101, Accuracy: 57.03%, Precision: 55.36%\n",
            "Epoch: 0, Batch: 91, Loss: 0.6038, Accuracy: 57.14%, Precision: 55.51%\n",
            "Epoch: 0, Batch: 92, Loss: 0.5948, Accuracy: 57.27%, Precision: 55.59%\n",
            "Epoch: 0, Batch: 93, Loss: 0.6120, Accuracy: 57.39%, Precision: 55.68%\n",
            "Epoch: 0, Batch: 94, Loss: 0.6320, Accuracy: 57.43%, Precision: 55.77%\n",
            "Epoch: 0, Batch: 95, Loss: 0.6407, Accuracy: 57.43%, Precision: 55.62%\n",
            "Epoch: 0, Batch: 96, Loss: 0.6608, Accuracy: 57.39%, Precision: 55.59%\n",
            "Epoch: 0, Batch: 97, Loss: 0.5903, Accuracy: 57.41%, Precision: 55.64%\n",
            "Epoch: 0, Batch: 98, Loss: 0.6093, Accuracy: 57.43%, Precision: 55.68%\n",
            "Epoch: 0, Batch: 99, Loss: 0.5736, Accuracy: 57.50%, Precision: 55.69%\n",
            "Epoch: 0, Batch: 100, Loss: 0.6444, Accuracy: 57.55%, Precision: 55.74%\n",
            "Epoch: 0, Batch: 101, Loss: 0.5433, Accuracy: 57.70%, Precision: 55.94%\n",
            "Epoch: 0, Batch: 102, Loss: 0.6033, Accuracy: 57.78%, Precision: 55.95%\n",
            "Epoch: 0, Batch: 103, Loss: 0.5787, Accuracy: 57.84%, Precision: 55.99%\n",
            "Epoch: 0, Batch: 104, Loss: 0.6521, Accuracy: 57.90%, Precision: 55.99%\n",
            "Epoch: 0, Batch: 105, Loss: 0.4953, Accuracy: 58.14%, Precision: 56.16%\n",
            "Epoch: 0, Batch: 106, Loss: 0.5317, Accuracy: 58.27%, Precision: 56.26%\n",
            "Epoch: 0, Batch: 107, Loss: 0.6368, Accuracy: 58.38%, Precision: 56.43%\n",
            "Epoch: 0, Batch: 108, Loss: 0.6490, Accuracy: 58.46%, Precision: 56.56%\n",
            "Epoch: 0, Batch: 109, Loss: 0.6236, Accuracy: 58.56%, Precision: 56.69%\n",
            "Epoch: 0, Batch: 110, Loss: 0.6751, Accuracy: 58.55%, Precision: 56.60%\n",
            "Epoch: 0, Batch: 111, Loss: 0.6523, Accuracy: 58.59%, Precision: 56.59%\n",
            "Epoch: 0, Batch: 112, Loss: 0.6579, Accuracy: 58.62%, Precision: 56.63%\n",
            "Epoch: 0, Batch: 113, Loss: 0.6251, Accuracy: 58.63%, Precision: 56.64%\n",
            "Epoch: 0, Batch: 114, Loss: 0.5816, Accuracy: 58.73%, Precision: 56.68%\n",
            "Epoch: 0, Batch: 115, Loss: 0.6609, Accuracy: 58.72%, Precision: 56.80%\n",
            "Epoch: 0, Batch: 116, Loss: 0.6339, Accuracy: 58.70%, Precision: 56.74%\n",
            "Epoch: 0, Batch: 117, Loss: 0.6022, Accuracy: 58.79%, Precision: 56.81%\n",
            "Epoch: 0, Batch: 118, Loss: 0.6446, Accuracy: 58.82%, Precision: 56.81%\n",
            "Epoch: 0, Batch: 119, Loss: 0.6531, Accuracy: 58.92%, Precision: 56.96%\n",
            "Epoch: 0, Batch: 120, Loss: 0.5678, Accuracy: 59.01%, Precision: 57.05%\n",
            "Epoch: 0, Batch: 121, Loss: 0.6606, Accuracy: 58.95%, Precision: 56.93%\n",
            "Epoch: 0, Batch: 122, Loss: 0.6356, Accuracy: 58.99%, Precision: 56.95%\n",
            "Epoch: 0, Batch: 123, Loss: 0.5601, Accuracy: 59.07%, Precision: 57.02%\n",
            "Epoch: 0, Batch: 124, Loss: 0.6732, Accuracy: 59.07%, Precision: 57.01%\n",
            "Epoch: 0, Batch: 125, Loss: 0.5634, Accuracy: 59.19%, Precision: 57.14%\n",
            "Epoch: 0, Batch: 126, Loss: 0.5723, Accuracy: 59.28%, Precision: 57.24%\n",
            "Epoch: 0, Batch: 127, Loss: 0.6184, Accuracy: 59.33%, Precision: 57.29%\n",
            "Epoch: 0, Batch: 128, Loss: 0.6555, Accuracy: 59.30%, Precision: 57.29%\n",
            "Epoch: 0, Batch: 129, Loss: 0.6612, Accuracy: 59.29%, Precision: 57.20%\n",
            "Epoch: 0, Batch: 130, Loss: 0.5608, Accuracy: 59.39%, Precision: 57.25%\n",
            "Epoch: 0, Batch: 131, Loss: 0.6313, Accuracy: 59.39%, Precision: 57.30%\n",
            "Epoch: 0, Batch: 132, Loss: 0.6218, Accuracy: 59.41%, Precision: 57.41%\n",
            "Epoch: 0, Batch: 133, Loss: 0.6370, Accuracy: 59.45%, Precision: 57.49%\n",
            "Epoch: 0, Batch: 134, Loss: 0.6087, Accuracy: 59.47%, Precision: 57.42%\n",
            "Epoch: 0, Batch: 135, Loss: 0.6240, Accuracy: 59.47%, Precision: 57.38%\n",
            "Epoch: 0, Batch: 136, Loss: 0.6202, Accuracy: 59.51%, Precision: 57.40%\n",
            "Epoch: 0, Batch: 137, Loss: 0.6144, Accuracy: 59.57%, Precision: 57.45%\n",
            "Epoch: 0, Batch: 138, Loss: 0.6027, Accuracy: 59.61%, Precision: 57.49%\n",
            "Epoch: 0, Batch: 139, Loss: 0.5408, Accuracy: 59.68%, Precision: 57.48%\n",
            "Epoch: 0, Batch: 140, Loss: 0.5117, Accuracy: 59.79%, Precision: 57.57%\n",
            "Epoch: 0, Batch: 141, Loss: 0.6887, Accuracy: 59.72%, Precision: 57.55%\n",
            "Epoch: 0, Batch: 142, Loss: 0.6563, Accuracy: 59.69%, Precision: 57.47%\n",
            "Epoch: 0, Batch: 143, Loss: 0.5652, Accuracy: 59.77%, Precision: 57.55%\n",
            "Epoch: 0, Batch: 144, Loss: 0.6963, Accuracy: 59.81%, Precision: 57.66%\n",
            "Epoch: 0, Batch: 145, Loss: 0.6272, Accuracy: 59.83%, Precision: 57.69%\n",
            "Epoch: 0, Batch: 146, Loss: 0.7313, Accuracy: 59.78%, Precision: 57.60%\n",
            "Epoch: 0, Batch: 147, Loss: 0.6572, Accuracy: 59.82%, Precision: 57.63%\n",
            "Epoch: 0, Batch: 148, Loss: 0.5612, Accuracy: 59.92%, Precision: 57.75%\n",
            "Epoch: 0, Batch: 149, Loss: 0.6746, Accuracy: 59.89%, Precision: 57.63%\n",
            "Epoch: 0, Batch: 150, Loss: 0.5531, Accuracy: 59.94%, Precision: 57.68%\n",
            "Epoch: 0, Batch: 151, Loss: 0.5963, Accuracy: 59.98%, Precision: 57.75%\n",
            "Epoch: 0, Batch: 152, Loss: 0.6531, Accuracy: 59.95%, Precision: 57.80%\n",
            "Epoch: 0, Batch: 153, Loss: 0.5613, Accuracy: 60.00%, Precision: 57.83%\n",
            "Epoch: 0, Batch: 154, Loss: 0.6781, Accuracy: 59.99%, Precision: 57.89%\n",
            "Epoch: 0, Batch: 155, Loss: 0.5769, Accuracy: 60.07%, Precision: 57.91%\n",
            "Epoch: 0, Batch: 156, Loss: 0.6186, Accuracy: 60.08%, Precision: 57.87%\n",
            "Epoch: 0, Batch: 157, Loss: 0.6193, Accuracy: 60.16%, Precision: 57.97%\n",
            "Epoch: 0, Batch: 158, Loss: 0.5597, Accuracy: 60.22%, Precision: 57.99%\n",
            "Epoch: 0, Batch: 159, Loss: 0.5727, Accuracy: 60.25%, Precision: 58.00%\n",
            "Epoch: 0, Batch: 160, Loss: 0.6092, Accuracy: 60.29%, Precision: 58.10%\n",
            "Epoch: 0, Batch: 161, Loss: 0.6187, Accuracy: 60.29%, Precision: 58.05%\n",
            "Epoch: 0, Batch: 162, Loss: 0.7002, Accuracy: 60.28%, Precision: 57.97%\n",
            "Epoch: 0, Batch: 163, Loss: 0.5546, Accuracy: 60.37%, Precision: 58.05%\n",
            "Epoch: 0, Batch: 164, Loss: 0.6387, Accuracy: 60.38%, Precision: 58.10%\n",
            "Epoch: 0, Batch: 165, Loss: 0.6653, Accuracy: 60.38%, Precision: 58.14%\n",
            "Epoch: 0, Batch: 166, Loss: 0.6204, Accuracy: 60.40%, Precision: 58.21%\n",
            "Epoch: 0, Batch: 167, Loss: 0.6186, Accuracy: 60.45%, Precision: 58.24%\n",
            "Epoch: 0, Batch: 168, Loss: 0.6550, Accuracy: 60.43%, Precision: 58.21%\n",
            "Epoch: 0, Batch: 169, Loss: 0.6139, Accuracy: 60.43%, Precision: 58.20%\n",
            "Epoch: 0, Batch: 170, Loss: 0.5682, Accuracy: 60.49%, Precision: 58.24%\n",
            "Epoch: 0, Batch: 171, Loss: 0.5809, Accuracy: 60.51%, Precision: 58.28%\n",
            "Epoch: 0, Batch: 172, Loss: 0.6733, Accuracy: 60.53%, Precision: 58.28%\n",
            "Epoch: 0, Batch: 173, Loss: 0.6011, Accuracy: 60.55%, Precision: 58.31%\n",
            "Epoch: 0, Batch: 174, Loss: 0.5636, Accuracy: 60.59%, Precision: 58.32%\n",
            "Epoch: 0, Batch: 175, Loss: 0.6052, Accuracy: 60.67%, Precision: 58.43%\n",
            "Epoch: 0, Batch: 176, Loss: 0.6334, Accuracy: 60.67%, Precision: 58.45%\n",
            "Epoch: 0, Batch: 177, Loss: 0.5886, Accuracy: 60.71%, Precision: 58.51%\n",
            "Epoch: 0, Batch: 178, Loss: 0.5971, Accuracy: 60.72%, Precision: 58.45%\n",
            "Epoch: 0, Batch: 179, Loss: 0.5821, Accuracy: 60.75%, Precision: 58.44%\n",
            "Epoch: 0, Batch: 180, Loss: 0.5973, Accuracy: 60.74%, Precision: 58.47%\n",
            "Epoch: 0, Batch: 181, Loss: 0.6088, Accuracy: 60.76%, Precision: 58.45%\n",
            "Epoch: 0, Batch: 182, Loss: 0.6322, Accuracy: 60.76%, Precision: 58.48%\n",
            "Epoch: 0, Batch: 183, Loss: 0.6675, Accuracy: 60.72%, Precision: 58.46%\n",
            "Epoch: 0, Batch: 184, Loss: 0.5670, Accuracy: 60.76%, Precision: 58.50%\n",
            "Epoch: 0, Batch: 185, Loss: 0.6479, Accuracy: 60.72%, Precision: 58.46%\n",
            "Epoch: 0, Batch: 186, Loss: 0.5386, Accuracy: 60.80%, Precision: 58.51%\n",
            "Epoch: 0, Batch: 187, Loss: 0.5783, Accuracy: 60.84%, Precision: 58.52%\n",
            "Epoch: 0, Batch: 188, Loss: 0.6922, Accuracy: 60.82%, Precision: 58.43%\n",
            "Epoch: 0, Batch: 189, Loss: 0.5741, Accuracy: 60.87%, Precision: 58.46%\n",
            "Epoch: 0, Batch: 190, Loss: 0.6616, Accuracy: 60.90%, Precision: 58.50%\n",
            "Epoch: 0, Batch: 191, Loss: 0.6546, Accuracy: 60.90%, Precision: 58.47%\n",
            "Epoch: 0, Batch: 192, Loss: 0.6189, Accuracy: 60.90%, Precision: 58.52%\n",
            "Epoch: 0, Batch: 193, Loss: 0.7114, Accuracy: 60.85%, Precision: 58.51%\n",
            "Epoch: 0, Batch: 194, Loss: 0.5257, Accuracy: 60.91%, Precision: 58.58%\n",
            "Epoch: 0, Batch: 195, Loss: 0.6323, Accuracy: 60.92%, Precision: 58.60%\n",
            "Epoch: 0, Batch: 196, Loss: 0.6273, Accuracy: 60.94%, Precision: 58.65%\n",
            "Epoch: 0, Batch: 197, Loss: 0.6437, Accuracy: 60.93%, Precision: 58.59%\n",
            "Epoch: 0, Batch: 198, Loss: 0.6276, Accuracy: 60.96%, Precision: 58.60%\n",
            "Epoch: 0, Batch: 199, Loss: 0.6767, Accuracy: 60.96%, Precision: 58.63%\n",
            "Epoch: 0, Batch: 200, Loss: 0.5738, Accuracy: 61.02%, Precision: 58.70%\n",
            "Epoch: 0, Batch: 201, Loss: 0.6063, Accuracy: 61.05%, Precision: 58.73%\n",
            "Epoch: 0, Batch: 202, Loss: 0.5704, Accuracy: 61.10%, Precision: 58.74%\n",
            "Epoch: 0, Batch: 203, Loss: 0.6110, Accuracy: 61.10%, Precision: 58.73%\n",
            "Epoch: 0, Batch: 204, Loss: 0.5736, Accuracy: 61.14%, Precision: 58.77%\n",
            "Epoch: 0, Batch: 205, Loss: 0.6579, Accuracy: 61.15%, Precision: 58.83%\n",
            "Epoch: 0, Batch: 206, Loss: 0.7256, Accuracy: 61.16%, Precision: 58.89%\n",
            "Epoch: 0, Batch: 207, Loss: 0.5798, Accuracy: 61.17%, Precision: 58.90%\n",
            "Epoch: 0, Batch: 208, Loss: 0.5997, Accuracy: 61.19%, Precision: 58.91%\n",
            "Epoch: 0, Batch: 209, Loss: 0.6912, Accuracy: 61.18%, Precision: 58.90%\n",
            "Epoch: 0, Batch: 210, Loss: 0.6357, Accuracy: 61.21%, Precision: 58.95%\n",
            "Epoch: 0, Batch: 211, Loss: 0.5808, Accuracy: 61.24%, Precision: 59.00%\n",
            "Epoch: 0, Batch: 212, Loss: 0.6670, Accuracy: 61.20%, Precision: 58.90%\n",
            "Epoch: 0, Batch: 213, Loss: 0.6469, Accuracy: 61.19%, Precision: 58.86%\n",
            "Epoch: 0, Batch: 214, Loss: 0.5708, Accuracy: 61.21%, Precision: 58.86%\n",
            "Epoch: 0, Batch: 215, Loss: 0.6271, Accuracy: 61.21%, Precision: 58.84%\n",
            "Epoch: 0, Batch: 216, Loss: 0.6544, Accuracy: 61.21%, Precision: 58.87%\n",
            "Epoch: 0, Batch: 217, Loss: 0.6247, Accuracy: 61.22%, Precision: 58.92%\n",
            "Epoch: 0, Batch: 218, Loss: 0.5947, Accuracy: 61.24%, Precision: 58.96%\n",
            "Epoch: 0, Batch: 219, Loss: 0.6147, Accuracy: 61.22%, Precision: 58.96%\n",
            "Epoch: 0, Batch: 220, Loss: 0.5921, Accuracy: 61.26%, Precision: 58.99%\n",
            "Epoch: 0, Batch: 221, Loss: 0.5632, Accuracy: 61.28%, Precision: 58.96%\n",
            "Epoch: 0, Batch: 222, Loss: 0.6406, Accuracy: 61.28%, Precision: 59.02%\n",
            "Epoch: 0, Batch: 223, Loss: 0.6229, Accuracy: 61.29%, Precision: 59.00%\n",
            "Epoch: 0, Batch: 224, Loss: 0.6368, Accuracy: 61.29%, Precision: 58.93%\n",
            "Epoch: 0, Batch: 225, Loss: 0.5497, Accuracy: 61.32%, Precision: 58.92%\n",
            "Epoch: 0, Batch: 226, Loss: 0.5856, Accuracy: 61.35%, Precision: 58.95%\n",
            "Epoch: 0, Batch: 227, Loss: 0.5369, Accuracy: 61.38%, Precision: 59.06%\n",
            "Epoch: 0, Batch: 228, Loss: 0.6073, Accuracy: 61.40%, Precision: 59.10%\n",
            "Epoch: 0, Batch: 229, Loss: 0.5715, Accuracy: 61.44%, Precision: 59.13%\n",
            "Epoch: 0, Batch: 230, Loss: 0.6298, Accuracy: 61.43%, Precision: 59.09%\n",
            "Epoch: 0, Batch: 231, Loss: 0.5954, Accuracy: 61.46%, Precision: 59.11%\n",
            "Epoch: 0, Batch: 232, Loss: 0.6421, Accuracy: 61.49%, Precision: 59.15%\n",
            "Epoch: 0, Batch: 233, Loss: 0.5606, Accuracy: 61.53%, Precision: 59.21%\n",
            "Epoch: 0, Batch: 234, Loss: 0.5520, Accuracy: 61.57%, Precision: 59.26%\n",
            "Epoch: 0, Batch: 235, Loss: 0.5621, Accuracy: 61.60%, Precision: 59.28%\n",
            "Epoch: 0, Batch: 236, Loss: 0.7221, Accuracy: 61.58%, Precision: 59.25%\n",
            "Epoch: 0, Batch: 237, Loss: 0.6499, Accuracy: 61.58%, Precision: 59.24%\n",
            "Epoch: 0, Batch: 238, Loss: 0.6460, Accuracy: 61.58%, Precision: 59.23%\n",
            "Epoch: 0, Batch: 239, Loss: 0.6490, Accuracy: 61.57%, Precision: 59.21%\n",
            "Epoch: 0, Batch: 240, Loss: 0.5618, Accuracy: 61.59%, Precision: 59.19%\n",
            "Epoch: 0, Batch: 241, Loss: 0.6061, Accuracy: 61.57%, Precision: 59.19%\n",
            "Epoch: 0, Batch: 242, Loss: 0.5950, Accuracy: 61.61%, Precision: 59.20%\n",
            "Epoch: 0, Batch: 243, Loss: 0.5619, Accuracy: 61.61%, Precision: 59.19%\n",
            "Epoch: 1, Batch: 1, Loss: 0.6942, Accuracy: 61.62%, Precision: 59.22%\n",
            "Epoch: 1, Batch: 2, Loss: 0.6014, Accuracy: 61.63%, Precision: 59.21%\n",
            "Epoch: 1, Batch: 3, Loss: 0.6218, Accuracy: 61.62%, Precision: 59.22%\n",
            "Epoch: 1, Batch: 4, Loss: 0.6242, Accuracy: 61.65%, Precision: 59.27%\n",
            "Epoch: 1, Batch: 5, Loss: 0.5616, Accuracy: 61.70%, Precision: 59.30%\n",
            "Epoch: 1, Batch: 6, Loss: 0.5681, Accuracy: 61.73%, Precision: 59.32%\n",
            "Epoch: 1, Batch: 7, Loss: 0.6009, Accuracy: 61.76%, Precision: 59.33%\n",
            "Epoch: 1, Batch: 8, Loss: 0.5946, Accuracy: 61.78%, Precision: 59.38%\n",
            "Epoch: 1, Batch: 9, Loss: 0.6668, Accuracy: 61.74%, Precision: 59.38%\n",
            "Epoch: 1, Batch: 10, Loss: 0.6511, Accuracy: 61.72%, Precision: 59.34%\n",
            "Epoch: 1, Batch: 11, Loss: 0.5984, Accuracy: 61.74%, Precision: 59.34%\n",
            "Epoch: 1, Batch: 12, Loss: 0.5887, Accuracy: 61.75%, Precision: 59.34%\n",
            "Epoch: 1, Batch: 13, Loss: 0.5912, Accuracy: 61.75%, Precision: 59.33%\n",
            "Epoch: 1, Batch: 14, Loss: 0.6144, Accuracy: 61.77%, Precision: 59.33%\n",
            "Epoch: 1, Batch: 15, Loss: 0.6133, Accuracy: 61.81%, Precision: 59.37%\n",
            "Epoch: 1, Batch: 16, Loss: 0.6199, Accuracy: 61.84%, Precision: 59.37%\n",
            "Epoch: 1, Batch: 17, Loss: 0.6162, Accuracy: 61.83%, Precision: 59.36%\n",
            "Epoch: 1, Batch: 18, Loss: 0.5243, Accuracy: 61.88%, Precision: 59.38%\n",
            "Epoch: 1, Batch: 19, Loss: 0.6020, Accuracy: 61.88%, Precision: 59.43%\n",
            "Epoch: 1, Batch: 20, Loss: 0.7418, Accuracy: 61.87%, Precision: 59.43%\n",
            "Epoch: 1, Batch: 21, Loss: 0.6361, Accuracy: 61.85%, Precision: 59.43%\n",
            "Epoch: 1, Batch: 22, Loss: 0.5886, Accuracy: 61.87%, Precision: 59.46%\n",
            "Epoch: 1, Batch: 23, Loss: 0.5548, Accuracy: 61.91%, Precision: 59.50%\n",
            "Epoch: 1, Batch: 24, Loss: 0.6062, Accuracy: 61.91%, Precision: 59.47%\n",
            "Epoch: 1, Batch: 25, Loss: 0.6478, Accuracy: 61.93%, Precision: 59.48%\n",
            "Epoch: 1, Batch: 26, Loss: 0.5521, Accuracy: 61.96%, Precision: 59.49%\n",
            "Epoch: 1, Batch: 27, Loss: 0.5573, Accuracy: 62.03%, Precision: 59.56%\n",
            "Epoch: 1, Batch: 28, Loss: 0.6519, Accuracy: 62.06%, Precision: 59.57%\n",
            "Epoch: 1, Batch: 29, Loss: 0.5190, Accuracy: 62.11%, Precision: 59.62%\n",
            "Epoch: 1, Batch: 30, Loss: 0.5542, Accuracy: 62.13%, Precision: 59.66%\n",
            "Epoch: 1, Batch: 31, Loss: 0.5910, Accuracy: 62.13%, Precision: 59.67%\n",
            "Epoch: 1, Batch: 32, Loss: 0.6159, Accuracy: 62.15%, Precision: 59.71%\n",
            "Epoch: 1, Batch: 33, Loss: 0.6191, Accuracy: 62.15%, Precision: 59.74%\n",
            "Epoch: 1, Batch: 34, Loss: 0.5853, Accuracy: 62.17%, Precision: 59.70%\n",
            "Epoch: 1, Batch: 35, Loss: 0.6405, Accuracy: 62.16%, Precision: 59.71%\n",
            "Epoch: 1, Batch: 36, Loss: 0.6021, Accuracy: 62.18%, Precision: 59.73%\n",
            "Epoch: 1, Batch: 37, Loss: 0.6006, Accuracy: 62.20%, Precision: 59.77%\n",
            "Epoch: 1, Batch: 38, Loss: 0.5764, Accuracy: 62.21%, Precision: 59.77%\n",
            "Epoch: 1, Batch: 39, Loss: 0.5996, Accuracy: 62.21%, Precision: 59.75%\n",
            "Epoch: 1, Batch: 40, Loss: 0.6033, Accuracy: 62.22%, Precision: 59.76%\n",
            "Epoch: 1, Batch: 41, Loss: 0.5139, Accuracy: 62.25%, Precision: 59.76%\n",
            "Epoch: 1, Batch: 42, Loss: 0.5896, Accuracy: 62.26%, Precision: 59.78%\n",
            "Epoch: 1, Batch: 43, Loss: 0.5856, Accuracy: 62.28%, Precision: 59.79%\n",
            "Epoch: 1, Batch: 44, Loss: 0.6172, Accuracy: 62.28%, Precision: 59.80%\n",
            "Epoch: 1, Batch: 45, Loss: 0.5806, Accuracy: 62.29%, Precision: 59.83%\n",
            "Epoch: 1, Batch: 46, Loss: 0.5984, Accuracy: 62.31%, Precision: 59.85%\n",
            "Epoch: 1, Batch: 47, Loss: 0.5486, Accuracy: 62.35%, Precision: 59.90%\n",
            "Epoch: 1, Batch: 48, Loss: 0.6343, Accuracy: 62.36%, Precision: 59.91%\n",
            "Epoch: 1, Batch: 49, Loss: 0.5484, Accuracy: 62.41%, Precision: 59.95%\n",
            "Epoch: 1, Batch: 50, Loss: 0.5750, Accuracy: 62.43%, Precision: 59.98%\n",
            "Epoch: 1, Batch: 51, Loss: 0.6703, Accuracy: 62.41%, Precision: 59.95%\n",
            "Epoch: 1, Batch: 52, Loss: 0.7110, Accuracy: 62.39%, Precision: 59.90%\n",
            "Epoch: 1, Batch: 53, Loss: 0.5861, Accuracy: 62.39%, Precision: 59.91%\n",
            "Epoch: 1, Batch: 54, Loss: 0.6965, Accuracy: 62.39%, Precision: 59.91%\n",
            "Epoch: 1, Batch: 55, Loss: 0.6807, Accuracy: 62.39%, Precision: 59.93%\n",
            "Epoch: 1, Batch: 56, Loss: 0.5958, Accuracy: 62.40%, Precision: 59.94%\n",
            "Epoch: 1, Batch: 57, Loss: 0.6289, Accuracy: 62.39%, Precision: 59.99%\n",
            "Epoch: 1, Batch: 58, Loss: 0.6191, Accuracy: 62.39%, Precision: 59.98%\n",
            "Epoch: 1, Batch: 59, Loss: 0.6567, Accuracy: 62.36%, Precision: 59.91%\n",
            "Epoch: 1, Batch: 60, Loss: 0.6022, Accuracy: 62.39%, Precision: 59.95%\n",
            "Epoch: 1, Batch: 61, Loss: 0.5716, Accuracy: 62.41%, Precision: 59.97%\n",
            "Epoch: 1, Batch: 62, Loss: 0.5784, Accuracy: 62.44%, Precision: 59.99%\n",
            "Epoch: 1, Batch: 63, Loss: 0.5759, Accuracy: 62.43%, Precision: 59.94%\n",
            "Epoch: 1, Batch: 64, Loss: 0.5904, Accuracy: 62.44%, Precision: 59.93%\n",
            "Epoch: 1, Batch: 65, Loss: 0.5961, Accuracy: 62.45%, Precision: 59.94%\n",
            "Epoch: 1, Batch: 66, Loss: 0.5904, Accuracy: 62.49%, Precision: 60.00%\n",
            "Epoch: 1, Batch: 67, Loss: 0.5980, Accuracy: 62.49%, Precision: 60.00%\n",
            "Epoch: 1, Batch: 68, Loss: 0.5182, Accuracy: 62.53%, Precision: 60.01%\n",
            "Epoch: 1, Batch: 69, Loss: 0.5816, Accuracy: 62.53%, Precision: 60.01%\n",
            "Epoch: 1, Batch: 70, Loss: 0.5736, Accuracy: 62.53%, Precision: 60.02%\n",
            "Epoch: 1, Batch: 71, Loss: 0.6609, Accuracy: 62.52%, Precision: 60.01%\n",
            "Epoch: 1, Batch: 72, Loss: 0.6032, Accuracy: 62.53%, Precision: 60.06%\n",
            "Epoch: 1, Batch: 73, Loss: 0.6020, Accuracy: 62.54%, Precision: 60.09%\n",
            "Epoch: 1, Batch: 74, Loss: 0.6040, Accuracy: 62.54%, Precision: 60.09%\n",
            "Epoch: 1, Batch: 75, Loss: 0.5334, Accuracy: 62.58%, Precision: 60.15%\n",
            "Epoch: 1, Batch: 76, Loss: 0.5932, Accuracy: 62.60%, Precision: 60.14%\n",
            "Epoch: 1, Batch: 77, Loss: 0.6833, Accuracy: 62.58%, Precision: 60.08%\n",
            "Epoch: 1, Batch: 78, Loss: 0.5488, Accuracy: 62.60%, Precision: 60.12%\n",
            "Epoch: 1, Batch: 79, Loss: 0.7249, Accuracy: 62.58%, Precision: 60.09%\n",
            "Epoch: 1, Batch: 80, Loss: 0.5803, Accuracy: 62.59%, Precision: 60.06%\n",
            "Epoch: 1, Batch: 81, Loss: 0.7364, Accuracy: 62.57%, Precision: 60.03%\n",
            "Epoch: 1, Batch: 82, Loss: 0.7079, Accuracy: 62.56%, Precision: 60.03%\n",
            "Epoch: 1, Batch: 83, Loss: 0.5705, Accuracy: 62.61%, Precision: 60.05%\n",
            "Epoch: 1, Batch: 84, Loss: 0.7835, Accuracy: 62.58%, Precision: 60.07%\n",
            "Epoch: 1, Batch: 85, Loss: 0.6146, Accuracy: 62.59%, Precision: 60.09%\n",
            "Epoch: 1, Batch: 86, Loss: 0.5935, Accuracy: 62.61%, Precision: 60.11%\n",
            "Epoch: 1, Batch: 87, Loss: 0.5984, Accuracy: 62.62%, Precision: 60.14%\n",
            "Epoch: 1, Batch: 88, Loss: 0.5953, Accuracy: 62.64%, Precision: 60.16%\n",
            "Epoch: 1, Batch: 89, Loss: 0.5954, Accuracy: 62.64%, Precision: 60.15%\n",
            "Epoch: 1, Batch: 90, Loss: 0.5733, Accuracy: 62.66%, Precision: 60.18%\n",
            "Epoch: 1, Batch: 91, Loss: 0.5803, Accuracy: 62.68%, Precision: 60.19%\n",
            "Epoch: 1, Batch: 92, Loss: 0.5904, Accuracy: 62.72%, Precision: 60.24%\n",
            "Epoch: 1, Batch: 93, Loss: 0.5660, Accuracy: 62.73%, Precision: 60.23%\n",
            "Epoch: 1, Batch: 94, Loss: 0.5815, Accuracy: 62.76%, Precision: 60.24%\n",
            "Epoch: 1, Batch: 95, Loss: 0.5825, Accuracy: 62.77%, Precision: 60.26%\n",
            "Epoch: 1, Batch: 96, Loss: 0.5953, Accuracy: 62.76%, Precision: 60.25%\n",
            "Epoch: 1, Batch: 97, Loss: 0.5884, Accuracy: 62.75%, Precision: 60.23%\n",
            "Epoch: 1, Batch: 98, Loss: 0.5699, Accuracy: 62.77%, Precision: 60.24%\n",
            "Epoch: 1, Batch: 99, Loss: 0.5880, Accuracy: 62.78%, Precision: 60.22%\n",
            "Epoch: 1, Batch: 100, Loss: 0.5820, Accuracy: 62.79%, Precision: 60.23%\n",
            "Epoch: 1, Batch: 101, Loss: 0.6023, Accuracy: 62.79%, Precision: 60.22%\n",
            "Epoch: 1, Batch: 102, Loss: 0.6986, Accuracy: 62.78%, Precision: 60.23%\n",
            "Epoch: 1, Batch: 103, Loss: 0.5935, Accuracy: 62.80%, Precision: 60.27%\n",
            "Epoch: 1, Batch: 104, Loss: 0.5849, Accuracy: 62.82%, Precision: 60.28%\n",
            "Epoch: 1, Batch: 105, Loss: 0.6053, Accuracy: 62.83%, Precision: 60.29%\n",
            "Epoch: 1, Batch: 106, Loss: 0.6220, Accuracy: 62.83%, Precision: 60.30%\n",
            "Epoch: 1, Batch: 107, Loss: 0.6171, Accuracy: 62.83%, Precision: 60.31%\n",
            "Epoch: 1, Batch: 108, Loss: 0.6605, Accuracy: 62.84%, Precision: 60.31%\n",
            "Epoch: 1, Batch: 109, Loss: 0.6688, Accuracy: 62.82%, Precision: 60.28%\n",
            "Epoch: 1, Batch: 110, Loss: 0.5447, Accuracy: 62.86%, Precision: 60.33%\n",
            "Epoch: 1, Batch: 111, Loss: 0.6611, Accuracy: 62.85%, Precision: 60.31%\n",
            "Epoch: 1, Batch: 112, Loss: 0.5383, Accuracy: 62.88%, Precision: 60.34%\n",
            "Epoch: 1, Batch: 113, Loss: 0.5620, Accuracy: 62.91%, Precision: 60.36%\n",
            "Epoch: 1, Batch: 114, Loss: 0.6439, Accuracy: 62.90%, Precision: 60.37%\n",
            "Epoch: 1, Batch: 115, Loss: 0.5469, Accuracy: 62.92%, Precision: 60.40%\n",
            "Epoch: 1, Batch: 116, Loss: 0.6409, Accuracy: 62.91%, Precision: 60.39%\n",
            "Epoch: 1, Batch: 117, Loss: 0.5410, Accuracy: 62.92%, Precision: 60.38%\n",
            "Epoch: 1, Batch: 118, Loss: 0.5536, Accuracy: 62.93%, Precision: 60.38%\n",
            "Epoch: 1, Batch: 119, Loss: 0.5913, Accuracy: 62.93%, Precision: 60.36%\n",
            "Epoch: 1, Batch: 120, Loss: 0.5487, Accuracy: 62.94%, Precision: 60.37%\n",
            "Epoch: 1, Batch: 121, Loss: 0.5852, Accuracy: 62.96%, Precision: 60.39%\n",
            "Epoch: 1, Batch: 122, Loss: 0.6343, Accuracy: 62.95%, Precision: 60.37%\n",
            "Epoch: 1, Batch: 123, Loss: 0.5938, Accuracy: 62.95%, Precision: 60.41%\n",
            "Epoch: 1, Batch: 124, Loss: 0.5347, Accuracy: 62.99%, Precision: 60.43%\n",
            "Epoch: 1, Batch: 125, Loss: 0.6382, Accuracy: 62.97%, Precision: 60.38%\n",
            "Epoch: 1, Batch: 126, Loss: 0.5911, Accuracy: 62.98%, Precision: 60.40%\n",
            "Epoch: 1, Batch: 127, Loss: 0.5997, Accuracy: 62.99%, Precision: 60.42%\n",
            "Epoch: 1, Batch: 128, Loss: 0.6205, Accuracy: 63.01%, Precision: 60.45%\n",
            "Epoch: 1, Batch: 129, Loss: 0.5906, Accuracy: 63.01%, Precision: 60.43%\n",
            "Epoch: 1, Batch: 130, Loss: 0.7107, Accuracy: 62.99%, Precision: 60.38%\n",
            "Epoch: 1, Batch: 131, Loss: 0.5540, Accuracy: 63.01%, Precision: 60.41%\n",
            "Epoch: 1, Batch: 132, Loss: 0.5664, Accuracy: 63.03%, Precision: 60.44%\n",
            "Epoch: 1, Batch: 133, Loss: 0.6100, Accuracy: 63.03%, Precision: 60.44%\n",
            "Epoch: 1, Batch: 134, Loss: 0.6129, Accuracy: 63.04%, Precision: 60.45%\n",
            "Epoch: 1, Batch: 135, Loss: 0.6184, Accuracy: 63.04%, Precision: 60.43%\n",
            "Epoch: 1, Batch: 136, Loss: 0.5747, Accuracy: 63.05%, Precision: 60.44%\n",
            "Epoch: 1, Batch: 137, Loss: 0.6419, Accuracy: 63.03%, Precision: 60.41%\n",
            "Epoch: 1, Batch: 138, Loss: 0.5450, Accuracy: 63.05%, Precision: 60.43%\n",
            "Epoch: 1, Batch: 139, Loss: 0.6853, Accuracy: 63.06%, Precision: 60.46%\n",
            "Epoch: 1, Batch: 140, Loss: 0.6082, Accuracy: 63.06%, Precision: 60.46%\n",
            "Epoch: 1, Batch: 141, Loss: 0.5926, Accuracy: 63.08%, Precision: 60.47%\n",
            "Epoch: 1, Batch: 142, Loss: 0.6464, Accuracy: 63.05%, Precision: 60.44%\n",
            "Epoch: 1, Batch: 143, Loss: 0.5801, Accuracy: 63.07%, Precision: 60.46%\n",
            "Epoch: 1, Batch: 144, Loss: 0.5668, Accuracy: 63.08%, Precision: 60.48%\n",
            "Epoch: 1, Batch: 145, Loss: 0.6482, Accuracy: 63.07%, Precision: 60.48%\n",
            "Epoch: 1, Batch: 146, Loss: 0.6660, Accuracy: 63.05%, Precision: 60.47%\n",
            "Epoch: 1, Batch: 147, Loss: 0.5752, Accuracy: 63.06%, Precision: 60.50%\n",
            "Epoch: 1, Batch: 148, Loss: 0.6123, Accuracy: 63.08%, Precision: 60.53%\n",
            "Epoch: 1, Batch: 149, Loss: 0.5636, Accuracy: 63.09%, Precision: 60.52%\n",
            "Epoch: 1, Batch: 150, Loss: 0.6104, Accuracy: 63.10%, Precision: 60.55%\n",
            "Epoch: 1, Batch: 151, Loss: 0.5601, Accuracy: 63.10%, Precision: 60.51%\n",
            "Epoch: 1, Batch: 152, Loss: 0.5158, Accuracy: 63.13%, Precision: 60.55%\n",
            "Epoch: 1, Batch: 153, Loss: 0.5915, Accuracy: 63.15%, Precision: 60.57%\n",
            "Epoch: 1, Batch: 154, Loss: 0.5267, Accuracy: 63.17%, Precision: 60.59%\n",
            "Epoch: 1, Batch: 155, Loss: 0.5800, Accuracy: 63.17%, Precision: 60.58%\n",
            "Epoch: 1, Batch: 156, Loss: 0.6559, Accuracy: 63.17%, Precision: 60.56%\n",
            "Epoch: 1, Batch: 157, Loss: 0.5396, Accuracy: 63.19%, Precision: 60.59%\n",
            "Epoch: 1, Batch: 158, Loss: 0.6205, Accuracy: 63.19%, Precision: 60.60%\n",
            "Epoch: 1, Batch: 159, Loss: 0.5282, Accuracy: 63.22%, Precision: 60.63%\n",
            "Epoch: 1, Batch: 160, Loss: 0.5936, Accuracy: 63.22%, Precision: 60.59%\n",
            "Epoch: 1, Batch: 161, Loss: 0.6391, Accuracy: 63.22%, Precision: 60.59%\n",
            "Epoch: 1, Batch: 162, Loss: 0.6312, Accuracy: 63.25%, Precision: 60.63%\n",
            "Epoch: 1, Batch: 163, Loss: 0.6766, Accuracy: 63.24%, Precision: 60.62%\n",
            "Epoch: 1, Batch: 164, Loss: 0.6093, Accuracy: 63.26%, Precision: 60.66%\n",
            "Epoch: 1, Batch: 165, Loss: 0.6605, Accuracy: 63.26%, Precision: 60.69%\n",
            "Epoch: 1, Batch: 166, Loss: 0.5574, Accuracy: 63.28%, Precision: 60.70%\n",
            "Epoch: 1, Batch: 167, Loss: 0.6337, Accuracy: 63.27%, Precision: 60.70%\n",
            "Epoch: 1, Batch: 168, Loss: 0.5850, Accuracy: 63.27%, Precision: 60.66%\n",
            "Epoch: 1, Batch: 169, Loss: 0.5685, Accuracy: 63.28%, Precision: 60.69%\n",
            "Epoch: 1, Batch: 170, Loss: 0.6199, Accuracy: 63.27%, Precision: 60.66%\n",
            "Epoch: 1, Batch: 171, Loss: 0.6138, Accuracy: 63.27%, Precision: 60.66%\n",
            "Epoch: 1, Batch: 172, Loss: 0.5968, Accuracy: 63.27%, Precision: 60.67%\n",
            "Epoch: 1, Batch: 173, Loss: 0.5768, Accuracy: 63.29%, Precision: 60.68%\n",
            "Epoch: 1, Batch: 174, Loss: 0.6097, Accuracy: 63.29%, Precision: 60.67%\n",
            "Epoch: 1, Batch: 175, Loss: 0.6098, Accuracy: 63.29%, Precision: 60.66%\n",
            "Epoch: 1, Batch: 176, Loss: 0.6608, Accuracy: 63.28%, Precision: 60.67%\n",
            "Epoch: 1, Batch: 177, Loss: 0.6199, Accuracy: 63.27%, Precision: 60.67%\n",
            "Epoch: 1, Batch: 178, Loss: 0.6107, Accuracy: 63.28%, Precision: 60.69%\n",
            "Epoch: 1, Batch: 179, Loss: 0.6043, Accuracy: 63.28%, Precision: 60.70%\n",
            "Epoch: 1, Batch: 180, Loss: 0.5945, Accuracy: 63.30%, Precision: 60.71%\n",
            "Epoch: 1, Batch: 181, Loss: 0.6203, Accuracy: 63.29%, Precision: 60.71%\n",
            "Epoch: 1, Batch: 182, Loss: 0.5333, Accuracy: 63.31%, Precision: 60.72%\n",
            "Epoch: 1, Batch: 183, Loss: 0.5735, Accuracy: 63.32%, Precision: 60.73%\n",
            "Epoch: 1, Batch: 184, Loss: 0.5439, Accuracy: 63.35%, Precision: 60.78%\n",
            "Epoch: 1, Batch: 185, Loss: 0.6017, Accuracy: 63.37%, Precision: 60.80%\n",
            "Epoch: 1, Batch: 186, Loss: 0.5151, Accuracy: 63.40%, Precision: 60.82%\n",
            "Epoch: 1, Batch: 187, Loss: 0.5286, Accuracy: 63.42%, Precision: 60.85%\n",
            "Epoch: 1, Batch: 188, Loss: 0.5497, Accuracy: 63.43%, Precision: 60.85%\n",
            "Epoch: 1, Batch: 189, Loss: 0.5847, Accuracy: 63.44%, Precision: 60.86%\n",
            "Epoch: 1, Batch: 190, Loss: 0.5914, Accuracy: 63.44%, Precision: 60.84%\n",
            "Epoch: 1, Batch: 191, Loss: 0.5952, Accuracy: 63.44%, Precision: 60.85%\n",
            "Epoch: 1, Batch: 192, Loss: 0.4825, Accuracy: 63.48%, Precision: 60.91%\n",
            "Epoch: 1, Batch: 193, Loss: 0.6159, Accuracy: 63.50%, Precision: 60.94%\n",
            "Epoch: 1, Batch: 194, Loss: 0.6172, Accuracy: 63.50%, Precision: 60.92%\n",
            "Epoch: 1, Batch: 195, Loss: 0.5885, Accuracy: 63.50%, Precision: 60.90%\n",
            "Epoch: 1, Batch: 196, Loss: 0.5396, Accuracy: 63.52%, Precision: 60.91%\n",
            "Epoch: 1, Batch: 197, Loss: 0.6300, Accuracy: 63.54%, Precision: 60.94%\n",
            "Epoch: 1, Batch: 198, Loss: 0.5614, Accuracy: 63.55%, Precision: 60.97%\n",
            "Epoch: 1, Batch: 199, Loss: 0.5002, Accuracy: 63.58%, Precision: 60.99%\n",
            "Epoch: 1, Batch: 200, Loss: 0.5554, Accuracy: 63.60%, Precision: 61.01%\n",
            "Epoch: 1, Batch: 201, Loss: 0.5729, Accuracy: 63.61%, Precision: 61.04%\n",
            "Epoch: 1, Batch: 202, Loss: 0.4774, Accuracy: 63.65%, Precision: 61.07%\n",
            "Epoch: 1, Batch: 203, Loss: 0.7396, Accuracy: 63.64%, Precision: 61.06%\n",
            "Epoch: 1, Batch: 204, Loss: 0.6423, Accuracy: 63.65%, Precision: 61.08%\n",
            "Epoch: 1, Batch: 205, Loss: 0.6085, Accuracy: 63.66%, Precision: 61.09%\n",
            "Epoch: 1, Batch: 206, Loss: 0.5900, Accuracy: 63.67%, Precision: 61.10%\n",
            "Epoch: 1, Batch: 207, Loss: 0.6035, Accuracy: 63.67%, Precision: 61.09%\n",
            "Epoch: 1, Batch: 208, Loss: 0.5758, Accuracy: 63.68%, Precision: 61.12%\n",
            "Epoch: 1, Batch: 209, Loss: 0.6112, Accuracy: 63.68%, Precision: 61.13%\n",
            "Epoch: 1, Batch: 210, Loss: 0.4797, Accuracy: 63.71%, Precision: 61.15%\n",
            "Epoch: 1, Batch: 211, Loss: 0.5423, Accuracy: 63.74%, Precision: 61.17%\n",
            "Epoch: 1, Batch: 212, Loss: 0.5815, Accuracy: 63.74%, Precision: 61.16%\n",
            "Epoch: 1, Batch: 213, Loss: 0.5584, Accuracy: 63.75%, Precision: 61.15%\n",
            "Epoch: 1, Batch: 214, Loss: 0.5548, Accuracy: 63.76%, Precision: 61.17%\n",
            "Epoch: 1, Batch: 215, Loss: 0.5360, Accuracy: 63.78%, Precision: 61.20%\n",
            "Epoch: 1, Batch: 216, Loss: 0.5717, Accuracy: 63.79%, Precision: 61.21%\n",
            "Epoch: 1, Batch: 217, Loss: 0.5444, Accuracy: 63.80%, Precision: 61.21%\n",
            "Epoch: 1, Batch: 218, Loss: 0.5382, Accuracy: 63.81%, Precision: 61.19%\n",
            "Epoch: 1, Batch: 219, Loss: 0.6205, Accuracy: 63.81%, Precision: 61.22%\n",
            "Epoch: 1, Batch: 220, Loss: 0.5774, Accuracy: 63.84%, Precision: 61.24%\n",
            "Epoch: 1, Batch: 221, Loss: 0.5513, Accuracy: 63.85%, Precision: 61.24%\n",
            "Epoch: 1, Batch: 222, Loss: 0.5671, Accuracy: 63.87%, Precision: 61.24%\n",
            "Epoch: 1, Batch: 223, Loss: 0.5692, Accuracy: 63.86%, Precision: 61.26%\n",
            "Epoch: 1, Batch: 224, Loss: 0.6504, Accuracy: 63.87%, Precision: 61.30%\n",
            "Epoch: 1, Batch: 225, Loss: 0.5883, Accuracy: 63.87%, Precision: 61.29%\n",
            "Epoch: 1, Batch: 226, Loss: 0.6675, Accuracy: 63.86%, Precision: 61.25%\n",
            "Epoch: 1, Batch: 227, Loss: 0.6134, Accuracy: 63.86%, Precision: 61.25%\n",
            "Epoch: 1, Batch: 228, Loss: 0.5749, Accuracy: 63.87%, Precision: 61.26%\n",
            "Epoch: 1, Batch: 229, Loss: 0.5699, Accuracy: 63.88%, Precision: 61.29%\n",
            "Epoch: 1, Batch: 230, Loss: 0.6109, Accuracy: 63.88%, Precision: 61.27%\n",
            "Epoch: 1, Batch: 231, Loss: 0.5577, Accuracy: 63.90%, Precision: 61.31%\n",
            "Epoch: 1, Batch: 232, Loss: 0.6383, Accuracy: 63.90%, Precision: 61.29%\n",
            "Epoch: 1, Batch: 233, Loss: 0.6668, Accuracy: 63.89%, Precision: 61.27%\n",
            "Epoch: 1, Batch: 234, Loss: 0.5965, Accuracy: 63.88%, Precision: 61.27%\n",
            "Epoch: 1, Batch: 235, Loss: 0.5818, Accuracy: 63.90%, Precision: 61.28%\n",
            "Epoch: 1, Batch: 236, Loss: 0.6545, Accuracy: 63.89%, Precision: 61.27%\n",
            "Epoch: 1, Batch: 237, Loss: 0.6125, Accuracy: 63.89%, Precision: 61.28%\n",
            "Epoch: 1, Batch: 238, Loss: 0.5918, Accuracy: 63.90%, Precision: 61.30%\n",
            "Epoch: 1, Batch: 239, Loss: 0.5516, Accuracy: 63.92%, Precision: 61.30%\n",
            "Epoch: 1, Batch: 240, Loss: 0.6487, Accuracy: 63.91%, Precision: 61.29%\n",
            "Epoch: 1, Batch: 241, Loss: 0.6603, Accuracy: 63.91%, Precision: 61.28%\n",
            "Epoch: 1, Batch: 242, Loss: 0.5263, Accuracy: 63.93%, Precision: 61.29%\n",
            "Epoch: 1, Batch: 243, Loss: 0.5199, Accuracy: 63.94%, Precision: 61.30%\n",
            "Epoch: 2, Batch: 1, Loss: 0.5735, Accuracy: 63.95%, Precision: 61.33%\n",
            "Epoch: 2, Batch: 2, Loss: 0.5574, Accuracy: 63.97%, Precision: 61.35%\n",
            "Epoch: 2, Batch: 3, Loss: 0.5382, Accuracy: 64.00%, Precision: 61.37%\n",
            "Epoch: 2, Batch: 4, Loss: 0.5708, Accuracy: 64.01%, Precision: 61.39%\n",
            "Epoch: 2, Batch: 5, Loss: 0.6590, Accuracy: 64.01%, Precision: 61.39%\n",
            "Epoch: 2, Batch: 6, Loss: 0.5080, Accuracy: 64.03%, Precision: 61.39%\n",
            "Epoch: 2, Batch: 7, Loss: 0.6823, Accuracy: 64.02%, Precision: 61.39%\n",
            "Epoch: 2, Batch: 8, Loss: 0.5200, Accuracy: 64.04%, Precision: 61.42%\n",
            "Epoch: 2, Batch: 9, Loss: 0.5343, Accuracy: 64.06%, Precision: 61.44%\n",
            "Epoch: 2, Batch: 10, Loss: 0.5410, Accuracy: 64.07%, Precision: 61.46%\n",
            "Epoch: 2, Batch: 11, Loss: 0.4938, Accuracy: 64.10%, Precision: 61.48%\n",
            "Epoch: 2, Batch: 12, Loss: 0.5442, Accuracy: 64.10%, Precision: 61.46%\n",
            "Epoch: 2, Batch: 13, Loss: 0.5876, Accuracy: 64.09%, Precision: 61.47%\n",
            "Epoch: 2, Batch: 14, Loss: 0.5073, Accuracy: 64.12%, Precision: 61.49%\n",
            "Epoch: 2, Batch: 15, Loss: 0.6018, Accuracy: 64.13%, Precision: 61.49%\n",
            "Epoch: 2, Batch: 16, Loss: 0.6473, Accuracy: 64.12%, Precision: 61.48%\n",
            "Epoch: 2, Batch: 17, Loss: 0.5062, Accuracy: 64.15%, Precision: 61.52%\n",
            "Epoch: 2, Batch: 18, Loss: 0.7031, Accuracy: 64.15%, Precision: 61.52%\n",
            "Epoch: 2, Batch: 19, Loss: 0.5796, Accuracy: 64.15%, Precision: 61.54%\n",
            "Epoch: 2, Batch: 20, Loss: 0.5685, Accuracy: 64.17%, Precision: 61.57%\n",
            "Epoch: 2, Batch: 21, Loss: 0.5344, Accuracy: 64.18%, Precision: 61.57%\n",
            "Epoch: 2, Batch: 22, Loss: 0.5974, Accuracy: 64.20%, Precision: 61.58%\n",
            "Epoch: 2, Batch: 23, Loss: 0.5888, Accuracy: 64.19%, Precision: 61.58%\n",
            "Epoch: 2, Batch: 24, Loss: 0.5097, Accuracy: 64.22%, Precision: 61.61%\n",
            "Epoch: 2, Batch: 25, Loss: 0.5359, Accuracy: 64.25%, Precision: 61.65%\n",
            "Epoch: 2, Batch: 26, Loss: 0.5671, Accuracy: 64.26%, Precision: 61.66%\n",
            "Epoch: 2, Batch: 27, Loss: 0.6461, Accuracy: 64.27%, Precision: 61.67%\n",
            "Epoch: 2, Batch: 28, Loss: 0.4716, Accuracy: 64.30%, Precision: 61.68%\n",
            "Epoch: 2, Batch: 29, Loss: 0.5644, Accuracy: 64.30%, Precision: 61.67%\n",
            "Epoch: 2, Batch: 30, Loss: 0.5887, Accuracy: 64.30%, Precision: 61.68%\n",
            "Epoch: 2, Batch: 31, Loss: 0.4687, Accuracy: 64.33%, Precision: 61.72%\n",
            "Epoch: 2, Batch: 32, Loss: 0.6189, Accuracy: 64.33%, Precision: 61.72%\n",
            "Epoch: 2, Batch: 33, Loss: 0.5772, Accuracy: 64.35%, Precision: 61.72%\n",
            "Epoch: 2, Batch: 34, Loss: 0.5445, Accuracy: 64.36%, Precision: 61.75%\n",
            "Epoch: 2, Batch: 35, Loss: 0.6042, Accuracy: 64.37%, Precision: 61.73%\n",
            "Epoch: 2, Batch: 36, Loss: 0.6391, Accuracy: 64.37%, Precision: 61.72%\n",
            "Epoch: 2, Batch: 37, Loss: 0.5251, Accuracy: 64.39%, Precision: 61.75%\n",
            "Epoch: 2, Batch: 38, Loss: 0.5581, Accuracy: 64.40%, Precision: 61.78%\n",
            "Epoch: 2, Batch: 39, Loss: 0.5817, Accuracy: 64.40%, Precision: 61.79%\n",
            "Epoch: 2, Batch: 40, Loss: 0.5509, Accuracy: 64.41%, Precision: 61.82%\n",
            "Epoch: 2, Batch: 41, Loss: 0.6013, Accuracy: 64.40%, Precision: 61.80%\n",
            "Epoch: 2, Batch: 42, Loss: 0.6367, Accuracy: 64.41%, Precision: 61.80%\n",
            "Epoch: 2, Batch: 43, Loss: 0.5693, Accuracy: 64.42%, Precision: 61.79%\n",
            "Epoch: 2, Batch: 44, Loss: 0.6182, Accuracy: 64.41%, Precision: 61.76%\n",
            "Epoch: 2, Batch: 45, Loss: 0.6039, Accuracy: 64.42%, Precision: 61.75%\n",
            "Epoch: 2, Batch: 46, Loss: 0.5205, Accuracy: 64.43%, Precision: 61.74%\n",
            "Epoch: 2, Batch: 47, Loss: 0.5115, Accuracy: 64.45%, Precision: 61.77%\n",
            "Epoch: 2, Batch: 48, Loss: 0.5534, Accuracy: 64.46%, Precision: 61.78%\n",
            "Epoch: 2, Batch: 49, Loss: 0.6451, Accuracy: 64.46%, Precision: 61.80%\n",
            "Epoch: 2, Batch: 50, Loss: 0.6804, Accuracy: 64.45%, Precision: 61.81%\n",
            "Epoch: 2, Batch: 51, Loss: 0.6034, Accuracy: 64.46%, Precision: 61.84%\n",
            "Epoch: 2, Batch: 52, Loss: 0.6940, Accuracy: 64.45%, Precision: 61.84%\n",
            "Epoch: 2, Batch: 53, Loss: 0.5206, Accuracy: 64.46%, Precision: 61.84%\n",
            "Epoch: 2, Batch: 54, Loss: 0.5015, Accuracy: 64.49%, Precision: 61.88%\n",
            "Epoch: 2, Batch: 55, Loss: 0.5575, Accuracy: 64.49%, Precision: 61.87%\n",
            "Epoch: 2, Batch: 56, Loss: 0.5443, Accuracy: 64.50%, Precision: 61.88%\n",
            "Epoch: 2, Batch: 57, Loss: 0.5395, Accuracy: 64.51%, Precision: 61.87%\n",
            "Epoch: 2, Batch: 58, Loss: 0.5744, Accuracy: 64.52%, Precision: 61.88%\n",
            "Epoch: 2, Batch: 59, Loss: 0.5534, Accuracy: 64.53%, Precision: 61.90%\n",
            "Epoch: 2, Batch: 60, Loss: 0.4789, Accuracy: 64.56%, Precision: 61.93%\n",
            "Epoch: 2, Batch: 61, Loss: 0.6062, Accuracy: 64.55%, Precision: 61.90%\n",
            "Epoch: 2, Batch: 62, Loss: 0.5905, Accuracy: 64.55%, Precision: 61.89%\n",
            "Epoch: 2, Batch: 63, Loss: 0.5658, Accuracy: 64.55%, Precision: 61.87%\n",
            "Epoch: 2, Batch: 64, Loss: 0.5609, Accuracy: 64.55%, Precision: 61.87%\n",
            "Epoch: 2, Batch: 65, Loss: 0.5643, Accuracy: 64.56%, Precision: 61.89%\n",
            "Epoch: 2, Batch: 66, Loss: 0.5725, Accuracy: 64.55%, Precision: 61.89%\n",
            "Epoch: 2, Batch: 67, Loss: 0.6118, Accuracy: 64.55%, Precision: 61.89%\n",
            "Epoch: 2, Batch: 68, Loss: 0.5312, Accuracy: 64.56%, Precision: 61.90%\n",
            "Epoch: 2, Batch: 69, Loss: 0.6398, Accuracy: 64.56%, Precision: 61.92%\n",
            "Epoch: 2, Batch: 70, Loss: 0.6438, Accuracy: 64.57%, Precision: 61.94%\n",
            "Epoch: 2, Batch: 71, Loss: 0.5586, Accuracy: 64.58%, Precision: 61.94%\n",
            "Epoch: 2, Batch: 72, Loss: 0.5546, Accuracy: 64.59%, Precision: 61.95%\n",
            "Epoch: 2, Batch: 73, Loss: 0.5309, Accuracy: 64.60%, Precision: 61.95%\n",
            "Epoch: 2, Batch: 74, Loss: 0.5430, Accuracy: 64.61%, Precision: 61.96%\n",
            "Epoch: 2, Batch: 75, Loss: 0.6114, Accuracy: 64.61%, Precision: 61.94%\n",
            "Epoch: 2, Batch: 76, Loss: 0.5696, Accuracy: 64.61%, Precision: 61.94%\n",
            "Epoch: 2, Batch: 77, Loss: 0.4348, Accuracy: 64.65%, Precision: 61.98%\n",
            "Epoch: 2, Batch: 78, Loss: 0.5331, Accuracy: 64.67%, Precision: 62.00%\n",
            "Epoch: 2, Batch: 79, Loss: 0.5569, Accuracy: 64.67%, Precision: 62.00%\n",
            "Epoch: 2, Batch: 80, Loss: 0.5772, Accuracy: 64.69%, Precision: 62.02%\n",
            "Epoch: 2, Batch: 81, Loss: 0.5444, Accuracy: 64.71%, Precision: 62.04%\n",
            "Epoch: 2, Batch: 82, Loss: 0.5719, Accuracy: 64.71%, Precision: 62.03%\n",
            "Epoch: 2, Batch: 83, Loss: 0.5785, Accuracy: 64.72%, Precision: 62.04%\n",
            "Epoch: 2, Batch: 84, Loss: 0.5733, Accuracy: 64.73%, Precision: 62.05%\n",
            "Epoch: 2, Batch: 85, Loss: 0.7191, Accuracy: 64.72%, Precision: 62.05%\n",
            "Epoch: 2, Batch: 86, Loss: 0.5607, Accuracy: 64.73%, Precision: 62.06%\n",
            "Epoch: 2, Batch: 87, Loss: 0.6451, Accuracy: 64.73%, Precision: 62.07%\n",
            "Epoch: 2, Batch: 88, Loss: 0.5198, Accuracy: 64.74%, Precision: 62.08%\n",
            "Epoch: 2, Batch: 89, Loss: 0.4873, Accuracy: 64.76%, Precision: 62.08%\n",
            "Epoch: 2, Batch: 90, Loss: 0.6901, Accuracy: 64.75%, Precision: 62.07%\n",
            "Epoch: 2, Batch: 91, Loss: 0.5471, Accuracy: 64.76%, Precision: 62.08%\n",
            "Epoch: 2, Batch: 92, Loss: 0.6819, Accuracy: 64.75%, Precision: 62.05%\n",
            "Epoch: 2, Batch: 93, Loss: 0.5695, Accuracy: 64.76%, Precision: 62.07%\n",
            "Epoch: 2, Batch: 94, Loss: 0.6575, Accuracy: 64.74%, Precision: 62.06%\n",
            "Epoch: 2, Batch: 95, Loss: 0.6630, Accuracy: 64.73%, Precision: 62.06%\n",
            "Epoch: 2, Batch: 96, Loss: 0.5341, Accuracy: 64.75%, Precision: 62.09%\n",
            "Epoch: 2, Batch: 97, Loss: 0.5697, Accuracy: 64.76%, Precision: 62.10%\n",
            "Epoch: 2, Batch: 98, Loss: 0.5151, Accuracy: 64.77%, Precision: 62.12%\n",
            "Epoch: 2, Batch: 99, Loss: 0.6120, Accuracy: 64.77%, Precision: 62.11%\n",
            "Epoch: 2, Batch: 100, Loss: 0.6126, Accuracy: 64.77%, Precision: 62.10%\n",
            "Epoch: 2, Batch: 101, Loss: 0.6540, Accuracy: 64.76%, Precision: 62.08%\n",
            "Epoch: 2, Batch: 102, Loss: 0.5882, Accuracy: 64.78%, Precision: 62.08%\n",
            "Epoch: 2, Batch: 103, Loss: 0.5320, Accuracy: 64.79%, Precision: 62.10%\n",
            "Epoch: 2, Batch: 104, Loss: 0.6482, Accuracy: 64.78%, Precision: 62.08%\n",
            "Epoch: 2, Batch: 105, Loss: 0.5316, Accuracy: 64.79%, Precision: 62.10%\n",
            "Epoch: 2, Batch: 106, Loss: 0.6479, Accuracy: 64.80%, Precision: 62.10%\n",
            "Epoch: 2, Batch: 107, Loss: 0.6017, Accuracy: 64.80%, Precision: 62.12%\n",
            "Epoch: 2, Batch: 108, Loss: 0.5497, Accuracy: 64.81%, Precision: 62.12%\n",
            "Epoch: 2, Batch: 109, Loss: 0.6530, Accuracy: 64.79%, Precision: 62.09%\n",
            "Epoch: 2, Batch: 110, Loss: 0.4752, Accuracy: 64.81%, Precision: 62.11%\n",
            "Epoch: 2, Batch: 111, Loss: 0.6371, Accuracy: 64.80%, Precision: 62.10%\n",
            "Epoch: 2, Batch: 112, Loss: 0.5712, Accuracy: 64.81%, Precision: 62.10%\n",
            "Epoch: 2, Batch: 113, Loss: 0.5871, Accuracy: 64.83%, Precision: 62.13%\n",
            "Epoch: 2, Batch: 114, Loss: 0.5600, Accuracy: 64.84%, Precision: 62.15%\n",
            "Epoch: 2, Batch: 115, Loss: 0.6245, Accuracy: 64.85%, Precision: 62.15%\n",
            "Epoch: 2, Batch: 116, Loss: 0.5837, Accuracy: 64.87%, Precision: 62.18%\n",
            "Epoch: 2, Batch: 117, Loss: 0.5563, Accuracy: 64.88%, Precision: 62.18%\n",
            "Epoch: 2, Batch: 118, Loss: 0.5574, Accuracy: 64.89%, Precision: 62.19%\n",
            "Epoch: 2, Batch: 119, Loss: 0.5910, Accuracy: 64.88%, Precision: 62.17%\n",
            "Epoch: 2, Batch: 120, Loss: 0.5329, Accuracy: 64.89%, Precision: 62.19%\n",
            "Epoch: 2, Batch: 121, Loss: 0.5611, Accuracy: 64.89%, Precision: 62.19%\n",
            "Epoch: 2, Batch: 122, Loss: 0.5811, Accuracy: 64.90%, Precision: 62.17%\n",
            "Epoch: 2, Batch: 123, Loss: 0.6069, Accuracy: 64.91%, Precision: 62.19%\n",
            "Epoch: 2, Batch: 124, Loss: 0.5770, Accuracy: 64.91%, Precision: 62.21%\n",
            "Epoch: 2, Batch: 125, Loss: 0.6143, Accuracy: 64.91%, Precision: 62.21%\n",
            "Epoch: 2, Batch: 126, Loss: 0.6359, Accuracy: 64.90%, Precision: 62.20%\n",
            "Epoch: 2, Batch: 127, Loss: 0.5168, Accuracy: 64.92%, Precision: 62.22%\n",
            "Epoch: 2, Batch: 128, Loss: 0.6246, Accuracy: 64.91%, Precision: 62.20%\n",
            "Epoch: 2, Batch: 129, Loss: 0.5590, Accuracy: 64.93%, Precision: 62.22%\n",
            "Epoch: 2, Batch: 130, Loss: 0.5608, Accuracy: 64.94%, Precision: 62.23%\n",
            "Epoch: 2, Batch: 131, Loss: 0.5690, Accuracy: 64.95%, Precision: 62.23%\n",
            "Epoch: 2, Batch: 132, Loss: 0.6565, Accuracy: 64.93%, Precision: 62.23%\n",
            "Epoch: 2, Batch: 133, Loss: 0.5592, Accuracy: 64.94%, Precision: 62.24%\n",
            "Epoch: 2, Batch: 134, Loss: 0.5489, Accuracy: 64.94%, Precision: 62.26%\n",
            "Epoch: 2, Batch: 135, Loss: 0.5312, Accuracy: 64.96%, Precision: 62.27%\n",
            "Epoch: 2, Batch: 136, Loss: 0.5774, Accuracy: 64.96%, Precision: 62.26%\n",
            "Epoch: 2, Batch: 137, Loss: 0.4829, Accuracy: 64.97%, Precision: 62.27%\n",
            "Epoch: 2, Batch: 138, Loss: 0.5294, Accuracy: 64.99%, Precision: 62.29%\n",
            "Epoch: 2, Batch: 139, Loss: 0.5368, Accuracy: 65.00%, Precision: 62.30%\n",
            "Epoch: 2, Batch: 140, Loss: 0.5152, Accuracy: 65.00%, Precision: 62.31%\n",
            "Epoch: 2, Batch: 141, Loss: 0.4919, Accuracy: 65.01%, Precision: 62.31%\n",
            "Epoch: 2, Batch: 142, Loss: 0.6254, Accuracy: 65.02%, Precision: 62.30%\n",
            "Epoch: 2, Batch: 143, Loss: 0.5893, Accuracy: 65.02%, Precision: 62.31%\n",
            "Epoch: 2, Batch: 144, Loss: 0.5411, Accuracy: 65.02%, Precision: 62.31%\n",
            "Epoch: 2, Batch: 145, Loss: 0.5079, Accuracy: 65.03%, Precision: 62.31%\n",
            "Epoch: 2, Batch: 146, Loss: 0.5775, Accuracy: 65.04%, Precision: 62.31%\n",
            "Epoch: 2, Batch: 147, Loss: 0.6070, Accuracy: 65.04%, Precision: 62.31%\n",
            "Epoch: 2, Batch: 148, Loss: 0.5559, Accuracy: 65.04%, Precision: 62.34%\n",
            "Epoch: 2, Batch: 149, Loss: 0.5679, Accuracy: 65.05%, Precision: 62.36%\n",
            "Epoch: 2, Batch: 150, Loss: 0.5355, Accuracy: 65.08%, Precision: 62.39%\n",
            "Epoch: 2, Batch: 151, Loss: 0.6171, Accuracy: 65.08%, Precision: 62.40%\n",
            "Epoch: 2, Batch: 152, Loss: 0.4751, Accuracy: 65.10%, Precision: 62.43%\n",
            "Epoch: 2, Batch: 153, Loss: 0.5786, Accuracy: 65.12%, Precision: 62.45%\n",
            "Epoch: 2, Batch: 154, Loss: 0.6592, Accuracy: 65.10%, Precision: 62.42%\n",
            "Epoch: 2, Batch: 155, Loss: 0.6563, Accuracy: 65.09%, Precision: 62.39%\n",
            "Epoch: 2, Batch: 156, Loss: 0.5573, Accuracy: 65.10%, Precision: 62.40%\n",
            "Epoch: 2, Batch: 157, Loss: 0.5893, Accuracy: 65.10%, Precision: 62.38%\n",
            "Epoch: 2, Batch: 158, Loss: 0.5585, Accuracy: 65.10%, Precision: 62.40%\n",
            "Epoch: 2, Batch: 159, Loss: 0.5168, Accuracy: 65.13%, Precision: 62.42%\n",
            "Epoch: 2, Batch: 160, Loss: 0.5427, Accuracy: 65.14%, Precision: 62.43%\n",
            "Epoch: 2, Batch: 161, Loss: 0.6145, Accuracy: 65.13%, Precision: 62.42%\n",
            "Epoch: 2, Batch: 162, Loss: 0.5561, Accuracy: 65.15%, Precision: 62.44%\n",
            "Epoch: 2, Batch: 163, Loss: 0.7247, Accuracy: 65.15%, Precision: 62.45%\n",
            "Epoch: 2, Batch: 164, Loss: 0.5744, Accuracy: 65.16%, Precision: 62.48%\n",
            "Epoch: 2, Batch: 165, Loss: 0.5482, Accuracy: 65.18%, Precision: 62.50%\n",
            "Epoch: 2, Batch: 166, Loss: 0.5290, Accuracy: 65.19%, Precision: 62.51%\n",
            "Epoch: 2, Batch: 167, Loss: 0.5635, Accuracy: 65.20%, Precision: 62.51%\n",
            "Epoch: 2, Batch: 168, Loss: 0.5506, Accuracy: 65.21%, Precision: 62.51%\n",
            "Epoch: 2, Batch: 169, Loss: 0.5769, Accuracy: 65.22%, Precision: 62.50%\n",
            "Epoch: 2, Batch: 170, Loss: 0.5419, Accuracy: 65.23%, Precision: 62.52%\n",
            "Epoch: 2, Batch: 171, Loss: 0.6024, Accuracy: 65.22%, Precision: 62.52%\n",
            "Epoch: 2, Batch: 172, Loss: 0.5642, Accuracy: 65.23%, Precision: 62.50%\n",
            "Epoch: 2, Batch: 173, Loss: 0.4936, Accuracy: 65.24%, Precision: 62.52%\n",
            "Epoch: 2, Batch: 174, Loss: 0.5293, Accuracy: 65.25%, Precision: 62.54%\n",
            "Epoch: 2, Batch: 175, Loss: 0.6123, Accuracy: 65.24%, Precision: 62.54%\n",
            "Epoch: 2, Batch: 176, Loss: 0.5411, Accuracy: 65.25%, Precision: 62.55%\n",
            "Epoch: 2, Batch: 177, Loss: 0.5289, Accuracy: 65.25%, Precision: 62.56%\n",
            "Epoch: 2, Batch: 178, Loss: 0.6001, Accuracy: 65.25%, Precision: 62.56%\n",
            "Epoch: 2, Batch: 179, Loss: 0.5518, Accuracy: 65.25%, Precision: 62.56%\n",
            "Epoch: 2, Batch: 180, Loss: 0.4741, Accuracy: 65.27%, Precision: 62.58%\n",
            "Epoch: 2, Batch: 181, Loss: 0.5977, Accuracy: 65.27%, Precision: 62.59%\n",
            "Epoch: 2, Batch: 182, Loss: 0.5400, Accuracy: 65.28%, Precision: 62.61%\n",
            "Epoch: 2, Batch: 183, Loss: 0.5957, Accuracy: 65.28%, Precision: 62.60%\n",
            "Epoch: 2, Batch: 184, Loss: 0.4494, Accuracy: 65.30%, Precision: 62.61%\n",
            "Epoch: 2, Batch: 185, Loss: 0.5665, Accuracy: 65.31%, Precision: 62.60%\n",
            "Epoch: 2, Batch: 186, Loss: 0.5459, Accuracy: 65.32%, Precision: 62.63%\n",
            "Epoch: 2, Batch: 187, Loss: 0.6441, Accuracy: 65.30%, Precision: 62.62%\n",
            "Epoch: 2, Batch: 188, Loss: 0.5799, Accuracy: 65.31%, Precision: 62.63%\n",
            "Epoch: 2, Batch: 189, Loss: 0.5299, Accuracy: 65.31%, Precision: 62.63%\n",
            "Epoch: 2, Batch: 190, Loss: 0.6211, Accuracy: 65.31%, Precision: 62.64%\n",
            "Epoch: 2, Batch: 191, Loss: 0.4783, Accuracy: 65.34%, Precision: 62.67%\n",
            "Epoch: 2, Batch: 192, Loss: 0.5593, Accuracy: 65.34%, Precision: 62.68%\n",
            "Epoch: 2, Batch: 193, Loss: 0.6431, Accuracy: 65.34%, Precision: 62.67%\n",
            "Epoch: 2, Batch: 194, Loss: 0.6245, Accuracy: 65.34%, Precision: 62.66%\n",
            "Epoch: 2, Batch: 195, Loss: 0.5475, Accuracy: 65.35%, Precision: 62.67%\n",
            "Epoch: 2, Batch: 196, Loss: 0.5405, Accuracy: 65.35%, Precision: 62.65%\n",
            "Epoch: 2, Batch: 197, Loss: 0.5890, Accuracy: 65.36%, Precision: 62.66%\n",
            "Epoch: 2, Batch: 198, Loss: 0.5620, Accuracy: 65.36%, Precision: 62.66%\n",
            "Epoch: 2, Batch: 199, Loss: 0.5687, Accuracy: 65.36%, Precision: 62.66%\n",
            "Epoch: 2, Batch: 200, Loss: 0.5828, Accuracy: 65.36%, Precision: 62.67%\n",
            "Epoch: 2, Batch: 201, Loss: 0.5672, Accuracy: 65.37%, Precision: 62.67%\n",
            "Epoch: 2, Batch: 202, Loss: 0.5797, Accuracy: 65.37%, Precision: 62.67%\n",
            "Epoch: 2, Batch: 203, Loss: 0.5854, Accuracy: 65.37%, Precision: 62.69%\n",
            "Epoch: 2, Batch: 204, Loss: 0.6343, Accuracy: 65.35%, Precision: 62.68%\n",
            "Epoch: 2, Batch: 205, Loss: 0.5608, Accuracy: 65.35%, Precision: 62.68%\n",
            "Epoch: 2, Batch: 206, Loss: 0.5744, Accuracy: 65.35%, Precision: 62.69%\n",
            "Epoch: 2, Batch: 207, Loss: 0.5686, Accuracy: 65.34%, Precision: 62.70%\n",
            "Epoch: 2, Batch: 208, Loss: 0.4689, Accuracy: 65.37%, Precision: 62.73%\n",
            "Epoch: 2, Batch: 209, Loss: 0.6505, Accuracy: 65.38%, Precision: 62.75%\n",
            "Epoch: 2, Batch: 210, Loss: 0.5342, Accuracy: 65.38%, Precision: 62.75%\n",
            "Epoch: 2, Batch: 211, Loss: 0.5027, Accuracy: 65.40%, Precision: 62.76%\n",
            "Epoch: 2, Batch: 212, Loss: 0.5775, Accuracy: 65.41%, Precision: 62.76%\n",
            "Epoch: 2, Batch: 213, Loss: 0.5685, Accuracy: 65.43%, Precision: 62.79%\n",
            "Epoch: 2, Batch: 214, Loss: 0.5436, Accuracy: 65.44%, Precision: 62.79%\n",
            "Epoch: 2, Batch: 215, Loss: 0.6317, Accuracy: 65.43%, Precision: 62.77%\n",
            "Epoch: 2, Batch: 216, Loss: 0.6735, Accuracy: 65.43%, Precision: 62.75%\n",
            "Epoch: 2, Batch: 217, Loss: 0.5124, Accuracy: 65.43%, Precision: 62.74%\n",
            "Epoch: 2, Batch: 218, Loss: 0.6092, Accuracy: 65.44%, Precision: 62.76%\n",
            "Epoch: 2, Batch: 219, Loss: 0.6239, Accuracy: 65.44%, Precision: 62.77%\n",
            "Epoch: 2, Batch: 220, Loss: 0.5950, Accuracy: 65.44%, Precision: 62.77%\n",
            "Epoch: 2, Batch: 221, Loss: 0.5368, Accuracy: 65.45%, Precision: 62.77%\n",
            "Epoch: 2, Batch: 222, Loss: 0.5199, Accuracy: 65.47%, Precision: 62.79%\n",
            "Epoch: 2, Batch: 223, Loss: 0.5280, Accuracy: 65.48%, Precision: 62.79%\n",
            "Epoch: 2, Batch: 224, Loss: 0.5842, Accuracy: 65.48%, Precision: 62.79%\n",
            "Epoch: 2, Batch: 225, Loss: 0.5240, Accuracy: 65.49%, Precision: 62.80%\n",
            "Epoch: 2, Batch: 226, Loss: 0.4837, Accuracy: 65.51%, Precision: 62.80%\n",
            "Epoch: 2, Batch: 227, Loss: 0.5037, Accuracy: 65.52%, Precision: 62.81%\n",
            "Epoch: 2, Batch: 228, Loss: 0.5842, Accuracy: 65.52%, Precision: 62.82%\n",
            "Epoch: 2, Batch: 229, Loss: 0.5418, Accuracy: 65.53%, Precision: 62.84%\n",
            "Epoch: 2, Batch: 230, Loss: 0.5748, Accuracy: 65.54%, Precision: 62.84%\n",
            "Epoch: 2, Batch: 231, Loss: 0.5318, Accuracy: 65.54%, Precision: 62.84%\n",
            "Epoch: 2, Batch: 232, Loss: 0.6716, Accuracy: 65.53%, Precision: 62.82%\n",
            "Epoch: 2, Batch: 233, Loss: 0.4985, Accuracy: 65.54%, Precision: 62.82%\n",
            "Epoch: 2, Batch: 234, Loss: 0.6357, Accuracy: 65.54%, Precision: 62.81%\n",
            "Epoch: 2, Batch: 235, Loss: 0.6010, Accuracy: 65.55%, Precision: 62.81%\n",
            "Epoch: 2, Batch: 236, Loss: 0.6239, Accuracy: 65.54%, Precision: 62.79%\n",
            "Epoch: 2, Batch: 237, Loss: 0.6722, Accuracy: 65.54%, Precision: 62.80%\n",
            "Epoch: 2, Batch: 238, Loss: 0.5813, Accuracy: 65.53%, Precision: 62.80%\n",
            "Epoch: 2, Batch: 239, Loss: 0.5333, Accuracy: 65.55%, Precision: 62.81%\n",
            "Epoch: 2, Batch: 240, Loss: 0.5510, Accuracy: 65.56%, Precision: 62.84%\n",
            "Epoch: 2, Batch: 241, Loss: 0.6005, Accuracy: 65.55%, Precision: 62.83%\n",
            "Epoch: 2, Batch: 242, Loss: 0.5310, Accuracy: 65.56%, Precision: 62.85%\n",
            "Epoch: 2, Batch: 243, Loss: 0.6585, Accuracy: 65.56%, Precision: 62.84%\n",
            "Epoch: 3, Batch: 1, Loss: 0.5818, Accuracy: 65.57%, Precision: 62.86%\n",
            "Epoch: 3, Batch: 2, Loss: 0.5668, Accuracy: 65.57%, Precision: 62.86%\n",
            "Epoch: 3, Batch: 3, Loss: 0.4997, Accuracy: 65.59%, Precision: 62.89%\n",
            "Epoch: 3, Batch: 4, Loss: 0.5598, Accuracy: 65.60%, Precision: 62.89%\n",
            "Epoch: 3, Batch: 5, Loss: 0.5558, Accuracy: 65.61%, Precision: 62.90%\n",
            "Epoch: 3, Batch: 6, Loss: 0.5725, Accuracy: 65.62%, Precision: 62.90%\n",
            "Epoch: 3, Batch: 7, Loss: 0.5730, Accuracy: 65.62%, Precision: 62.89%\n",
            "Epoch: 3, Batch: 8, Loss: 0.6379, Accuracy: 65.61%, Precision: 62.88%\n",
            "Epoch: 3, Batch: 9, Loss: 0.5241, Accuracy: 65.62%, Precision: 62.90%\n",
            "Epoch: 3, Batch: 10, Loss: 0.5086, Accuracy: 65.63%, Precision: 62.90%\n",
            "Epoch: 3, Batch: 11, Loss: 0.5292, Accuracy: 65.64%, Precision: 62.92%\n",
            "Epoch: 3, Batch: 12, Loss: 0.5690, Accuracy: 65.65%, Precision: 62.94%\n",
            "Epoch: 3, Batch: 13, Loss: 0.5417, Accuracy: 65.66%, Precision: 62.94%\n",
            "Epoch: 3, Batch: 14, Loss: 0.6319, Accuracy: 65.66%, Precision: 62.95%\n",
            "Epoch: 3, Batch: 15, Loss: 0.5236, Accuracy: 65.67%, Precision: 62.97%\n",
            "Epoch: 3, Batch: 16, Loss: 0.5615, Accuracy: 65.67%, Precision: 62.95%\n",
            "Epoch: 3, Batch: 17, Loss: 0.5010, Accuracy: 65.68%, Precision: 62.97%\n",
            "Epoch: 3, Batch: 18, Loss: 0.5454, Accuracy: 65.69%, Precision: 62.96%\n",
            "Epoch: 3, Batch: 19, Loss: 0.5161, Accuracy: 65.70%, Precision: 62.97%\n",
            "Epoch: 3, Batch: 20, Loss: 0.4686, Accuracy: 65.72%, Precision: 62.99%\n",
            "Epoch: 3, Batch: 21, Loss: 0.5487, Accuracy: 65.74%, Precision: 63.01%\n",
            "Epoch: 3, Batch: 22, Loss: 0.6397, Accuracy: 65.73%, Precision: 63.01%\n",
            "Epoch: 3, Batch: 23, Loss: 0.5716, Accuracy: 65.73%, Precision: 63.01%\n",
            "Epoch: 3, Batch: 24, Loss: 0.5544, Accuracy: 65.73%, Precision: 63.00%\n",
            "Epoch: 3, Batch: 25, Loss: 0.4081, Accuracy: 65.75%, Precision: 63.01%\n",
            "Epoch: 3, Batch: 26, Loss: 0.6055, Accuracy: 65.76%, Precision: 63.02%\n",
            "Epoch: 3, Batch: 27, Loss: 0.4924, Accuracy: 65.78%, Precision: 63.05%\n",
            "Epoch: 3, Batch: 28, Loss: 0.5621, Accuracy: 65.78%, Precision: 63.06%\n",
            "Epoch: 3, Batch: 29, Loss: 0.5854, Accuracy: 65.79%, Precision: 63.06%\n",
            "Epoch: 3, Batch: 30, Loss: 0.5605, Accuracy: 65.79%, Precision: 63.06%\n",
            "Epoch: 3, Batch: 31, Loss: 0.4862, Accuracy: 65.80%, Precision: 63.08%\n",
            "Epoch: 3, Batch: 32, Loss: 0.5302, Accuracy: 65.81%, Precision: 63.08%\n",
            "Epoch: 3, Batch: 33, Loss: 0.6140, Accuracy: 65.81%, Precision: 63.07%\n",
            "Epoch: 3, Batch: 34, Loss: 0.4901, Accuracy: 65.82%, Precision: 63.09%\n",
            "Epoch: 3, Batch: 35, Loss: 0.5697, Accuracy: 65.83%, Precision: 63.09%\n",
            "Epoch: 3, Batch: 36, Loss: 0.5180, Accuracy: 65.84%, Precision: 63.12%\n",
            "Epoch: 3, Batch: 37, Loss: 0.4609, Accuracy: 65.85%, Precision: 63.12%\n",
            "Epoch: 3, Batch: 38, Loss: 0.6595, Accuracy: 65.84%, Precision: 63.12%\n",
            "Epoch: 3, Batch: 39, Loss: 0.5158, Accuracy: 65.85%, Precision: 63.12%\n",
            "Epoch: 3, Batch: 40, Loss: 0.5334, Accuracy: 65.85%, Precision: 63.12%\n",
            "Epoch: 3, Batch: 41, Loss: 0.5566, Accuracy: 65.86%, Precision: 63.11%\n",
            "Epoch: 3, Batch: 42, Loss: 0.4509, Accuracy: 65.88%, Precision: 63.12%\n",
            "Epoch: 3, Batch: 43, Loss: 0.5238, Accuracy: 65.89%, Precision: 63.14%\n",
            "Epoch: 3, Batch: 44, Loss: 0.5258, Accuracy: 65.90%, Precision: 63.16%\n",
            "Epoch: 3, Batch: 45, Loss: 0.5583, Accuracy: 65.90%, Precision: 63.16%\n",
            "Epoch: 3, Batch: 46, Loss: 0.5479, Accuracy: 65.90%, Precision: 63.17%\n",
            "Epoch: 3, Batch: 47, Loss: 0.5567, Accuracy: 65.91%, Precision: 63.18%\n",
            "Epoch: 3, Batch: 48, Loss: 0.5294, Accuracy: 65.92%, Precision: 63.18%\n",
            "Epoch: 3, Batch: 49, Loss: 0.5544, Accuracy: 65.92%, Precision: 63.18%\n",
            "Epoch: 3, Batch: 50, Loss: 0.5266, Accuracy: 65.94%, Precision: 63.19%\n",
            "Epoch: 3, Batch: 51, Loss: 0.3902, Accuracy: 65.96%, Precision: 63.20%\n",
            "Epoch: 3, Batch: 52, Loss: 0.6376, Accuracy: 65.95%, Precision: 63.18%\n",
            "Epoch: 3, Batch: 53, Loss: 0.5028, Accuracy: 65.95%, Precision: 63.17%\n",
            "Epoch: 3, Batch: 54, Loss: 0.4751, Accuracy: 65.96%, Precision: 63.19%\n",
            "Epoch: 3, Batch: 55, Loss: 0.5246, Accuracy: 65.97%, Precision: 63.21%\n",
            "Epoch: 3, Batch: 56, Loss: 0.5082, Accuracy: 65.98%, Precision: 63.21%\n",
            "Epoch: 3, Batch: 57, Loss: 0.5187, Accuracy: 65.99%, Precision: 63.22%\n",
            "Epoch: 3, Batch: 58, Loss: 0.5851, Accuracy: 65.98%, Precision: 63.22%\n",
            "Epoch: 3, Batch: 59, Loss: 0.5558, Accuracy: 65.99%, Precision: 63.22%\n",
            "Epoch: 3, Batch: 60, Loss: 0.6713, Accuracy: 65.99%, Precision: 63.23%\n",
            "Epoch: 3, Batch: 61, Loss: 0.4786, Accuracy: 66.00%, Precision: 63.23%\n",
            "Epoch: 3, Batch: 62, Loss: 0.5538, Accuracy: 66.00%, Precision: 63.23%\n",
            "Epoch: 3, Batch: 63, Loss: 0.5548, Accuracy: 66.01%, Precision: 63.23%\n",
            "Epoch: 3, Batch: 64, Loss: 0.4591, Accuracy: 66.04%, Precision: 63.26%\n",
            "Epoch: 3, Batch: 65, Loss: 0.4973, Accuracy: 66.04%, Precision: 63.26%\n",
            "Epoch: 3, Batch: 66, Loss: 0.5189, Accuracy: 66.05%, Precision: 63.27%\n",
            "Epoch: 3, Batch: 67, Loss: 0.6994, Accuracy: 66.05%, Precision: 63.28%\n",
            "Epoch: 3, Batch: 68, Loss: 0.5826, Accuracy: 66.06%, Precision: 63.29%\n",
            "Epoch: 3, Batch: 69, Loss: 0.6359, Accuracy: 66.05%, Precision: 63.28%\n",
            "Epoch: 3, Batch: 70, Loss: 0.4951, Accuracy: 66.07%, Precision: 63.30%\n",
            "Epoch: 3, Batch: 71, Loss: 0.5435, Accuracy: 66.07%, Precision: 63.31%\n",
            "Epoch: 3, Batch: 72, Loss: 0.4965, Accuracy: 66.08%, Precision: 63.31%\n",
            "Epoch: 3, Batch: 73, Loss: 0.5792, Accuracy: 66.09%, Precision: 63.31%\n",
            "Epoch: 3, Batch: 74, Loss: 0.6122, Accuracy: 66.08%, Precision: 63.30%\n",
            "Epoch: 3, Batch: 75, Loss: 0.5954, Accuracy: 66.07%, Precision: 63.31%\n",
            "Epoch: 3, Batch: 76, Loss: 0.5847, Accuracy: 66.07%, Precision: 63.32%\n",
            "Epoch: 3, Batch: 77, Loss: 0.5376, Accuracy: 66.07%, Precision: 63.34%\n",
            "Epoch: 3, Batch: 78, Loss: 0.5698, Accuracy: 66.08%, Precision: 63.33%\n",
            "Epoch: 3, Batch: 79, Loss: 0.5889, Accuracy: 66.08%, Precision: 63.33%\n",
            "Epoch: 3, Batch: 80, Loss: 0.4809, Accuracy: 66.10%, Precision: 63.35%\n",
            "Epoch: 3, Batch: 81, Loss: 0.5561, Accuracy: 66.10%, Precision: 63.36%\n",
            "Epoch: 3, Batch: 82, Loss: 0.5557, Accuracy: 66.10%, Precision: 63.36%\n",
            "Epoch: 3, Batch: 83, Loss: 0.5561, Accuracy: 66.10%, Precision: 63.36%\n",
            "Epoch: 3, Batch: 84, Loss: 0.4407, Accuracy: 66.12%, Precision: 63.37%\n",
            "Epoch: 3, Batch: 85, Loss: 0.5179, Accuracy: 66.13%, Precision: 63.38%\n",
            "Epoch: 3, Batch: 86, Loss: 0.5509, Accuracy: 66.14%, Precision: 63.39%\n",
            "Epoch: 3, Batch: 87, Loss: 0.6020, Accuracy: 66.14%, Precision: 63.40%\n",
            "Epoch: 3, Batch: 88, Loss: 0.5106, Accuracy: 66.15%, Precision: 63.41%\n",
            "Epoch: 3, Batch: 89, Loss: 0.3887, Accuracy: 66.17%, Precision: 63.43%\n",
            "Epoch: 3, Batch: 90, Loss: 0.6030, Accuracy: 66.17%, Precision: 63.43%\n",
            "Epoch: 3, Batch: 91, Loss: 0.5273, Accuracy: 66.18%, Precision: 63.43%\n",
            "Epoch: 3, Batch: 92, Loss: 0.6092, Accuracy: 66.18%, Precision: 63.44%\n",
            "Epoch: 3, Batch: 93, Loss: 0.5407, Accuracy: 66.18%, Precision: 63.43%\n",
            "Epoch: 3, Batch: 94, Loss: 0.6510, Accuracy: 66.18%, Precision: 63.43%\n",
            "Epoch: 3, Batch: 95, Loss: 0.4874, Accuracy: 66.20%, Precision: 63.45%\n",
            "Epoch: 3, Batch: 96, Loss: 0.4722, Accuracy: 66.20%, Precision: 63.45%\n",
            "Epoch: 3, Batch: 97, Loss: 0.5762, Accuracy: 66.21%, Precision: 63.46%\n",
            "Epoch: 3, Batch: 98, Loss: 0.5644, Accuracy: 66.20%, Precision: 63.45%\n",
            "Epoch: 3, Batch: 99, Loss: 0.5979, Accuracy: 66.20%, Precision: 63.46%\n",
            "Epoch: 3, Batch: 100, Loss: 0.5161, Accuracy: 66.22%, Precision: 63.48%\n",
            "Epoch: 3, Batch: 101, Loss: 0.5560, Accuracy: 66.23%, Precision: 63.48%\n",
            "Epoch: 3, Batch: 102, Loss: 0.5064, Accuracy: 66.24%, Precision: 63.50%\n",
            "Epoch: 3, Batch: 103, Loss: 0.5099, Accuracy: 66.25%, Precision: 63.51%\n",
            "Epoch: 3, Batch: 104, Loss: 0.5781, Accuracy: 66.26%, Precision: 63.53%\n",
            "Epoch: 3, Batch: 105, Loss: 0.4725, Accuracy: 66.28%, Precision: 63.55%\n",
            "Epoch: 3, Batch: 106, Loss: 0.5264, Accuracy: 66.28%, Precision: 63.55%\n",
            "Epoch: 3, Batch: 107, Loss: 0.6168, Accuracy: 66.27%, Precision: 63.54%\n",
            "Epoch: 3, Batch: 108, Loss: 0.4818, Accuracy: 66.29%, Precision: 63.56%\n",
            "Epoch: 3, Batch: 109, Loss: 0.5043, Accuracy: 66.30%, Precision: 63.57%\n",
            "Epoch: 3, Batch: 110, Loss: 0.5137, Accuracy: 66.31%, Precision: 63.58%\n",
            "Epoch: 3, Batch: 111, Loss: 0.5071, Accuracy: 66.32%, Precision: 63.58%\n",
            "Epoch: 3, Batch: 112, Loss: 0.5221, Accuracy: 66.32%, Precision: 63.59%\n",
            "Epoch: 3, Batch: 113, Loss: 0.5848, Accuracy: 66.33%, Precision: 63.59%\n",
            "Epoch: 3, Batch: 114, Loss: 0.6063, Accuracy: 66.33%, Precision: 63.59%\n",
            "Epoch: 3, Batch: 115, Loss: 0.5535, Accuracy: 66.33%, Precision: 63.59%\n",
            "Epoch: 3, Batch: 116, Loss: 0.6509, Accuracy: 66.32%, Precision: 63.59%\n",
            "Epoch: 3, Batch: 117, Loss: 0.5676, Accuracy: 66.32%, Precision: 63.60%\n",
            "Epoch: 3, Batch: 118, Loss: 0.5709, Accuracy: 66.32%, Precision: 63.61%\n",
            "Epoch: 3, Batch: 119, Loss: 0.5623, Accuracy: 66.32%, Precision: 63.60%\n",
            "Epoch: 3, Batch: 120, Loss: 0.5708, Accuracy: 66.33%, Precision: 63.61%\n",
            "Epoch: 3, Batch: 121, Loss: 0.6093, Accuracy: 66.33%, Precision: 63.61%\n",
            "Epoch: 3, Batch: 122, Loss: 0.5737, Accuracy: 66.34%, Precision: 63.60%\n",
            "Epoch: 3, Batch: 123, Loss: 0.5150, Accuracy: 66.35%, Precision: 63.61%\n",
            "Epoch: 3, Batch: 124, Loss: 0.5841, Accuracy: 66.35%, Precision: 63.59%\n",
            "Epoch: 3, Batch: 125, Loss: 0.5607, Accuracy: 66.35%, Precision: 63.60%\n",
            "Epoch: 3, Batch: 126, Loss: 0.5600, Accuracy: 66.36%, Precision: 63.61%\n",
            "Epoch: 3, Batch: 127, Loss: 0.5058, Accuracy: 66.37%, Precision: 63.62%\n",
            "Epoch: 3, Batch: 128, Loss: 0.5480, Accuracy: 66.38%, Precision: 63.63%\n",
            "Epoch: 3, Batch: 129, Loss: 0.5714, Accuracy: 66.39%, Precision: 63.64%\n",
            "Epoch: 3, Batch: 130, Loss: 0.5109, Accuracy: 66.40%, Precision: 63.66%\n",
            "Epoch: 3, Batch: 131, Loss: 0.5535, Accuracy: 66.40%, Precision: 63.66%\n",
            "Epoch: 3, Batch: 132, Loss: 0.4971, Accuracy: 66.42%, Precision: 63.68%\n",
            "Epoch: 3, Batch: 133, Loss: 0.5681, Accuracy: 66.43%, Precision: 63.69%\n",
            "Epoch: 3, Batch: 134, Loss: 0.4970, Accuracy: 66.43%, Precision: 63.70%\n",
            "Epoch: 3, Batch: 135, Loss: 0.5239, Accuracy: 66.45%, Precision: 63.72%\n",
            "Epoch: 3, Batch: 136, Loss: 0.5815, Accuracy: 66.45%, Precision: 63.72%\n",
            "Epoch: 3, Batch: 137, Loss: 0.5751, Accuracy: 66.45%, Precision: 63.72%\n",
            "Epoch: 3, Batch: 138, Loss: 0.5319, Accuracy: 66.46%, Precision: 63.74%\n",
            "Epoch: 3, Batch: 139, Loss: 0.6054, Accuracy: 66.46%, Precision: 63.73%\n",
            "Epoch: 3, Batch: 140, Loss: 0.5841, Accuracy: 66.46%, Precision: 63.73%\n",
            "Epoch: 3, Batch: 141, Loss: 0.6574, Accuracy: 66.46%, Precision: 63.74%\n",
            "Epoch: 3, Batch: 142, Loss: 0.4559, Accuracy: 66.48%, Precision: 63.76%\n",
            "Epoch: 3, Batch: 143, Loss: 0.5532, Accuracy: 66.48%, Precision: 63.74%\n",
            "Epoch: 3, Batch: 144, Loss: 0.6214, Accuracy: 66.48%, Precision: 63.75%\n",
            "Epoch: 3, Batch: 145, Loss: 0.5196, Accuracy: 66.49%, Precision: 63.76%\n",
            "Epoch: 3, Batch: 146, Loss: 0.5875, Accuracy: 66.49%, Precision: 63.75%\n",
            "Epoch: 3, Batch: 147, Loss: 0.5756, Accuracy: 66.49%, Precision: 63.76%\n",
            "Epoch: 3, Batch: 148, Loss: 0.5678, Accuracy: 66.50%, Precision: 63.76%\n",
            "Epoch: 3, Batch: 149, Loss: 0.5442, Accuracy: 66.51%, Precision: 63.76%\n",
            "Epoch: 3, Batch: 150, Loss: 0.5574, Accuracy: 66.51%, Precision: 63.76%\n",
            "Epoch: 3, Batch: 151, Loss: 0.4895, Accuracy: 66.52%, Precision: 63.78%\n",
            "Epoch: 3, Batch: 152, Loss: 0.6390, Accuracy: 66.52%, Precision: 63.76%\n",
            "Epoch: 3, Batch: 153, Loss: 0.5128, Accuracy: 66.53%, Precision: 63.77%\n",
            "Epoch: 3, Batch: 154, Loss: 0.5841, Accuracy: 66.53%, Precision: 63.77%\n",
            "Epoch: 3, Batch: 155, Loss: 0.4718, Accuracy: 66.54%, Precision: 63.77%\n",
            "Epoch: 3, Batch: 156, Loss: 0.5595, Accuracy: 66.54%, Precision: 63.78%\n",
            "Epoch: 3, Batch: 157, Loss: 0.5605, Accuracy: 66.54%, Precision: 63.79%\n",
            "Epoch: 3, Batch: 158, Loss: 0.5040, Accuracy: 66.55%, Precision: 63.81%\n",
            "Epoch: 3, Batch: 159, Loss: 0.5431, Accuracy: 66.56%, Precision: 63.81%\n",
            "Epoch: 3, Batch: 160, Loss: 0.4482, Accuracy: 66.57%, Precision: 63.82%\n",
            "Epoch: 3, Batch: 161, Loss: 0.5859, Accuracy: 66.57%, Precision: 63.83%\n",
            "Epoch: 3, Batch: 162, Loss: 0.4842, Accuracy: 66.59%, Precision: 63.83%\n",
            "Epoch: 3, Batch: 163, Loss: 0.4772, Accuracy: 66.60%, Precision: 63.85%\n",
            "Epoch: 3, Batch: 164, Loss: 0.5529, Accuracy: 66.60%, Precision: 63.85%\n",
            "Epoch: 3, Batch: 165, Loss: 0.5602, Accuracy: 66.61%, Precision: 63.86%\n",
            "Epoch: 3, Batch: 166, Loss: 0.6016, Accuracy: 66.62%, Precision: 63.87%\n",
            "Epoch: 3, Batch: 167, Loss: 0.6007, Accuracy: 66.61%, Precision: 63.86%\n",
            "Epoch: 3, Batch: 168, Loss: 0.6203, Accuracy: 66.61%, Precision: 63.85%\n",
            "Epoch: 3, Batch: 169, Loss: 0.6052, Accuracy: 66.61%, Precision: 63.85%\n",
            "Epoch: 3, Batch: 170, Loss: 0.5247, Accuracy: 66.62%, Precision: 63.85%\n",
            "Epoch: 3, Batch: 171, Loss: 0.4798, Accuracy: 66.63%, Precision: 63.86%\n",
            "Epoch: 3, Batch: 172, Loss: 0.5387, Accuracy: 66.63%, Precision: 63.87%\n",
            "Epoch: 3, Batch: 173, Loss: 0.6712, Accuracy: 66.62%, Precision: 63.87%\n",
            "Epoch: 3, Batch: 174, Loss: 0.4936, Accuracy: 66.64%, Precision: 63.88%\n",
            "Epoch: 3, Batch: 175, Loss: 0.4551, Accuracy: 66.65%, Precision: 63.89%\n",
            "Epoch: 3, Batch: 176, Loss: 0.5220, Accuracy: 66.66%, Precision: 63.90%\n",
            "Epoch: 3, Batch: 177, Loss: 0.5369, Accuracy: 66.66%, Precision: 63.90%\n",
            "Epoch: 3, Batch: 178, Loss: 0.5967, Accuracy: 66.67%, Precision: 63.91%\n",
            "Epoch: 3, Batch: 179, Loss: 0.5778, Accuracy: 66.66%, Precision: 63.90%\n",
            "Epoch: 3, Batch: 180, Loss: 0.5718, Accuracy: 66.65%, Precision: 63.88%\n",
            "Epoch: 3, Batch: 181, Loss: 0.6175, Accuracy: 66.65%, Precision: 63.88%\n",
            "Epoch: 3, Batch: 182, Loss: 0.5022, Accuracy: 66.66%, Precision: 63.89%\n",
            "Epoch: 3, Batch: 183, Loss: 0.5240, Accuracy: 66.66%, Precision: 63.89%\n",
            "Epoch: 3, Batch: 184, Loss: 0.5664, Accuracy: 66.66%, Precision: 63.90%\n",
            "Epoch: 3, Batch: 185, Loss: 0.5645, Accuracy: 66.66%, Precision: 63.89%\n",
            "Epoch: 3, Batch: 186, Loss: 0.4974, Accuracy: 66.68%, Precision: 63.90%\n",
            "Epoch: 3, Batch: 187, Loss: 0.5738, Accuracy: 66.67%, Precision: 63.89%\n",
            "Epoch: 3, Batch: 188, Loss: 0.6328, Accuracy: 66.67%, Precision: 63.91%\n",
            "Epoch: 3, Batch: 189, Loss: 0.4934, Accuracy: 66.68%, Precision: 63.92%\n",
            "Epoch: 3, Batch: 190, Loss: 0.4389, Accuracy: 66.68%, Precision: 63.93%\n",
            "Epoch: 3, Batch: 191, Loss: 0.5282, Accuracy: 66.69%, Precision: 63.93%\n",
            "Epoch: 3, Batch: 192, Loss: 0.5731, Accuracy: 66.69%, Precision: 63.93%\n",
            "Epoch: 3, Batch: 193, Loss: 0.6207, Accuracy: 66.69%, Precision: 63.93%\n",
            "Epoch: 3, Batch: 194, Loss: 0.5102, Accuracy: 66.69%, Precision: 63.94%\n",
            "Epoch: 3, Batch: 195, Loss: 0.4953, Accuracy: 66.71%, Precision: 63.96%\n",
            "Epoch: 3, Batch: 196, Loss: 0.5281, Accuracy: 66.71%, Precision: 63.97%\n",
            "Epoch: 3, Batch: 197, Loss: 0.5490, Accuracy: 66.72%, Precision: 63.96%\n",
            "Epoch: 3, Batch: 198, Loss: 0.5095, Accuracy: 66.73%, Precision: 63.97%\n",
            "Epoch: 3, Batch: 199, Loss: 0.4674, Accuracy: 66.73%, Precision: 63.97%\n",
            "Epoch: 3, Batch: 200, Loss: 0.4940, Accuracy: 66.75%, Precision: 63.98%\n",
            "Epoch: 3, Batch: 201, Loss: 0.6071, Accuracy: 66.75%, Precision: 63.97%\n",
            "Epoch: 3, Batch: 202, Loss: 0.4993, Accuracy: 66.75%, Precision: 63.99%\n",
            "Epoch: 3, Batch: 203, Loss: 0.5787, Accuracy: 66.75%, Precision: 63.99%\n",
            "Epoch: 3, Batch: 204, Loss: 0.5531, Accuracy: 66.75%, Precision: 64.00%\n",
            "Epoch: 3, Batch: 205, Loss: 0.6327, Accuracy: 66.75%, Precision: 64.01%\n",
            "Epoch: 3, Batch: 206, Loss: 0.5334, Accuracy: 66.75%, Precision: 64.02%\n",
            "Epoch: 3, Batch: 207, Loss: 0.6227, Accuracy: 66.75%, Precision: 64.01%\n",
            "Epoch: 3, Batch: 208, Loss: 0.5836, Accuracy: 66.75%, Precision: 64.00%\n",
            "Epoch: 3, Batch: 209, Loss: 0.5811, Accuracy: 66.75%, Precision: 63.99%\n",
            "Epoch: 3, Batch: 210, Loss: 0.5983, Accuracy: 66.76%, Precision: 63.99%\n",
            "Epoch: 3, Batch: 211, Loss: 0.6504, Accuracy: 66.75%, Precision: 63.98%\n",
            "Epoch: 3, Batch: 212, Loss: 0.5307, Accuracy: 66.76%, Precision: 63.98%\n",
            "Epoch: 3, Batch: 213, Loss: 0.5140, Accuracy: 66.76%, Precision: 63.98%\n",
            "Epoch: 3, Batch: 214, Loss: 0.5361, Accuracy: 66.77%, Precision: 64.00%\n",
            "Epoch: 3, Batch: 215, Loss: 0.5068, Accuracy: 66.78%, Precision: 64.00%\n",
            "Epoch: 3, Batch: 216, Loss: 0.4623, Accuracy: 66.79%, Precision: 64.00%\n",
            "Epoch: 3, Batch: 217, Loss: 0.4793, Accuracy: 66.79%, Precision: 64.01%\n",
            "Epoch: 3, Batch: 218, Loss: 0.5516, Accuracy: 66.80%, Precision: 64.02%\n",
            "Epoch: 3, Batch: 219, Loss: 0.5240, Accuracy: 66.81%, Precision: 64.02%\n",
            "Epoch: 3, Batch: 220, Loss: 0.5193, Accuracy: 66.82%, Precision: 64.03%\n",
            "Epoch: 3, Batch: 221, Loss: 0.4884, Accuracy: 66.83%, Precision: 64.04%\n",
            "Epoch: 3, Batch: 222, Loss: 0.4753, Accuracy: 66.84%, Precision: 64.05%\n",
            "Epoch: 3, Batch: 223, Loss: 0.5035, Accuracy: 66.85%, Precision: 64.07%\n",
            "Epoch: 3, Batch: 224, Loss: 0.4901, Accuracy: 66.86%, Precision: 64.09%\n",
            "Epoch: 3, Batch: 225, Loss: 0.5217, Accuracy: 66.87%, Precision: 64.11%\n",
            "Epoch: 3, Batch: 226, Loss: 0.5032, Accuracy: 66.88%, Precision: 64.11%\n",
            "Epoch: 3, Batch: 227, Loss: 0.5504, Accuracy: 66.89%, Precision: 64.12%\n",
            "Epoch: 3, Batch: 228, Loss: 0.6095, Accuracy: 66.88%, Precision: 64.11%\n",
            "Epoch: 3, Batch: 229, Loss: 0.4161, Accuracy: 66.90%, Precision: 64.14%\n",
            "Epoch: 3, Batch: 230, Loss: 0.5992, Accuracy: 66.90%, Precision: 64.14%\n",
            "Epoch: 3, Batch: 231, Loss: 0.6410, Accuracy: 66.90%, Precision: 64.14%\n",
            "Epoch: 3, Batch: 232, Loss: 0.4844, Accuracy: 66.91%, Precision: 64.14%\n",
            "Epoch: 3, Batch: 233, Loss: 0.5023, Accuracy: 66.92%, Precision: 64.15%\n",
            "Epoch: 3, Batch: 234, Loss: 0.5788, Accuracy: 66.91%, Precision: 64.14%\n",
            "Epoch: 3, Batch: 235, Loss: 0.4749, Accuracy: 66.92%, Precision: 64.15%\n",
            "Epoch: 3, Batch: 236, Loss: 0.6817, Accuracy: 66.92%, Precision: 64.16%\n",
            "Epoch: 3, Batch: 237, Loss: 0.5977, Accuracy: 66.92%, Precision: 64.16%\n",
            "Epoch: 3, Batch: 238, Loss: 0.5722, Accuracy: 66.92%, Precision: 64.16%\n",
            "Epoch: 3, Batch: 239, Loss: 0.5524, Accuracy: 66.92%, Precision: 64.16%\n",
            "Epoch: 3, Batch: 240, Loss: 0.5009, Accuracy: 66.93%, Precision: 64.17%\n",
            "Epoch: 3, Batch: 241, Loss: 0.5418, Accuracy: 66.94%, Precision: 64.18%\n",
            "Epoch: 3, Batch: 242, Loss: 0.5927, Accuracy: 66.94%, Precision: 64.18%\n",
            "Epoch: 3, Batch: 243, Loss: 0.6656, Accuracy: 66.94%, Precision: 64.17%\n",
            "Epoch: 4, Batch: 1, Loss: 0.6273, Accuracy: 66.94%, Precision: 64.16%\n",
            "Epoch: 4, Batch: 2, Loss: 0.5145, Accuracy: 66.95%, Precision: 64.17%\n",
            "Epoch: 4, Batch: 3, Loss: 0.5061, Accuracy: 66.96%, Precision: 64.18%\n",
            "Epoch: 4, Batch: 4, Loss: 0.5877, Accuracy: 66.96%, Precision: 64.19%\n",
            "Epoch: 4, Batch: 5, Loss: 0.5139, Accuracy: 66.96%, Precision: 64.21%\n",
            "Epoch: 4, Batch: 6, Loss: 0.5188, Accuracy: 66.98%, Precision: 64.23%\n",
            "Epoch: 4, Batch: 7, Loss: 0.4531, Accuracy: 66.99%, Precision: 64.25%\n",
            "Epoch: 4, Batch: 8, Loss: 0.5183, Accuracy: 67.00%, Precision: 64.26%\n",
            "Epoch: 4, Batch: 9, Loss: 0.4587, Accuracy: 67.00%, Precision: 64.28%\n",
            "Epoch: 4, Batch: 10, Loss: 0.4621, Accuracy: 67.02%, Precision: 64.29%\n",
            "Epoch: 4, Batch: 11, Loss: 0.5237, Accuracy: 67.02%, Precision: 64.29%\n",
            "Epoch: 4, Batch: 12, Loss: 0.4600, Accuracy: 67.03%, Precision: 64.30%\n",
            "Epoch: 4, Batch: 13, Loss: 0.5416, Accuracy: 67.04%, Precision: 64.30%\n",
            "Epoch: 4, Batch: 14, Loss: 0.5352, Accuracy: 67.04%, Precision: 64.30%\n",
            "Epoch: 4, Batch: 15, Loss: 0.4057, Accuracy: 67.06%, Precision: 64.33%\n",
            "Epoch: 4, Batch: 16, Loss: 0.5379, Accuracy: 67.07%, Precision: 64.33%\n",
            "Epoch: 4, Batch: 17, Loss: 0.4888, Accuracy: 67.07%, Precision: 64.33%\n",
            "Epoch: 4, Batch: 18, Loss: 0.5720, Accuracy: 67.07%, Precision: 64.33%\n",
            "Epoch: 4, Batch: 19, Loss: 0.4845, Accuracy: 67.08%, Precision: 64.34%\n",
            "Epoch: 4, Batch: 20, Loss: 0.4467, Accuracy: 67.09%, Precision: 64.35%\n",
            "Epoch: 4, Batch: 21, Loss: 0.3522, Accuracy: 67.11%, Precision: 64.37%\n",
            "Epoch: 4, Batch: 22, Loss: 0.5238, Accuracy: 67.12%, Precision: 64.37%\n",
            "Epoch: 4, Batch: 23, Loss: 0.4933, Accuracy: 67.13%, Precision: 64.38%\n",
            "Epoch: 4, Batch: 24, Loss: 0.5343, Accuracy: 67.13%, Precision: 64.39%\n",
            "Epoch: 4, Batch: 25, Loss: 0.4779, Accuracy: 67.14%, Precision: 64.39%\n",
            "Epoch: 4, Batch: 26, Loss: 0.4899, Accuracy: 67.16%, Precision: 64.41%\n",
            "Epoch: 4, Batch: 27, Loss: 0.4794, Accuracy: 67.16%, Precision: 64.42%\n",
            "Epoch: 4, Batch: 28, Loss: 0.5350, Accuracy: 67.17%, Precision: 64.42%\n",
            "Epoch: 4, Batch: 29, Loss: 0.3913, Accuracy: 67.19%, Precision: 64.45%\n",
            "Epoch: 4, Batch: 30, Loss: 0.4783, Accuracy: 67.20%, Precision: 64.46%\n",
            "Epoch: 4, Batch: 31, Loss: 0.4973, Accuracy: 67.21%, Precision: 64.46%\n",
            "Epoch: 4, Batch: 32, Loss: 0.3892, Accuracy: 67.23%, Precision: 64.48%\n",
            "Epoch: 4, Batch: 33, Loss: 0.5054, Accuracy: 67.24%, Precision: 64.48%\n",
            "Epoch: 4, Batch: 34, Loss: 0.5109, Accuracy: 67.25%, Precision: 64.50%\n",
            "Epoch: 4, Batch: 35, Loss: 0.5052, Accuracy: 67.25%, Precision: 64.51%\n",
            "Epoch: 4, Batch: 36, Loss: 0.5052, Accuracy: 67.26%, Precision: 64.52%\n",
            "Epoch: 4, Batch: 37, Loss: 0.4175, Accuracy: 67.27%, Precision: 64.53%\n",
            "Epoch: 4, Batch: 38, Loss: 0.5255, Accuracy: 67.28%, Precision: 64.53%\n",
            "Epoch: 4, Batch: 39, Loss: 0.4467, Accuracy: 67.29%, Precision: 64.54%\n",
            "Epoch: 4, Batch: 40, Loss: 0.4498, Accuracy: 67.30%, Precision: 64.54%\n",
            "Epoch: 4, Batch: 41, Loss: 0.6066, Accuracy: 67.30%, Precision: 64.54%\n",
            "Epoch: 4, Batch: 42, Loss: 0.4003, Accuracy: 67.31%, Precision: 64.54%\n",
            "Epoch: 4, Batch: 43, Loss: 0.4252, Accuracy: 67.32%, Precision: 64.55%\n",
            "Epoch: 4, Batch: 44, Loss: 0.4481, Accuracy: 67.33%, Precision: 64.56%\n",
            "Epoch: 4, Batch: 45, Loss: 0.4843, Accuracy: 67.33%, Precision: 64.56%\n",
            "Epoch: 4, Batch: 46, Loss: 0.5288, Accuracy: 67.34%, Precision: 64.57%\n",
            "Epoch: 4, Batch: 47, Loss: 0.5731, Accuracy: 67.34%, Precision: 64.58%\n",
            "Epoch: 4, Batch: 48, Loss: 0.5299, Accuracy: 67.34%, Precision: 64.60%\n",
            "Epoch: 4, Batch: 49, Loss: 0.6487, Accuracy: 67.33%, Precision: 64.59%\n",
            "Epoch: 4, Batch: 50, Loss: 0.5192, Accuracy: 67.34%, Precision: 64.60%\n",
            "Epoch: 4, Batch: 51, Loss: 0.5819, Accuracy: 67.34%, Precision: 64.59%\n",
            "Epoch: 4, Batch: 52, Loss: 0.6172, Accuracy: 67.34%, Precision: 64.58%\n",
            "Epoch: 4, Batch: 53, Loss: 0.6271, Accuracy: 67.34%, Precision: 64.57%\n",
            "Epoch: 4, Batch: 54, Loss: 0.4813, Accuracy: 67.35%, Precision: 64.58%\n",
            "Epoch: 4, Batch: 55, Loss: 0.4896, Accuracy: 67.36%, Precision: 64.58%\n",
            "Epoch: 4, Batch: 56, Loss: 0.5167, Accuracy: 67.36%, Precision: 64.58%\n",
            "Epoch: 4, Batch: 57, Loss: 0.5444, Accuracy: 67.36%, Precision: 64.59%\n",
            "Epoch: 4, Batch: 58, Loss: 0.5074, Accuracy: 67.37%, Precision: 64.61%\n",
            "Epoch: 4, Batch: 59, Loss: 0.5054, Accuracy: 67.38%, Precision: 64.61%\n",
            "Epoch: 4, Batch: 60, Loss: 0.4710, Accuracy: 67.39%, Precision: 64.63%\n",
            "Epoch: 4, Batch: 61, Loss: 0.4871, Accuracy: 67.40%, Precision: 64.65%\n",
            "Epoch: 4, Batch: 62, Loss: 0.5271, Accuracy: 67.40%, Precision: 64.65%\n",
            "Epoch: 4, Batch: 63, Loss: 0.5701, Accuracy: 67.41%, Precision: 64.66%\n",
            "Epoch: 4, Batch: 64, Loss: 0.4926, Accuracy: 67.42%, Precision: 64.68%\n",
            "Epoch: 4, Batch: 65, Loss: 0.4485, Accuracy: 67.43%, Precision: 64.69%\n",
            "Epoch: 4, Batch: 66, Loss: 0.5130, Accuracy: 67.43%, Precision: 64.69%\n",
            "Epoch: 4, Batch: 67, Loss: 0.4031, Accuracy: 67.45%, Precision: 64.71%\n",
            "Epoch: 4, Batch: 68, Loss: 0.5460, Accuracy: 67.46%, Precision: 64.72%\n",
            "Epoch: 4, Batch: 69, Loss: 0.5021, Accuracy: 67.46%, Precision: 64.73%\n",
            "Epoch: 4, Batch: 70, Loss: 0.4738, Accuracy: 67.47%, Precision: 64.73%\n",
            "Epoch: 4, Batch: 71, Loss: 0.5885, Accuracy: 67.47%, Precision: 64.73%\n",
            "Epoch: 4, Batch: 72, Loss: 0.5500, Accuracy: 67.48%, Precision: 64.74%\n",
            "Epoch: 4, Batch: 73, Loss: 0.4987, Accuracy: 67.49%, Precision: 64.76%\n",
            "Epoch: 4, Batch: 74, Loss: 0.5861, Accuracy: 67.49%, Precision: 64.75%\n",
            "Epoch: 4, Batch: 75, Loss: 0.5553, Accuracy: 67.49%, Precision: 64.75%\n",
            "Epoch: 4, Batch: 76, Loss: 0.5552, Accuracy: 67.49%, Precision: 64.75%\n",
            "Epoch: 4, Batch: 77, Loss: 0.5065, Accuracy: 67.49%, Precision: 64.75%\n",
            "Epoch: 4, Batch: 78, Loss: 0.5590, Accuracy: 67.49%, Precision: 64.74%\n",
            "Epoch: 4, Batch: 79, Loss: 0.4972, Accuracy: 67.50%, Precision: 64.75%\n",
            "Epoch: 4, Batch: 80, Loss: 0.6347, Accuracy: 67.50%, Precision: 64.76%\n",
            "Epoch: 4, Batch: 81, Loss: 0.5621, Accuracy: 67.49%, Precision: 64.77%\n",
            "Epoch: 4, Batch: 82, Loss: 0.4487, Accuracy: 67.50%, Precision: 64.78%\n",
            "Epoch: 4, Batch: 83, Loss: 0.5494, Accuracy: 67.51%, Precision: 64.78%\n",
            "Epoch: 4, Batch: 84, Loss: 0.4569, Accuracy: 67.52%, Precision: 64.78%\n",
            "Epoch: 4, Batch: 85, Loss: 0.6431, Accuracy: 67.51%, Precision: 64.79%\n",
            "Epoch: 4, Batch: 86, Loss: 0.4363, Accuracy: 67.52%, Precision: 64.79%\n",
            "Epoch: 4, Batch: 87, Loss: 0.4382, Accuracy: 67.53%, Precision: 64.80%\n",
            "Epoch: 4, Batch: 88, Loss: 0.5535, Accuracy: 67.53%, Precision: 64.80%\n",
            "Epoch: 4, Batch: 89, Loss: 0.5722, Accuracy: 67.53%, Precision: 64.80%\n",
            "Epoch: 4, Batch: 90, Loss: 0.5782, Accuracy: 67.54%, Precision: 64.80%\n",
            "Epoch: 4, Batch: 91, Loss: 0.5013, Accuracy: 67.54%, Precision: 64.81%\n",
            "Epoch: 4, Batch: 92, Loss: 0.5380, Accuracy: 67.54%, Precision: 64.80%\n",
            "Epoch: 4, Batch: 93, Loss: 0.5859, Accuracy: 67.54%, Precision: 64.80%\n",
            "Epoch: 4, Batch: 94, Loss: 0.4222, Accuracy: 67.56%, Precision: 64.81%\n",
            "Epoch: 4, Batch: 95, Loss: 0.5026, Accuracy: 67.56%, Precision: 64.83%\n",
            "Epoch: 4, Batch: 96, Loss: 0.5434, Accuracy: 67.56%, Precision: 64.82%\n",
            "Epoch: 4, Batch: 97, Loss: 0.4958, Accuracy: 67.57%, Precision: 64.83%\n",
            "Epoch: 4, Batch: 98, Loss: 0.5967, Accuracy: 67.57%, Precision: 64.83%\n",
            "Epoch: 4, Batch: 99, Loss: 0.4009, Accuracy: 67.59%, Precision: 64.85%\n",
            "Epoch: 4, Batch: 100, Loss: 0.5351, Accuracy: 67.59%, Precision: 64.85%\n",
            "Epoch: 4, Batch: 101, Loss: 0.3867, Accuracy: 67.61%, Precision: 64.85%\n",
            "Epoch: 4, Batch: 102, Loss: 0.4641, Accuracy: 67.62%, Precision: 64.86%\n",
            "Epoch: 4, Batch: 103, Loss: 0.4730, Accuracy: 67.63%, Precision: 64.88%\n",
            "Epoch: 4, Batch: 104, Loss: 0.6445, Accuracy: 67.63%, Precision: 64.88%\n",
            "Epoch: 4, Batch: 105, Loss: 0.4394, Accuracy: 67.64%, Precision: 64.89%\n",
            "Epoch: 4, Batch: 106, Loss: 0.5530, Accuracy: 67.65%, Precision: 64.89%\n",
            "Epoch: 4, Batch: 107, Loss: 0.5888, Accuracy: 67.65%, Precision: 64.90%\n",
            "Epoch: 4, Batch: 108, Loss: 0.4747, Accuracy: 67.66%, Precision: 64.90%\n",
            "Epoch: 4, Batch: 109, Loss: 0.4562, Accuracy: 67.67%, Precision: 64.91%\n",
            "Epoch: 4, Batch: 110, Loss: 0.5535, Accuracy: 67.67%, Precision: 64.90%\n",
            "Epoch: 4, Batch: 111, Loss: 0.4923, Accuracy: 67.67%, Precision: 64.90%\n",
            "Epoch: 4, Batch: 112, Loss: 0.5424, Accuracy: 67.67%, Precision: 64.90%\n",
            "Epoch: 4, Batch: 113, Loss: 0.5411, Accuracy: 67.67%, Precision: 64.90%\n",
            "Epoch: 4, Batch: 114, Loss: 0.4587, Accuracy: 67.69%, Precision: 64.92%\n",
            "Epoch: 4, Batch: 115, Loss: 0.5260, Accuracy: 67.68%, Precision: 64.92%\n",
            "Epoch: 4, Batch: 116, Loss: 0.7023, Accuracy: 67.68%, Precision: 64.91%\n",
            "Epoch: 4, Batch: 117, Loss: 0.5032, Accuracy: 67.69%, Precision: 64.92%\n",
            "Epoch: 4, Batch: 118, Loss: 0.5374, Accuracy: 67.69%, Precision: 64.92%\n",
            "Epoch: 4, Batch: 119, Loss: 0.5154, Accuracy: 67.69%, Precision: 64.92%\n",
            "Epoch: 4, Batch: 120, Loss: 0.4221, Accuracy: 67.71%, Precision: 64.95%\n",
            "Epoch: 4, Batch: 121, Loss: 0.4060, Accuracy: 67.72%, Precision: 64.95%\n",
            "Epoch: 4, Batch: 122, Loss: 0.4733, Accuracy: 67.73%, Precision: 64.96%\n",
            "Epoch: 4, Batch: 123, Loss: 0.3977, Accuracy: 67.74%, Precision: 64.97%\n",
            "Epoch: 4, Batch: 124, Loss: 0.5076, Accuracy: 67.75%, Precision: 64.97%\n",
            "Epoch: 4, Batch: 125, Loss: 0.5237, Accuracy: 67.76%, Precision: 64.97%\n",
            "Epoch: 4, Batch: 126, Loss: 0.5421, Accuracy: 67.76%, Precision: 64.99%\n",
            "Epoch: 4, Batch: 127, Loss: 0.5308, Accuracy: 67.77%, Precision: 64.99%\n",
            "Epoch: 4, Batch: 128, Loss: 0.4112, Accuracy: 67.78%, Precision: 65.00%\n",
            "Epoch: 4, Batch: 129, Loss: 0.4907, Accuracy: 67.79%, Precision: 65.01%\n",
            "Epoch: 4, Batch: 130, Loss: 0.4288, Accuracy: 67.80%, Precision: 65.01%\n",
            "Epoch: 4, Batch: 131, Loss: 0.5447, Accuracy: 67.81%, Precision: 65.01%\n",
            "Epoch: 4, Batch: 132, Loss: 0.4347, Accuracy: 67.82%, Precision: 65.02%\n",
            "Epoch: 4, Batch: 133, Loss: 0.4699, Accuracy: 67.83%, Precision: 65.03%\n",
            "Epoch: 4, Batch: 134, Loss: 0.5830, Accuracy: 67.83%, Precision: 65.05%\n",
            "Epoch: 4, Batch: 135, Loss: 0.5586, Accuracy: 67.83%, Precision: 65.05%\n",
            "Epoch: 4, Batch: 136, Loss: 0.5032, Accuracy: 67.83%, Precision: 65.06%\n",
            "Epoch: 4, Batch: 137, Loss: 0.5271, Accuracy: 67.84%, Precision: 65.07%\n",
            "Epoch: 4, Batch: 138, Loss: 0.5350, Accuracy: 67.85%, Precision: 65.07%\n",
            "Epoch: 4, Batch: 139, Loss: 0.4971, Accuracy: 67.85%, Precision: 65.08%\n",
            "Epoch: 4, Batch: 140, Loss: 0.5193, Accuracy: 67.85%, Precision: 65.08%\n",
            "Epoch: 4, Batch: 141, Loss: 0.3748, Accuracy: 67.87%, Precision: 65.09%\n",
            "Epoch: 4, Batch: 142, Loss: 0.5263, Accuracy: 67.87%, Precision: 65.09%\n",
            "Epoch: 4, Batch: 143, Loss: 0.6408, Accuracy: 67.86%, Precision: 65.09%\n",
            "Epoch: 4, Batch: 144, Loss: 0.4934, Accuracy: 67.87%, Precision: 65.09%\n",
            "Epoch: 4, Batch: 145, Loss: 0.4458, Accuracy: 67.88%, Precision: 65.10%\n",
            "Epoch: 4, Batch: 146, Loss: 0.5593, Accuracy: 67.88%, Precision: 65.10%\n",
            "Epoch: 4, Batch: 147, Loss: 0.5155, Accuracy: 67.89%, Precision: 65.10%\n",
            "Epoch: 4, Batch: 148, Loss: 0.5474, Accuracy: 67.88%, Precision: 65.10%\n",
            "Epoch: 4, Batch: 149, Loss: 0.5610, Accuracy: 67.88%, Precision: 65.10%\n",
            "Epoch: 4, Batch: 150, Loss: 0.4990, Accuracy: 67.89%, Precision: 65.11%\n",
            "Epoch: 4, Batch: 151, Loss: 0.5165, Accuracy: 67.89%, Precision: 65.11%\n",
            "Epoch: 4, Batch: 152, Loss: 0.5354, Accuracy: 67.90%, Precision: 65.12%\n",
            "Epoch: 4, Batch: 153, Loss: 0.4817, Accuracy: 67.90%, Precision: 65.13%\n",
            "Epoch: 4, Batch: 154, Loss: 0.5823, Accuracy: 67.90%, Precision: 65.11%\n",
            "Epoch: 4, Batch: 155, Loss: 0.6150, Accuracy: 67.90%, Precision: 65.11%\n",
            "Epoch: 4, Batch: 156, Loss: 0.5958, Accuracy: 67.90%, Precision: 65.11%\n",
            "Epoch: 4, Batch: 157, Loss: 0.5171, Accuracy: 67.90%, Precision: 65.11%\n",
            "Epoch: 4, Batch: 158, Loss: 0.5415, Accuracy: 67.91%, Precision: 65.11%\n",
            "Epoch: 4, Batch: 159, Loss: 0.5537, Accuracy: 67.90%, Precision: 65.11%\n",
            "Epoch: 4, Batch: 160, Loss: 0.5817, Accuracy: 67.90%, Precision: 65.11%\n",
            "Epoch: 4, Batch: 161, Loss: 0.6051, Accuracy: 67.90%, Precision: 65.12%\n",
            "Epoch: 4, Batch: 162, Loss: 0.4966, Accuracy: 67.90%, Precision: 65.12%\n",
            "Epoch: 4, Batch: 163, Loss: 0.5716, Accuracy: 67.91%, Precision: 65.13%\n",
            "Epoch: 4, Batch: 164, Loss: 0.4107, Accuracy: 67.92%, Precision: 65.14%\n",
            "Epoch: 4, Batch: 165, Loss: 0.5224, Accuracy: 67.92%, Precision: 65.13%\n",
            "Epoch: 4, Batch: 166, Loss: 0.5897, Accuracy: 67.92%, Precision: 65.13%\n",
            "Epoch: 4, Batch: 167, Loss: 0.5082, Accuracy: 67.93%, Precision: 65.12%\n",
            "Epoch: 4, Batch: 168, Loss: 0.5098, Accuracy: 67.93%, Precision: 65.12%\n",
            "Epoch: 4, Batch: 169, Loss: 0.4666, Accuracy: 67.94%, Precision: 65.13%\n",
            "Epoch: 4, Batch: 170, Loss: 0.4883, Accuracy: 67.94%, Precision: 65.13%\n",
            "Epoch: 4, Batch: 171, Loss: 0.5436, Accuracy: 67.95%, Precision: 65.14%\n",
            "Epoch: 4, Batch: 172, Loss: 0.5205, Accuracy: 67.96%, Precision: 65.14%\n",
            "Epoch: 4, Batch: 173, Loss: 0.4083, Accuracy: 67.97%, Precision: 65.16%\n",
            "Epoch: 4, Batch: 174, Loss: 0.5026, Accuracy: 67.98%, Precision: 65.17%\n",
            "Epoch: 4, Batch: 175, Loss: 0.4125, Accuracy: 67.99%, Precision: 65.18%\n",
            "Epoch: 4, Batch: 176, Loss: 0.5446, Accuracy: 67.99%, Precision: 65.18%\n",
            "Epoch: 4, Batch: 177, Loss: 0.5824, Accuracy: 67.99%, Precision: 65.18%\n",
            "Epoch: 4, Batch: 178, Loss: 0.4317, Accuracy: 68.00%, Precision: 65.19%\n",
            "Epoch: 4, Batch: 179, Loss: 0.6038, Accuracy: 68.00%, Precision: 65.19%\n",
            "Epoch: 4, Batch: 180, Loss: 0.5183, Accuracy: 68.00%, Precision: 65.20%\n",
            "Epoch: 4, Batch: 181, Loss: 0.4527, Accuracy: 68.01%, Precision: 65.21%\n",
            "Epoch: 4, Batch: 182, Loss: 0.5308, Accuracy: 68.01%, Precision: 65.21%\n",
            "Epoch: 4, Batch: 183, Loss: 0.4256, Accuracy: 68.03%, Precision: 65.22%\n",
            "Epoch: 4, Batch: 184, Loss: 0.4834, Accuracy: 68.04%, Precision: 65.23%\n",
            "Epoch: 4, Batch: 185, Loss: 0.4912, Accuracy: 68.04%, Precision: 65.23%\n",
            "Epoch: 4, Batch: 186, Loss: 0.4659, Accuracy: 68.05%, Precision: 65.24%\n",
            "Epoch: 4, Batch: 187, Loss: 0.4839, Accuracy: 68.06%, Precision: 65.25%\n",
            "Epoch: 4, Batch: 188, Loss: 0.5098, Accuracy: 68.06%, Precision: 65.26%\n",
            "Epoch: 4, Batch: 189, Loss: 0.4800, Accuracy: 68.07%, Precision: 65.27%\n",
            "Epoch: 4, Batch: 190, Loss: 0.5054, Accuracy: 68.08%, Precision: 65.27%\n",
            "Epoch: 4, Batch: 191, Loss: 0.3697, Accuracy: 68.09%, Precision: 65.28%\n",
            "Epoch: 4, Batch: 192, Loss: 0.4842, Accuracy: 68.10%, Precision: 65.29%\n",
            "Epoch: 4, Batch: 193, Loss: 0.4082, Accuracy: 68.11%, Precision: 65.30%\n",
            "Epoch: 4, Batch: 194, Loss: 0.4801, Accuracy: 68.12%, Precision: 65.30%\n",
            "Epoch: 4, Batch: 195, Loss: 0.5019, Accuracy: 68.13%, Precision: 65.30%\n",
            "Epoch: 4, Batch: 196, Loss: 0.5096, Accuracy: 68.13%, Precision: 65.31%\n",
            "Epoch: 4, Batch: 197, Loss: 0.6075, Accuracy: 68.13%, Precision: 65.30%\n",
            "Epoch: 4, Batch: 198, Loss: 0.6463, Accuracy: 68.13%, Precision: 65.30%\n",
            "Epoch: 4, Batch: 199, Loss: 0.4916, Accuracy: 68.13%, Precision: 65.30%\n",
            "Epoch: 4, Batch: 200, Loss: 0.4901, Accuracy: 68.14%, Precision: 65.31%\n",
            "Epoch: 4, Batch: 201, Loss: 0.5770, Accuracy: 68.14%, Precision: 65.31%\n",
            "Epoch: 4, Batch: 202, Loss: 0.7431, Accuracy: 68.14%, Precision: 65.32%\n",
            "Epoch: 4, Batch: 203, Loss: 0.4709, Accuracy: 68.14%, Precision: 65.33%\n",
            "Epoch: 4, Batch: 204, Loss: 0.4856, Accuracy: 68.15%, Precision: 65.35%\n",
            "Epoch: 4, Batch: 205, Loss: 0.4682, Accuracy: 68.16%, Precision: 65.35%\n",
            "Epoch: 4, Batch: 206, Loss: 0.4507, Accuracy: 68.17%, Precision: 65.36%\n",
            "Epoch: 4, Batch: 207, Loss: 0.5349, Accuracy: 68.17%, Precision: 65.36%\n",
            "Epoch: 4, Batch: 208, Loss: 0.6080, Accuracy: 68.16%, Precision: 65.34%\n",
            "Epoch: 4, Batch: 209, Loss: 0.5736, Accuracy: 68.16%, Precision: 65.33%\n",
            "Epoch: 4, Batch: 210, Loss: 0.5639, Accuracy: 68.16%, Precision: 65.33%\n",
            "Epoch: 4, Batch: 211, Loss: 0.4796, Accuracy: 68.16%, Precision: 65.33%\n",
            "Epoch: 4, Batch: 212, Loss: 0.5081, Accuracy: 68.17%, Precision: 65.34%\n",
            "Epoch: 4, Batch: 213, Loss: 0.5939, Accuracy: 68.17%, Precision: 65.34%\n",
            "Epoch: 4, Batch: 214, Loss: 0.4530, Accuracy: 68.18%, Precision: 65.35%\n",
            "Epoch: 4, Batch: 215, Loss: 0.5308, Accuracy: 68.18%, Precision: 65.36%\n",
            "Epoch: 4, Batch: 216, Loss: 0.5115, Accuracy: 68.19%, Precision: 65.37%\n",
            "Epoch: 4, Batch: 217, Loss: 0.4193, Accuracy: 68.21%, Precision: 65.38%\n",
            "Epoch: 4, Batch: 218, Loss: 0.5190, Accuracy: 68.21%, Precision: 65.39%\n",
            "Epoch: 4, Batch: 219, Loss: 0.5260, Accuracy: 68.21%, Precision: 65.39%\n",
            "Epoch: 4, Batch: 220, Loss: 0.5055, Accuracy: 68.22%, Precision: 65.40%\n",
            "Epoch: 4, Batch: 221, Loss: 0.4780, Accuracy: 68.23%, Precision: 65.41%\n",
            "Epoch: 4, Batch: 222, Loss: 0.4693, Accuracy: 68.23%, Precision: 65.42%\n",
            "Epoch: 4, Batch: 223, Loss: 0.4810, Accuracy: 68.24%, Precision: 65.43%\n",
            "Epoch: 4, Batch: 224, Loss: 0.5952, Accuracy: 68.24%, Precision: 65.42%\n",
            "Epoch: 4, Batch: 225, Loss: 0.5840, Accuracy: 68.24%, Precision: 65.42%\n",
            "Epoch: 4, Batch: 226, Loss: 0.6273, Accuracy: 68.23%, Precision: 65.41%\n",
            "Epoch: 4, Batch: 227, Loss: 0.5381, Accuracy: 68.24%, Precision: 65.42%\n",
            "Epoch: 4, Batch: 228, Loss: 0.6689, Accuracy: 68.23%, Precision: 65.42%\n",
            "Epoch: 4, Batch: 229, Loss: 0.4760, Accuracy: 68.23%, Precision: 65.42%\n",
            "Epoch: 4, Batch: 230, Loss: 0.5310, Accuracy: 68.24%, Precision: 65.43%\n",
            "Epoch: 4, Batch: 231, Loss: 0.5803, Accuracy: 68.24%, Precision: 65.44%\n",
            "Epoch: 4, Batch: 232, Loss: 0.5547, Accuracy: 68.24%, Precision: 65.45%\n",
            "Epoch: 4, Batch: 233, Loss: 0.4463, Accuracy: 68.25%, Precision: 65.46%\n",
            "Epoch: 4, Batch: 234, Loss: 0.5260, Accuracy: 68.25%, Precision: 65.46%\n",
            "Epoch: 4, Batch: 235, Loss: 0.4565, Accuracy: 68.26%, Precision: 65.46%\n",
            "Epoch: 4, Batch: 236, Loss: 0.5130, Accuracy: 68.26%, Precision: 65.47%\n",
            "Epoch: 4, Batch: 237, Loss: 0.4796, Accuracy: 68.27%, Precision: 65.48%\n",
            "Epoch: 4, Batch: 238, Loss: 0.5446, Accuracy: 68.27%, Precision: 65.49%\n",
            "Epoch: 4, Batch: 239, Loss: 0.5815, Accuracy: 68.28%, Precision: 65.48%\n",
            "Epoch: 4, Batch: 240, Loss: 0.5446, Accuracy: 68.27%, Precision: 65.47%\n",
            "Epoch: 4, Batch: 241, Loss: 0.4209, Accuracy: 68.28%, Precision: 65.48%\n",
            "Epoch: 4, Batch: 242, Loss: 0.5348, Accuracy: 68.29%, Precision: 65.49%\n",
            "Epoch: 4, Batch: 243, Loss: 0.3580, Accuracy: 68.29%, Precision: 65.49%\n",
            "Epoch: 5, Batch: 1, Loss: 0.5011, Accuracy: 68.30%, Precision: 65.49%\n",
            "Epoch: 5, Batch: 2, Loss: 0.4834, Accuracy: 68.30%, Precision: 65.50%\n",
            "Epoch: 5, Batch: 3, Loss: 0.5149, Accuracy: 68.31%, Precision: 65.51%\n",
            "Epoch: 5, Batch: 4, Loss: 0.4058, Accuracy: 68.32%, Precision: 65.52%\n",
            "Epoch: 5, Batch: 5, Loss: 0.4244, Accuracy: 68.33%, Precision: 65.52%\n",
            "Epoch: 5, Batch: 6, Loss: 0.5222, Accuracy: 68.34%, Precision: 65.53%\n",
            "Epoch: 5, Batch: 7, Loss: 0.5479, Accuracy: 68.34%, Precision: 65.53%\n",
            "Epoch: 5, Batch: 8, Loss: 0.4182, Accuracy: 68.35%, Precision: 65.54%\n",
            "Epoch: 5, Batch: 9, Loss: 0.5222, Accuracy: 68.35%, Precision: 65.55%\n",
            "Epoch: 5, Batch: 10, Loss: 0.3521, Accuracy: 68.37%, Precision: 65.56%\n",
            "Epoch: 5, Batch: 11, Loss: 0.5315, Accuracy: 68.37%, Precision: 65.56%\n",
            "Epoch: 5, Batch: 12, Loss: 0.4036, Accuracy: 68.37%, Precision: 65.56%\n",
            "Epoch: 5, Batch: 13, Loss: 0.3968, Accuracy: 68.38%, Precision: 65.57%\n",
            "Epoch: 5, Batch: 14, Loss: 0.3899, Accuracy: 68.39%, Precision: 65.58%\n",
            "Epoch: 5, Batch: 15, Loss: 0.4307, Accuracy: 68.40%, Precision: 65.59%\n",
            "Epoch: 5, Batch: 16, Loss: 0.4941, Accuracy: 68.41%, Precision: 65.61%\n",
            "Epoch: 5, Batch: 17, Loss: 0.4330, Accuracy: 68.42%, Precision: 65.62%\n",
            "Epoch: 5, Batch: 18, Loss: 0.4873, Accuracy: 68.43%, Precision: 65.62%\n",
            "Epoch: 5, Batch: 19, Loss: 0.4911, Accuracy: 68.44%, Precision: 65.63%\n",
            "Epoch: 5, Batch: 20, Loss: 0.4428, Accuracy: 68.45%, Precision: 65.63%\n",
            "Epoch: 5, Batch: 21, Loss: 0.4519, Accuracy: 68.46%, Precision: 65.65%\n",
            "Epoch: 5, Batch: 22, Loss: 0.5392, Accuracy: 68.46%, Precision: 65.64%\n",
            "Epoch: 5, Batch: 23, Loss: 0.4257, Accuracy: 68.47%, Precision: 65.65%\n",
            "Epoch: 5, Batch: 24, Loss: 0.3720, Accuracy: 68.48%, Precision: 65.66%\n",
            "Epoch: 5, Batch: 25, Loss: 0.5355, Accuracy: 68.48%, Precision: 65.67%\n",
            "Epoch: 5, Batch: 26, Loss: 0.5506, Accuracy: 68.48%, Precision: 65.67%\n",
            "Epoch: 5, Batch: 27, Loss: 0.4443, Accuracy: 68.49%, Precision: 65.68%\n",
            "Epoch: 5, Batch: 28, Loss: 0.4783, Accuracy: 68.50%, Precision: 65.70%\n",
            "Epoch: 5, Batch: 29, Loss: 0.4137, Accuracy: 68.51%, Precision: 65.70%\n",
            "Epoch: 5, Batch: 30, Loss: 0.4124, Accuracy: 68.52%, Precision: 65.71%\n",
            "Epoch: 5, Batch: 31, Loss: 0.4421, Accuracy: 68.52%, Precision: 65.72%\n",
            "Epoch: 5, Batch: 32, Loss: 0.4141, Accuracy: 68.53%, Precision: 65.73%\n",
            "Epoch: 5, Batch: 33, Loss: 0.4868, Accuracy: 68.54%, Precision: 65.73%\n",
            "Epoch: 5, Batch: 34, Loss: 0.4493, Accuracy: 68.55%, Precision: 65.74%\n",
            "Epoch: 5, Batch: 35, Loss: 0.4734, Accuracy: 68.56%, Precision: 65.74%\n",
            "Epoch: 5, Batch: 36, Loss: 0.4059, Accuracy: 68.57%, Precision: 65.75%\n",
            "Epoch: 5, Batch: 37, Loss: 0.3509, Accuracy: 68.58%, Precision: 65.76%\n",
            "Epoch: 5, Batch: 38, Loss: 0.4941, Accuracy: 68.59%, Precision: 65.78%\n",
            "Epoch: 5, Batch: 39, Loss: 0.5265, Accuracy: 68.60%, Precision: 65.79%\n",
            "Epoch: 5, Batch: 40, Loss: 0.4321, Accuracy: 68.60%, Precision: 65.80%\n",
            "Epoch: 5, Batch: 41, Loss: 0.4384, Accuracy: 68.61%, Precision: 65.81%\n",
            "Epoch: 5, Batch: 42, Loss: 0.4104, Accuracy: 68.62%, Precision: 65.81%\n",
            "Epoch: 5, Batch: 43, Loss: 0.3976, Accuracy: 68.63%, Precision: 65.83%\n",
            "Epoch: 5, Batch: 44, Loss: 0.4254, Accuracy: 68.64%, Precision: 65.84%\n",
            "Epoch: 5, Batch: 45, Loss: 0.4238, Accuracy: 68.65%, Precision: 65.85%\n",
            "Epoch: 5, Batch: 46, Loss: 0.6235, Accuracy: 68.65%, Precision: 65.84%\n",
            "Epoch: 5, Batch: 47, Loss: 0.5036, Accuracy: 68.65%, Precision: 65.84%\n",
            "Epoch: 5, Batch: 48, Loss: 0.4517, Accuracy: 68.66%, Precision: 65.84%\n",
            "Epoch: 5, Batch: 49, Loss: 0.4077, Accuracy: 68.67%, Precision: 65.85%\n",
            "Epoch: 5, Batch: 50, Loss: 0.3847, Accuracy: 68.68%, Precision: 65.86%\n",
            "Epoch: 5, Batch: 51, Loss: 0.5003, Accuracy: 68.68%, Precision: 65.87%\n",
            "Epoch: 5, Batch: 52, Loss: 0.5155, Accuracy: 68.68%, Precision: 65.87%\n",
            "Epoch: 5, Batch: 53, Loss: 0.4722, Accuracy: 68.69%, Precision: 65.88%\n",
            "Epoch: 5, Batch: 54, Loss: 0.3907, Accuracy: 68.70%, Precision: 65.89%\n",
            "Epoch: 5, Batch: 55, Loss: 0.5715, Accuracy: 68.70%, Precision: 65.89%\n",
            "Epoch: 5, Batch: 56, Loss: 0.5347, Accuracy: 68.70%, Precision: 65.89%\n",
            "Epoch: 5, Batch: 57, Loss: 0.4725, Accuracy: 68.70%, Precision: 65.89%\n",
            "Epoch: 5, Batch: 58, Loss: 0.3930, Accuracy: 68.72%, Precision: 65.90%\n",
            "Epoch: 5, Batch: 59, Loss: 0.4792, Accuracy: 68.72%, Precision: 65.91%\n",
            "Epoch: 5, Batch: 60, Loss: 0.5335, Accuracy: 68.72%, Precision: 65.91%\n",
            "Epoch: 5, Batch: 61, Loss: 0.4692, Accuracy: 68.73%, Precision: 65.92%\n",
            "Epoch: 5, Batch: 62, Loss: 0.5475, Accuracy: 68.73%, Precision: 65.92%\n",
            "Epoch: 5, Batch: 63, Loss: 0.4581, Accuracy: 68.73%, Precision: 65.92%\n",
            "Epoch: 5, Batch: 64, Loss: 0.4587, Accuracy: 68.73%, Precision: 65.91%\n",
            "Epoch: 5, Batch: 65, Loss: 0.5370, Accuracy: 68.73%, Precision: 65.91%\n",
            "Epoch: 5, Batch: 66, Loss: 0.4108, Accuracy: 68.74%, Precision: 65.91%\n",
            "Epoch: 5, Batch: 67, Loss: 0.5772, Accuracy: 68.74%, Precision: 65.91%\n",
            "Epoch: 5, Batch: 68, Loss: 0.4206, Accuracy: 68.75%, Precision: 65.93%\n",
            "Epoch: 5, Batch: 69, Loss: 0.5193, Accuracy: 68.75%, Precision: 65.92%\n",
            "Epoch: 5, Batch: 70, Loss: 0.3779, Accuracy: 68.76%, Precision: 65.93%\n",
            "Epoch: 5, Batch: 71, Loss: 0.4270, Accuracy: 68.77%, Precision: 65.94%\n",
            "Epoch: 5, Batch: 72, Loss: 0.4842, Accuracy: 68.77%, Precision: 65.95%\n",
            "Epoch: 5, Batch: 73, Loss: 0.4999, Accuracy: 68.77%, Precision: 65.95%\n",
            "Epoch: 5, Batch: 74, Loss: 0.4683, Accuracy: 68.77%, Precision: 65.95%\n",
            "Epoch: 5, Batch: 75, Loss: 0.4010, Accuracy: 68.78%, Precision: 65.97%\n",
            "Epoch: 5, Batch: 76, Loss: 0.3539, Accuracy: 68.80%, Precision: 65.98%\n",
            "Epoch: 5, Batch: 77, Loss: 0.4117, Accuracy: 68.81%, Precision: 65.99%\n",
            "Epoch: 5, Batch: 78, Loss: 0.4525, Accuracy: 68.82%, Precision: 66.00%\n",
            "Epoch: 5, Batch: 79, Loss: 0.4625, Accuracy: 68.82%, Precision: 66.01%\n",
            "Epoch: 5, Batch: 80, Loss: 0.4261, Accuracy: 68.83%, Precision: 66.02%\n",
            "Epoch: 5, Batch: 81, Loss: 0.3887, Accuracy: 68.84%, Precision: 66.03%\n",
            "Epoch: 5, Batch: 82, Loss: 0.3909, Accuracy: 68.85%, Precision: 66.04%\n",
            "Epoch: 5, Batch: 83, Loss: 0.4634, Accuracy: 68.86%, Precision: 66.05%\n",
            "Epoch: 5, Batch: 84, Loss: 0.3757, Accuracy: 68.87%, Precision: 66.06%\n",
            "Epoch: 5, Batch: 85, Loss: 0.5171, Accuracy: 68.87%, Precision: 66.07%\n",
            "Epoch: 5, Batch: 86, Loss: 0.3864, Accuracy: 68.88%, Precision: 66.08%\n",
            "Epoch: 5, Batch: 87, Loss: 0.4987, Accuracy: 68.89%, Precision: 66.09%\n",
            "Epoch: 5, Batch: 88, Loss: 0.5746, Accuracy: 68.89%, Precision: 66.09%\n",
            "Epoch: 5, Batch: 89, Loss: 0.5094, Accuracy: 68.90%, Precision: 66.10%\n",
            "Epoch: 5, Batch: 90, Loss: 0.4773, Accuracy: 68.90%, Precision: 66.09%\n",
            "Epoch: 5, Batch: 91, Loss: 0.4555, Accuracy: 68.91%, Precision: 66.11%\n",
            "Epoch: 5, Batch: 92, Loss: 0.5009, Accuracy: 68.91%, Precision: 66.10%\n",
            "Epoch: 5, Batch: 93, Loss: 0.4583, Accuracy: 68.92%, Precision: 66.11%\n",
            "Epoch: 5, Batch: 94, Loss: 0.3297, Accuracy: 68.93%, Precision: 66.13%\n",
            "Epoch: 5, Batch: 95, Loss: 0.4721, Accuracy: 68.93%, Precision: 66.13%\n",
            "Epoch: 5, Batch: 96, Loss: 0.3875, Accuracy: 68.94%, Precision: 66.15%\n",
            "Epoch: 5, Batch: 97, Loss: 0.4909, Accuracy: 68.95%, Precision: 66.15%\n",
            "Epoch: 5, Batch: 98, Loss: 0.4648, Accuracy: 68.95%, Precision: 66.15%\n",
            "Epoch: 5, Batch: 99, Loss: 0.4367, Accuracy: 68.96%, Precision: 66.16%\n",
            "Epoch: 5, Batch: 100, Loss: 0.3949, Accuracy: 68.97%, Precision: 66.17%\n",
            "Epoch: 5, Batch: 101, Loss: 0.4783, Accuracy: 68.98%, Precision: 66.18%\n",
            "Epoch: 5, Batch: 102, Loss: 0.4337, Accuracy: 68.99%, Precision: 66.19%\n",
            "Epoch: 5, Batch: 103, Loss: 0.4568, Accuracy: 68.99%, Precision: 66.19%\n",
            "Epoch: 5, Batch: 104, Loss: 0.4541, Accuracy: 69.00%, Precision: 66.19%\n",
            "Epoch: 5, Batch: 105, Loss: 0.4229, Accuracy: 69.01%, Precision: 66.20%\n",
            "Epoch: 5, Batch: 106, Loss: 0.5312, Accuracy: 69.01%, Precision: 66.20%\n",
            "Epoch: 5, Batch: 107, Loss: 0.4334, Accuracy: 69.02%, Precision: 66.21%\n",
            "Epoch: 5, Batch: 108, Loss: 0.4623, Accuracy: 69.02%, Precision: 66.22%\n",
            "Epoch: 5, Batch: 109, Loss: 0.3850, Accuracy: 69.03%, Precision: 66.23%\n",
            "Epoch: 5, Batch: 110, Loss: 0.4785, Accuracy: 69.04%, Precision: 66.24%\n",
            "Epoch: 5, Batch: 111, Loss: 0.4156, Accuracy: 69.05%, Precision: 66.24%\n",
            "Epoch: 5, Batch: 112, Loss: 0.3970, Accuracy: 69.06%, Precision: 66.25%\n",
            "Epoch: 5, Batch: 113, Loss: 0.5675, Accuracy: 69.06%, Precision: 66.24%\n",
            "Epoch: 5, Batch: 114, Loss: 0.3688, Accuracy: 69.07%, Precision: 66.25%\n",
            "Epoch: 5, Batch: 115, Loss: 0.5157, Accuracy: 69.07%, Precision: 66.25%\n",
            "Epoch: 5, Batch: 116, Loss: 0.5832, Accuracy: 69.07%, Precision: 66.26%\n",
            "Epoch: 5, Batch: 117, Loss: 0.4483, Accuracy: 69.08%, Precision: 66.27%\n",
            "Epoch: 5, Batch: 118, Loss: 0.4508, Accuracy: 69.09%, Precision: 66.28%\n",
            "Epoch: 5, Batch: 119, Loss: 0.3366, Accuracy: 69.10%, Precision: 66.29%\n",
            "Epoch: 5, Batch: 120, Loss: 0.4703, Accuracy: 69.11%, Precision: 66.30%\n",
            "Epoch: 5, Batch: 121, Loss: 0.5910, Accuracy: 69.11%, Precision: 66.30%\n",
            "Epoch: 5, Batch: 122, Loss: 0.4036, Accuracy: 69.12%, Precision: 66.31%\n",
            "Epoch: 5, Batch: 123, Loss: 0.5522, Accuracy: 69.12%, Precision: 66.30%\n",
            "Epoch: 5, Batch: 124, Loss: 0.4119, Accuracy: 69.12%, Precision: 66.30%\n",
            "Epoch: 5, Batch: 125, Loss: 0.6025, Accuracy: 69.12%, Precision: 66.30%\n",
            "Epoch: 5, Batch: 126, Loss: 0.5300, Accuracy: 69.12%, Precision: 66.31%\n",
            "Epoch: 5, Batch: 127, Loss: 0.4991, Accuracy: 69.13%, Precision: 66.31%\n",
            "Epoch: 5, Batch: 128, Loss: 0.5529, Accuracy: 69.13%, Precision: 66.32%\n",
            "Epoch: 5, Batch: 129, Loss: 0.4055, Accuracy: 69.14%, Precision: 66.33%\n",
            "Epoch: 5, Batch: 130, Loss: 0.4796, Accuracy: 69.15%, Precision: 66.34%\n",
            "Epoch: 5, Batch: 131, Loss: 0.5291, Accuracy: 69.15%, Precision: 66.35%\n",
            "Epoch: 5, Batch: 132, Loss: 0.5176, Accuracy: 69.15%, Precision: 66.35%\n",
            "Epoch: 5, Batch: 133, Loss: 0.5579, Accuracy: 69.15%, Precision: 66.36%\n",
            "Epoch: 5, Batch: 134, Loss: 0.4654, Accuracy: 69.16%, Precision: 66.36%\n",
            "Epoch: 5, Batch: 135, Loss: 0.5161, Accuracy: 69.16%, Precision: 66.36%\n",
            "Epoch: 5, Batch: 136, Loss: 0.5188, Accuracy: 69.16%, Precision: 66.37%\n",
            "Epoch: 5, Batch: 137, Loss: 0.3972, Accuracy: 69.17%, Precision: 66.38%\n",
            "Epoch: 5, Batch: 138, Loss: 0.4887, Accuracy: 69.18%, Precision: 66.39%\n",
            "Epoch: 5, Batch: 139, Loss: 0.4629, Accuracy: 69.18%, Precision: 66.40%\n",
            "Epoch: 5, Batch: 140, Loss: 0.4780, Accuracy: 69.18%, Precision: 66.40%\n",
            "Epoch: 5, Batch: 141, Loss: 0.5591, Accuracy: 69.19%, Precision: 66.40%\n",
            "Epoch: 5, Batch: 142, Loss: 0.4154, Accuracy: 69.19%, Precision: 66.41%\n",
            "Epoch: 5, Batch: 143, Loss: 0.4194, Accuracy: 69.20%, Precision: 66.42%\n",
            "Epoch: 5, Batch: 144, Loss: 0.3883, Accuracy: 69.21%, Precision: 66.44%\n",
            "Epoch: 5, Batch: 145, Loss: 0.3802, Accuracy: 69.22%, Precision: 66.45%\n",
            "Epoch: 5, Batch: 146, Loss: 0.4403, Accuracy: 69.23%, Precision: 66.45%\n",
            "Epoch: 5, Batch: 147, Loss: 0.5222, Accuracy: 69.23%, Precision: 66.45%\n",
            "Epoch: 5, Batch: 148, Loss: 0.5847, Accuracy: 69.24%, Precision: 66.47%\n",
            "Epoch: 5, Batch: 149, Loss: 0.3334, Accuracy: 69.25%, Precision: 66.48%\n",
            "Epoch: 5, Batch: 150, Loss: 0.3812, Accuracy: 69.26%, Precision: 66.50%\n",
            "Epoch: 5, Batch: 151, Loss: 0.4823, Accuracy: 69.26%, Precision: 66.49%\n",
            "Epoch: 5, Batch: 152, Loss: 0.5619, Accuracy: 69.26%, Precision: 66.49%\n",
            "Epoch: 5, Batch: 153, Loss: 0.3923, Accuracy: 69.27%, Precision: 66.50%\n",
            "Epoch: 5, Batch: 154, Loss: 0.4676, Accuracy: 69.28%, Precision: 66.51%\n",
            "Epoch: 5, Batch: 155, Loss: 0.4625, Accuracy: 69.28%, Precision: 66.52%\n",
            "Epoch: 5, Batch: 156, Loss: 0.4639, Accuracy: 69.29%, Precision: 66.53%\n",
            "Epoch: 5, Batch: 157, Loss: 0.4674, Accuracy: 69.29%, Precision: 66.53%\n",
            "Epoch: 5, Batch: 158, Loss: 0.4565, Accuracy: 69.30%, Precision: 66.54%\n",
            "Epoch: 5, Batch: 159, Loss: 0.4233, Accuracy: 69.31%, Precision: 66.55%\n",
            "Epoch: 5, Batch: 160, Loss: 0.3495, Accuracy: 69.32%, Precision: 66.56%\n",
            "Epoch: 5, Batch: 161, Loss: 0.5600, Accuracy: 69.32%, Precision: 66.56%\n",
            "Epoch: 5, Batch: 162, Loss: 0.4434, Accuracy: 69.33%, Precision: 66.56%\n",
            "Epoch: 5, Batch: 163, Loss: 0.4329, Accuracy: 69.33%, Precision: 66.57%\n",
            "Epoch: 5, Batch: 164, Loss: 0.3716, Accuracy: 69.35%, Precision: 66.58%\n",
            "Epoch: 5, Batch: 165, Loss: 0.5376, Accuracy: 69.35%, Precision: 66.59%\n",
            "Epoch: 5, Batch: 166, Loss: 0.4960, Accuracy: 69.35%, Precision: 66.59%\n",
            "Epoch: 5, Batch: 167, Loss: 0.5118, Accuracy: 69.35%, Precision: 66.59%\n",
            "Epoch: 5, Batch: 168, Loss: 0.5048, Accuracy: 69.36%, Precision: 66.60%\n",
            "Epoch: 5, Batch: 169, Loss: 0.5114, Accuracy: 69.37%, Precision: 66.61%\n",
            "Epoch: 5, Batch: 170, Loss: 0.4286, Accuracy: 69.37%, Precision: 66.61%\n",
            "Epoch: 5, Batch: 171, Loss: 0.5039, Accuracy: 69.38%, Precision: 66.62%\n",
            "Epoch: 5, Batch: 172, Loss: 0.4049, Accuracy: 69.39%, Precision: 66.62%\n",
            "Epoch: 5, Batch: 173, Loss: 0.4211, Accuracy: 69.39%, Precision: 66.63%\n",
            "Epoch: 5, Batch: 174, Loss: 0.4490, Accuracy: 69.40%, Precision: 66.63%\n",
            "Epoch: 5, Batch: 175, Loss: 0.4985, Accuracy: 69.40%, Precision: 66.64%\n",
            "Epoch: 5, Batch: 176, Loss: 0.5017, Accuracy: 69.40%, Precision: 66.64%\n",
            "Epoch: 5, Batch: 177, Loss: 0.4140, Accuracy: 69.41%, Precision: 66.66%\n",
            "Epoch: 5, Batch: 178, Loss: 0.3697, Accuracy: 69.42%, Precision: 66.67%\n",
            "Epoch: 5, Batch: 179, Loss: 0.4693, Accuracy: 69.43%, Precision: 66.68%\n",
            "Epoch: 5, Batch: 180, Loss: 0.4164, Accuracy: 69.44%, Precision: 66.69%\n",
            "Epoch: 5, Batch: 181, Loss: 0.4531, Accuracy: 69.44%, Precision: 66.69%\n",
            "Epoch: 5, Batch: 182, Loss: 0.4478, Accuracy: 69.45%, Precision: 66.70%\n",
            "Epoch: 5, Batch: 183, Loss: 0.3772, Accuracy: 69.46%, Precision: 66.71%\n",
            "Epoch: 5, Batch: 184, Loss: 0.4769, Accuracy: 69.47%, Precision: 66.72%\n",
            "Epoch: 5, Batch: 185, Loss: 0.3814, Accuracy: 69.47%, Precision: 66.72%\n",
            "Epoch: 5, Batch: 186, Loss: 0.3713, Accuracy: 69.49%, Precision: 66.73%\n",
            "Epoch: 5, Batch: 187, Loss: 0.4103, Accuracy: 69.49%, Precision: 66.74%\n",
            "Epoch: 5, Batch: 188, Loss: 0.6007, Accuracy: 69.49%, Precision: 66.74%\n",
            "Epoch: 5, Batch: 189, Loss: 0.4217, Accuracy: 69.50%, Precision: 66.76%\n",
            "Epoch: 5, Batch: 190, Loss: 0.4227, Accuracy: 69.51%, Precision: 66.76%\n",
            "Epoch: 5, Batch: 191, Loss: 0.4794, Accuracy: 69.51%, Precision: 66.76%\n",
            "Epoch: 5, Batch: 192, Loss: 0.4298, Accuracy: 69.52%, Precision: 66.77%\n",
            "Epoch: 5, Batch: 193, Loss: 0.4960, Accuracy: 69.52%, Precision: 66.77%\n",
            "Epoch: 5, Batch: 194, Loss: 0.5637, Accuracy: 69.52%, Precision: 66.77%\n",
            "Epoch: 5, Batch: 195, Loss: 0.5468, Accuracy: 69.52%, Precision: 66.77%\n",
            "Epoch: 5, Batch: 196, Loss: 0.5079, Accuracy: 69.52%, Precision: 66.77%\n",
            "Epoch: 5, Batch: 197, Loss: 0.5184, Accuracy: 69.52%, Precision: 66.78%\n",
            "Epoch: 5, Batch: 198, Loss: 0.3993, Accuracy: 69.53%, Precision: 66.79%\n",
            "Epoch: 5, Batch: 199, Loss: 0.4646, Accuracy: 69.54%, Precision: 66.80%\n",
            "Epoch: 5, Batch: 200, Loss: 0.5036, Accuracy: 69.54%, Precision: 66.81%\n",
            "Epoch: 5, Batch: 201, Loss: 0.4654, Accuracy: 69.55%, Precision: 66.82%\n",
            "Epoch: 5, Batch: 202, Loss: 0.4915, Accuracy: 69.55%, Precision: 66.82%\n",
            "Epoch: 5, Batch: 203, Loss: 0.5802, Accuracy: 69.55%, Precision: 66.81%\n",
            "Epoch: 5, Batch: 204, Loss: 0.6467, Accuracy: 69.55%, Precision: 66.80%\n",
            "Epoch: 5, Batch: 205, Loss: 0.5036, Accuracy: 69.55%, Precision: 66.80%\n",
            "Epoch: 5, Batch: 206, Loss: 0.4907, Accuracy: 69.55%, Precision: 66.80%\n",
            "Epoch: 5, Batch: 207, Loss: 0.4489, Accuracy: 69.56%, Precision: 66.81%\n",
            "Epoch: 5, Batch: 208, Loss: 0.4750, Accuracy: 69.57%, Precision: 66.82%\n",
            "Epoch: 5, Batch: 209, Loss: 0.4546, Accuracy: 69.57%, Precision: 66.82%\n",
            "Epoch: 5, Batch: 210, Loss: 0.4793, Accuracy: 69.57%, Precision: 66.82%\n",
            "Epoch: 5, Batch: 211, Loss: 0.5205, Accuracy: 69.58%, Precision: 66.84%\n",
            "Epoch: 5, Batch: 212, Loss: 0.5600, Accuracy: 69.57%, Precision: 66.84%\n",
            "Epoch: 5, Batch: 213, Loss: 0.4672, Accuracy: 69.58%, Precision: 66.83%\n",
            "Epoch: 5, Batch: 214, Loss: 0.4613, Accuracy: 69.58%, Precision: 66.84%\n",
            "Epoch: 5, Batch: 215, Loss: 0.4299, Accuracy: 69.59%, Precision: 66.85%\n",
            "Epoch: 5, Batch: 216, Loss: 0.4965, Accuracy: 69.60%, Precision: 66.86%\n",
            "Epoch: 5, Batch: 217, Loss: 0.4869, Accuracy: 69.60%, Precision: 66.86%\n",
            "Epoch: 5, Batch: 218, Loss: 0.3964, Accuracy: 69.61%, Precision: 66.87%\n",
            "Epoch: 5, Batch: 219, Loss: 0.4365, Accuracy: 69.61%, Precision: 66.88%\n",
            "Epoch: 5, Batch: 220, Loss: 0.5143, Accuracy: 69.62%, Precision: 66.88%\n",
            "Epoch: 5, Batch: 221, Loss: 0.5747, Accuracy: 69.62%, Precision: 66.88%\n",
            "Epoch: 5, Batch: 222, Loss: 0.4596, Accuracy: 69.62%, Precision: 66.87%\n",
            "Epoch: 5, Batch: 223, Loss: 0.5148, Accuracy: 69.63%, Precision: 66.88%\n",
            "Epoch: 5, Batch: 224, Loss: 0.3982, Accuracy: 69.63%, Precision: 66.89%\n",
            "Epoch: 5, Batch: 225, Loss: 0.4548, Accuracy: 69.64%, Precision: 66.90%\n",
            "Epoch: 5, Batch: 226, Loss: 0.4778, Accuracy: 69.64%, Precision: 66.89%\n",
            "Epoch: 5, Batch: 227, Loss: 0.4515, Accuracy: 69.65%, Precision: 66.90%\n",
            "Epoch: 5, Batch: 228, Loss: 0.4945, Accuracy: 69.65%, Precision: 66.91%\n",
            "Epoch: 5, Batch: 229, Loss: 0.3917, Accuracy: 69.66%, Precision: 66.92%\n",
            "Epoch: 5, Batch: 230, Loss: 0.4757, Accuracy: 69.66%, Precision: 66.93%\n",
            "Epoch: 5, Batch: 231, Loss: 0.4263, Accuracy: 69.66%, Precision: 66.93%\n",
            "Epoch: 5, Batch: 232, Loss: 0.5157, Accuracy: 69.66%, Precision: 66.93%\n",
            "Epoch: 5, Batch: 233, Loss: 0.5433, Accuracy: 69.67%, Precision: 66.92%\n",
            "Epoch: 5, Batch: 234, Loss: 0.4518, Accuracy: 69.67%, Precision: 66.93%\n",
            "Epoch: 5, Batch: 235, Loss: 0.4513, Accuracy: 69.68%, Precision: 66.93%\n",
            "Epoch: 5, Batch: 236, Loss: 0.4573, Accuracy: 69.68%, Precision: 66.93%\n",
            "Epoch: 5, Batch: 237, Loss: 0.4377, Accuracy: 69.69%, Precision: 66.94%\n",
            "Epoch: 5, Batch: 238, Loss: 0.4076, Accuracy: 69.69%, Precision: 66.94%\n",
            "Epoch: 5, Batch: 239, Loss: 0.4230, Accuracy: 69.70%, Precision: 66.95%\n",
            "Epoch: 5, Batch: 240, Loss: 0.4035, Accuracy: 69.70%, Precision: 66.96%\n",
            "Epoch: 5, Batch: 241, Loss: 0.5437, Accuracy: 69.70%, Precision: 66.96%\n",
            "Epoch: 5, Batch: 242, Loss: 0.4348, Accuracy: 69.71%, Precision: 66.96%\n",
            "Epoch: 5, Batch: 243, Loss: 0.4873, Accuracy: 69.71%, Precision: 66.97%\n",
            "Epoch: 6, Batch: 1, Loss: 0.3938, Accuracy: 69.72%, Precision: 66.97%\n",
            "Epoch: 6, Batch: 2, Loss: 0.4283, Accuracy: 69.72%, Precision: 66.97%\n",
            "Epoch: 6, Batch: 3, Loss: 0.4263, Accuracy: 69.73%, Precision: 66.98%\n",
            "Epoch: 6, Batch: 4, Loss: 0.5550, Accuracy: 69.73%, Precision: 66.97%\n",
            "Epoch: 6, Batch: 5, Loss: 0.4453, Accuracy: 69.73%, Precision: 66.97%\n",
            "Epoch: 6, Batch: 6, Loss: 0.4164, Accuracy: 69.74%, Precision: 66.97%\n",
            "Epoch: 6, Batch: 7, Loss: 0.3588, Accuracy: 69.75%, Precision: 66.98%\n",
            "Epoch: 6, Batch: 8, Loss: 0.3452, Accuracy: 69.76%, Precision: 66.99%\n",
            "Epoch: 6, Batch: 9, Loss: 0.4937, Accuracy: 69.77%, Precision: 67.00%\n",
            "Epoch: 6, Batch: 10, Loss: 0.4147, Accuracy: 69.77%, Precision: 67.01%\n",
            "Epoch: 6, Batch: 11, Loss: 0.4677, Accuracy: 69.78%, Precision: 67.02%\n",
            "Epoch: 6, Batch: 12, Loss: 0.3402, Accuracy: 69.79%, Precision: 67.03%\n",
            "Epoch: 6, Batch: 13, Loss: 0.4410, Accuracy: 69.79%, Precision: 67.04%\n",
            "Epoch: 6, Batch: 14, Loss: 0.3604, Accuracy: 69.80%, Precision: 67.05%\n",
            "Epoch: 6, Batch: 15, Loss: 0.3341, Accuracy: 69.81%, Precision: 67.06%\n",
            "Epoch: 6, Batch: 16, Loss: 0.3876, Accuracy: 69.82%, Precision: 67.06%\n",
            "Epoch: 6, Batch: 17, Loss: 0.4488, Accuracy: 69.82%, Precision: 67.07%\n",
            "Epoch: 6, Batch: 18, Loss: 0.3929, Accuracy: 69.83%, Precision: 67.07%\n",
            "Epoch: 6, Batch: 19, Loss: 0.3840, Accuracy: 69.84%, Precision: 67.08%\n",
            "Epoch: 6, Batch: 20, Loss: 0.3742, Accuracy: 69.85%, Precision: 67.09%\n",
            "Epoch: 6, Batch: 21, Loss: 0.3252, Accuracy: 69.86%, Precision: 67.10%\n",
            "Epoch: 6, Batch: 22, Loss: 0.4214, Accuracy: 69.86%, Precision: 67.10%\n",
            "Epoch: 6, Batch: 23, Loss: 0.5416, Accuracy: 69.87%, Precision: 67.11%\n",
            "Epoch: 6, Batch: 24, Loss: 0.3823, Accuracy: 69.87%, Precision: 67.12%\n",
            "Epoch: 6, Batch: 25, Loss: 0.2812, Accuracy: 69.89%, Precision: 67.14%\n",
            "Epoch: 6, Batch: 26, Loss: 0.3361, Accuracy: 69.90%, Precision: 67.15%\n",
            "Epoch: 6, Batch: 27, Loss: 0.4577, Accuracy: 69.90%, Precision: 67.15%\n",
            "Epoch: 6, Batch: 28, Loss: 0.3996, Accuracy: 69.90%, Precision: 67.15%\n",
            "Epoch: 6, Batch: 29, Loss: 0.3700, Accuracy: 69.91%, Precision: 67.17%\n",
            "Epoch: 6, Batch: 30, Loss: 0.3906, Accuracy: 69.92%, Precision: 67.18%\n",
            "Epoch: 6, Batch: 31, Loss: 0.4520, Accuracy: 69.93%, Precision: 67.18%\n",
            "Epoch: 6, Batch: 32, Loss: 0.2799, Accuracy: 69.94%, Precision: 67.20%\n",
            "Epoch: 6, Batch: 33, Loss: 0.4443, Accuracy: 69.94%, Precision: 67.20%\n",
            "Epoch: 6, Batch: 34, Loss: 0.3231, Accuracy: 69.95%, Precision: 67.21%\n",
            "Epoch: 6, Batch: 35, Loss: 0.3575, Accuracy: 69.96%, Precision: 67.22%\n",
            "Epoch: 6, Batch: 36, Loss: 0.2585, Accuracy: 69.98%, Precision: 67.24%\n",
            "Epoch: 6, Batch: 37, Loss: 0.4887, Accuracy: 69.98%, Precision: 67.24%\n",
            "Epoch: 6, Batch: 38, Loss: 0.3996, Accuracy: 69.99%, Precision: 67.25%\n",
            "Epoch: 6, Batch: 39, Loss: 0.3809, Accuracy: 70.00%, Precision: 67.25%\n",
            "Epoch: 6, Batch: 40, Loss: 0.3293, Accuracy: 70.01%, Precision: 67.26%\n",
            "Epoch: 6, Batch: 41, Loss: 0.5028, Accuracy: 70.01%, Precision: 67.27%\n",
            "Epoch: 6, Batch: 42, Loss: 0.3414, Accuracy: 70.02%, Precision: 67.28%\n",
            "Epoch: 6, Batch: 43, Loss: 0.3740, Accuracy: 70.03%, Precision: 67.28%\n",
            "Epoch: 6, Batch: 44, Loss: 0.3228, Accuracy: 70.04%, Precision: 67.30%\n",
            "Epoch: 6, Batch: 45, Loss: 0.3350, Accuracy: 70.05%, Precision: 67.31%\n",
            "Epoch: 6, Batch: 46, Loss: 0.2976, Accuracy: 70.06%, Precision: 67.32%\n",
            "Epoch: 6, Batch: 47, Loss: 0.3709, Accuracy: 70.07%, Precision: 67.33%\n",
            "Epoch: 6, Batch: 48, Loss: 0.4896, Accuracy: 70.07%, Precision: 67.33%\n",
            "Epoch: 6, Batch: 49, Loss: 0.4885, Accuracy: 70.07%, Precision: 67.33%\n",
            "Epoch: 6, Batch: 50, Loss: 0.3719, Accuracy: 70.08%, Precision: 67.34%\n",
            "Epoch: 6, Batch: 51, Loss: 0.4109, Accuracy: 70.09%, Precision: 67.35%\n",
            "Epoch: 6, Batch: 52, Loss: 0.3581, Accuracy: 70.09%, Precision: 67.35%\n",
            "Epoch: 6, Batch: 53, Loss: 0.4470, Accuracy: 70.09%, Precision: 67.35%\n",
            "Epoch: 6, Batch: 54, Loss: 0.4605, Accuracy: 70.10%, Precision: 67.35%\n",
            "Epoch: 6, Batch: 55, Loss: 0.4306, Accuracy: 70.10%, Precision: 67.35%\n",
            "Epoch: 6, Batch: 56, Loss: 0.4119, Accuracy: 70.11%, Precision: 67.36%\n",
            "Epoch: 6, Batch: 57, Loss: 0.5205, Accuracy: 70.11%, Precision: 67.36%\n",
            "Epoch: 6, Batch: 58, Loss: 0.3278, Accuracy: 70.12%, Precision: 67.36%\n",
            "Epoch: 6, Batch: 59, Loss: 0.3991, Accuracy: 70.13%, Precision: 67.38%\n",
            "Epoch: 6, Batch: 60, Loss: 0.5132, Accuracy: 70.13%, Precision: 67.38%\n",
            "Epoch: 6, Batch: 61, Loss: 0.4332, Accuracy: 70.13%, Precision: 67.38%\n",
            "Epoch: 6, Batch: 62, Loss: 0.3508, Accuracy: 70.14%, Precision: 67.40%\n",
            "Epoch: 6, Batch: 63, Loss: 0.3759, Accuracy: 70.15%, Precision: 67.40%\n",
            "Epoch: 6, Batch: 64, Loss: 0.4208, Accuracy: 70.16%, Precision: 67.41%\n",
            "Epoch: 6, Batch: 65, Loss: 0.4572, Accuracy: 70.17%, Precision: 67.42%\n",
            "Epoch: 6, Batch: 66, Loss: 0.5208, Accuracy: 70.17%, Precision: 67.42%\n",
            "Epoch: 6, Batch: 67, Loss: 0.4483, Accuracy: 70.18%, Precision: 67.43%\n",
            "Epoch: 6, Batch: 68, Loss: 0.3384, Accuracy: 70.18%, Precision: 67.44%\n",
            "Epoch: 6, Batch: 69, Loss: 0.3539, Accuracy: 70.19%, Precision: 67.45%\n",
            "Epoch: 6, Batch: 70, Loss: 0.5396, Accuracy: 70.20%, Precision: 67.46%\n",
            "Epoch: 6, Batch: 71, Loss: 0.3545, Accuracy: 70.21%, Precision: 67.47%\n",
            "Epoch: 6, Batch: 72, Loss: 0.3511, Accuracy: 70.22%, Precision: 67.48%\n",
            "Epoch: 6, Batch: 73, Loss: 0.3881, Accuracy: 70.22%, Precision: 67.49%\n",
            "Epoch: 6, Batch: 74, Loss: 0.3911, Accuracy: 70.23%, Precision: 67.50%\n",
            "Epoch: 6, Batch: 75, Loss: 0.4139, Accuracy: 70.24%, Precision: 67.50%\n",
            "Epoch: 6, Batch: 76, Loss: 0.3759, Accuracy: 70.25%, Precision: 67.51%\n",
            "Epoch: 6, Batch: 77, Loss: 0.5313, Accuracy: 70.25%, Precision: 67.51%\n",
            "Epoch: 6, Batch: 78, Loss: 0.3220, Accuracy: 70.27%, Precision: 67.52%\n",
            "Epoch: 6, Batch: 79, Loss: 0.4469, Accuracy: 70.27%, Precision: 67.53%\n",
            "Epoch: 6, Batch: 80, Loss: 0.3599, Accuracy: 70.28%, Precision: 67.54%\n",
            "Epoch: 6, Batch: 81, Loss: 0.3155, Accuracy: 70.29%, Precision: 67.55%\n",
            "Epoch: 6, Batch: 82, Loss: 0.4467, Accuracy: 70.30%, Precision: 67.56%\n",
            "Epoch: 6, Batch: 83, Loss: 0.4593, Accuracy: 70.30%, Precision: 67.57%\n",
            "Epoch: 6, Batch: 84, Loss: 0.4786, Accuracy: 70.30%, Precision: 67.57%\n",
            "Epoch: 6, Batch: 85, Loss: 0.3911, Accuracy: 70.31%, Precision: 67.58%\n",
            "Epoch: 6, Batch: 86, Loss: 0.4902, Accuracy: 70.31%, Precision: 67.59%\n",
            "Epoch: 6, Batch: 87, Loss: 0.3155, Accuracy: 70.32%, Precision: 67.60%\n",
            "Epoch: 6, Batch: 88, Loss: 0.3488, Accuracy: 70.33%, Precision: 67.61%\n",
            "Epoch: 6, Batch: 89, Loss: 0.3808, Accuracy: 70.34%, Precision: 67.62%\n",
            "Epoch: 6, Batch: 90, Loss: 0.6367, Accuracy: 70.33%, Precision: 67.61%\n",
            "Epoch: 6, Batch: 91, Loss: 0.4403, Accuracy: 70.34%, Precision: 67.61%\n",
            "Epoch: 6, Batch: 92, Loss: 0.3378, Accuracy: 70.35%, Precision: 67.62%\n",
            "Epoch: 6, Batch: 93, Loss: 0.4463, Accuracy: 70.35%, Precision: 67.63%\n",
            "Epoch: 6, Batch: 94, Loss: 0.3035, Accuracy: 70.36%, Precision: 67.65%\n",
            "Epoch: 6, Batch: 95, Loss: 0.6072, Accuracy: 70.36%, Precision: 67.65%\n",
            "Epoch: 6, Batch: 96, Loss: 0.2866, Accuracy: 70.38%, Precision: 67.67%\n",
            "Epoch: 6, Batch: 97, Loss: 0.4260, Accuracy: 70.39%, Precision: 67.68%\n",
            "Epoch: 6, Batch: 98, Loss: 0.4182, Accuracy: 70.39%, Precision: 67.69%\n",
            "Epoch: 6, Batch: 99, Loss: 0.5554, Accuracy: 70.40%, Precision: 67.69%\n",
            "Epoch: 6, Batch: 100, Loss: 0.4104, Accuracy: 70.40%, Precision: 67.69%\n",
            "Epoch: 6, Batch: 101, Loss: 0.4153, Accuracy: 70.41%, Precision: 67.70%\n",
            "Epoch: 6, Batch: 102, Loss: 0.4289, Accuracy: 70.41%, Precision: 67.70%\n",
            "Epoch: 6, Batch: 103, Loss: 0.5053, Accuracy: 70.42%, Precision: 67.71%\n",
            "Epoch: 6, Batch: 104, Loss: 0.5150, Accuracy: 70.42%, Precision: 67.71%\n",
            "Epoch: 6, Batch: 105, Loss: 0.3958, Accuracy: 70.42%, Precision: 67.72%\n",
            "Epoch: 6, Batch: 106, Loss: 0.3663, Accuracy: 70.43%, Precision: 67.73%\n",
            "Epoch: 6, Batch: 107, Loss: 0.4507, Accuracy: 70.43%, Precision: 67.73%\n",
            "Epoch: 6, Batch: 108, Loss: 0.4239, Accuracy: 70.44%, Precision: 67.74%\n",
            "Epoch: 6, Batch: 109, Loss: 0.4622, Accuracy: 70.44%, Precision: 67.74%\n",
            "Epoch: 6, Batch: 110, Loss: 0.4016, Accuracy: 70.45%, Precision: 67.75%\n",
            "Epoch: 6, Batch: 111, Loss: 0.3911, Accuracy: 70.46%, Precision: 67.76%\n",
            "Epoch: 6, Batch: 112, Loss: 0.5021, Accuracy: 70.46%, Precision: 67.77%\n",
            "Epoch: 6, Batch: 113, Loss: 0.3177, Accuracy: 70.47%, Precision: 67.77%\n",
            "Epoch: 6, Batch: 114, Loss: 0.4685, Accuracy: 70.47%, Precision: 67.77%\n",
            "Epoch: 6, Batch: 115, Loss: 0.4105, Accuracy: 70.48%, Precision: 67.78%\n",
            "Epoch: 6, Batch: 116, Loss: 0.5385, Accuracy: 70.48%, Precision: 67.78%\n",
            "Epoch: 6, Batch: 117, Loss: 0.5213, Accuracy: 70.48%, Precision: 67.78%\n",
            "Epoch: 6, Batch: 118, Loss: 0.4589, Accuracy: 70.49%, Precision: 67.79%\n",
            "Epoch: 6, Batch: 119, Loss: 0.3377, Accuracy: 70.50%, Precision: 67.80%\n",
            "Epoch: 6, Batch: 120, Loss: 0.4039, Accuracy: 70.50%, Precision: 67.80%\n",
            "Epoch: 6, Batch: 121, Loss: 0.4238, Accuracy: 70.51%, Precision: 67.82%\n",
            "Epoch: 6, Batch: 122, Loss: 0.3386, Accuracy: 70.52%, Precision: 67.83%\n",
            "Epoch: 6, Batch: 123, Loss: 0.4237, Accuracy: 70.53%, Precision: 67.84%\n",
            "Epoch: 6, Batch: 124, Loss: 0.3872, Accuracy: 70.54%, Precision: 67.84%\n",
            "Epoch: 6, Batch: 125, Loss: 0.4317, Accuracy: 70.54%, Precision: 67.85%\n",
            "Epoch: 6, Batch: 126, Loss: 0.3964, Accuracy: 70.55%, Precision: 67.86%\n",
            "Epoch: 6, Batch: 127, Loss: 0.3662, Accuracy: 70.56%, Precision: 67.88%\n",
            "Epoch: 6, Batch: 128, Loss: 0.3619, Accuracy: 70.57%, Precision: 67.89%\n",
            "Epoch: 6, Batch: 129, Loss: 0.3959, Accuracy: 70.57%, Precision: 67.89%\n",
            "Epoch: 6, Batch: 130, Loss: 0.3775, Accuracy: 70.58%, Precision: 67.90%\n",
            "Epoch: 6, Batch: 131, Loss: 0.5539, Accuracy: 70.58%, Precision: 67.89%\n",
            "Epoch: 6, Batch: 132, Loss: 0.3416, Accuracy: 70.59%, Precision: 67.91%\n",
            "Epoch: 6, Batch: 133, Loss: 0.3852, Accuracy: 70.60%, Precision: 67.91%\n",
            "Epoch: 6, Batch: 134, Loss: 0.6538, Accuracy: 70.60%, Precision: 67.92%\n",
            "Epoch: 6, Batch: 135, Loss: 0.3456, Accuracy: 70.61%, Precision: 67.92%\n",
            "Epoch: 6, Batch: 136, Loss: 0.5168, Accuracy: 70.60%, Precision: 67.92%\n",
            "Epoch: 6, Batch: 137, Loss: 0.3081, Accuracy: 70.61%, Precision: 67.93%\n",
            "Epoch: 6, Batch: 138, Loss: 0.5553, Accuracy: 70.62%, Precision: 67.93%\n",
            "Epoch: 6, Batch: 139, Loss: 0.3292, Accuracy: 70.63%, Precision: 67.94%\n",
            "Epoch: 6, Batch: 140, Loss: 0.2566, Accuracy: 70.64%, Precision: 67.96%\n",
            "Epoch: 6, Batch: 141, Loss: 0.4179, Accuracy: 70.65%, Precision: 67.96%\n",
            "Epoch: 6, Batch: 142, Loss: 0.3441, Accuracy: 70.65%, Precision: 67.96%\n",
            "Epoch: 6, Batch: 143, Loss: 0.5567, Accuracy: 70.66%, Precision: 67.97%\n",
            "Epoch: 6, Batch: 144, Loss: 0.4158, Accuracy: 70.66%, Precision: 67.97%\n",
            "Epoch: 6, Batch: 145, Loss: 0.3921, Accuracy: 70.67%, Precision: 67.98%\n",
            "Epoch: 6, Batch: 146, Loss: 0.6640, Accuracy: 70.67%, Precision: 67.98%\n",
            "Epoch: 6, Batch: 147, Loss: 0.4134, Accuracy: 70.67%, Precision: 67.98%\n",
            "Epoch: 6, Batch: 148, Loss: 0.3290, Accuracy: 70.68%, Precision: 67.99%\n",
            "Epoch: 6, Batch: 149, Loss: 0.3522, Accuracy: 70.69%, Precision: 68.00%\n",
            "Epoch: 6, Batch: 150, Loss: 0.4269, Accuracy: 70.70%, Precision: 68.01%\n",
            "Epoch: 6, Batch: 151, Loss: 0.2660, Accuracy: 70.71%, Precision: 68.02%\n",
            "Epoch: 6, Batch: 152, Loss: 0.3617, Accuracy: 70.71%, Precision: 68.03%\n",
            "Epoch: 6, Batch: 153, Loss: 0.4477, Accuracy: 70.72%, Precision: 68.03%\n",
            "Epoch: 6, Batch: 154, Loss: 0.4838, Accuracy: 70.72%, Precision: 68.03%\n",
            "Epoch: 6, Batch: 155, Loss: 0.4236, Accuracy: 70.73%, Precision: 68.05%\n",
            "Epoch: 6, Batch: 156, Loss: 0.3875, Accuracy: 70.73%, Precision: 68.06%\n",
            "Epoch: 6, Batch: 157, Loss: 0.3534, Accuracy: 70.74%, Precision: 68.06%\n",
            "Epoch: 6, Batch: 158, Loss: 0.3761, Accuracy: 70.74%, Precision: 68.06%\n",
            "Epoch: 6, Batch: 159, Loss: 0.4341, Accuracy: 70.75%, Precision: 68.07%\n",
            "Epoch: 6, Batch: 160, Loss: 0.3293, Accuracy: 70.76%, Precision: 68.08%\n",
            "Epoch: 6, Batch: 161, Loss: 0.3138, Accuracy: 70.77%, Precision: 68.09%\n",
            "Epoch: 6, Batch: 162, Loss: 0.5560, Accuracy: 70.77%, Precision: 68.09%\n",
            "Epoch: 6, Batch: 163, Loss: 0.4296, Accuracy: 70.77%, Precision: 68.09%\n",
            "Epoch: 6, Batch: 164, Loss: 0.5767, Accuracy: 70.77%, Precision: 68.09%\n",
            "Epoch: 6, Batch: 165, Loss: 0.4971, Accuracy: 70.77%, Precision: 68.09%\n",
            "Epoch: 6, Batch: 166, Loss: 0.5245, Accuracy: 70.78%, Precision: 68.10%\n",
            "Epoch: 6, Batch: 167, Loss: 0.3836, Accuracy: 70.78%, Precision: 68.10%\n",
            "Epoch: 6, Batch: 168, Loss: 0.4475, Accuracy: 70.79%, Precision: 68.11%\n",
            "Epoch: 6, Batch: 169, Loss: 0.3910, Accuracy: 70.79%, Precision: 68.11%\n",
            "Epoch: 6, Batch: 170, Loss: 0.4227, Accuracy: 70.80%, Precision: 68.11%\n",
            "Epoch: 6, Batch: 171, Loss: 0.3636, Accuracy: 70.81%, Precision: 68.12%\n",
            "Epoch: 6, Batch: 172, Loss: 0.3633, Accuracy: 70.82%, Precision: 68.12%\n",
            "Epoch: 6, Batch: 173, Loss: 0.3341, Accuracy: 70.82%, Precision: 68.13%\n",
            "Epoch: 6, Batch: 174, Loss: 0.4134, Accuracy: 70.83%, Precision: 68.14%\n",
            "Epoch: 6, Batch: 175, Loss: 0.5650, Accuracy: 70.83%, Precision: 68.14%\n",
            "Epoch: 6, Batch: 176, Loss: 0.4230, Accuracy: 70.84%, Precision: 68.15%\n",
            "Epoch: 6, Batch: 177, Loss: 0.4532, Accuracy: 70.85%, Precision: 68.15%\n",
            "Epoch: 6, Batch: 178, Loss: 0.5416, Accuracy: 70.84%, Precision: 68.15%\n",
            "Epoch: 6, Batch: 179, Loss: 0.4097, Accuracy: 70.84%, Precision: 68.15%\n",
            "Epoch: 6, Batch: 180, Loss: 0.3507, Accuracy: 70.86%, Precision: 68.16%\n",
            "Epoch: 6, Batch: 181, Loss: 0.4373, Accuracy: 70.86%, Precision: 68.17%\n",
            "Epoch: 6, Batch: 182, Loss: 0.3933, Accuracy: 70.87%, Precision: 68.18%\n",
            "Epoch: 6, Batch: 183, Loss: 0.3100, Accuracy: 70.88%, Precision: 68.18%\n",
            "Epoch: 6, Batch: 184, Loss: 0.4809, Accuracy: 70.88%, Precision: 68.18%\n",
            "Epoch: 6, Batch: 185, Loss: 0.4299, Accuracy: 70.88%, Precision: 68.19%\n",
            "Epoch: 6, Batch: 186, Loss: 0.4056, Accuracy: 70.89%, Precision: 68.19%\n",
            "Epoch: 6, Batch: 187, Loss: 0.4984, Accuracy: 70.89%, Precision: 68.20%\n",
            "Epoch: 6, Batch: 188, Loss: 0.4738, Accuracy: 70.90%, Precision: 68.21%\n",
            "Epoch: 6, Batch: 189, Loss: 0.4696, Accuracy: 70.90%, Precision: 68.21%\n",
            "Epoch: 6, Batch: 190, Loss: 0.3869, Accuracy: 70.91%, Precision: 68.21%\n",
            "Epoch: 6, Batch: 191, Loss: 0.3838, Accuracy: 70.91%, Precision: 68.22%\n",
            "Epoch: 6, Batch: 192, Loss: 0.3960, Accuracy: 70.92%, Precision: 68.23%\n",
            "Epoch: 6, Batch: 193, Loss: 0.4483, Accuracy: 70.92%, Precision: 68.23%\n",
            "Epoch: 6, Batch: 194, Loss: 0.4765, Accuracy: 70.93%, Precision: 68.23%\n",
            "Epoch: 6, Batch: 195, Loss: 0.4703, Accuracy: 70.93%, Precision: 68.23%\n",
            "Epoch: 6, Batch: 196, Loss: 0.4473, Accuracy: 70.93%, Precision: 68.23%\n",
            "Epoch: 6, Batch: 197, Loss: 0.4688, Accuracy: 70.94%, Precision: 68.23%\n",
            "Epoch: 6, Batch: 198, Loss: 0.5242, Accuracy: 70.94%, Precision: 68.24%\n",
            "Epoch: 6, Batch: 199, Loss: 0.4042, Accuracy: 70.94%, Precision: 68.25%\n",
            "Epoch: 6, Batch: 200, Loss: 0.3689, Accuracy: 70.95%, Precision: 68.25%\n",
            "Epoch: 6, Batch: 201, Loss: 0.4089, Accuracy: 70.96%, Precision: 68.26%\n",
            "Epoch: 6, Batch: 202, Loss: 0.3561, Accuracy: 70.96%, Precision: 68.27%\n",
            "Epoch: 6, Batch: 203, Loss: 0.3946, Accuracy: 70.97%, Precision: 68.27%\n",
            "Epoch: 6, Batch: 204, Loss: 0.3528, Accuracy: 70.98%, Precision: 68.27%\n",
            "Epoch: 6, Batch: 205, Loss: 0.3418, Accuracy: 70.99%, Precision: 68.28%\n",
            "Epoch: 6, Batch: 206, Loss: 0.3734, Accuracy: 70.99%, Precision: 68.29%\n",
            "Epoch: 6, Batch: 207, Loss: 0.3746, Accuracy: 71.00%, Precision: 68.29%\n",
            "Epoch: 6, Batch: 208, Loss: 0.3159, Accuracy: 71.01%, Precision: 68.30%\n",
            "Epoch: 6, Batch: 209, Loss: 0.4087, Accuracy: 71.02%, Precision: 68.31%\n",
            "Epoch: 6, Batch: 210, Loss: 0.4229, Accuracy: 71.02%, Precision: 68.32%\n",
            "Epoch: 6, Batch: 211, Loss: 0.4427, Accuracy: 71.02%, Precision: 68.32%\n",
            "Epoch: 6, Batch: 212, Loss: 0.4000, Accuracy: 71.03%, Precision: 68.33%\n",
            "Epoch: 6, Batch: 213, Loss: 0.3180, Accuracy: 71.04%, Precision: 68.34%\n",
            "Epoch: 6, Batch: 214, Loss: 0.4602, Accuracy: 71.04%, Precision: 68.35%\n",
            "Epoch: 6, Batch: 215, Loss: 0.5193, Accuracy: 71.05%, Precision: 68.35%\n",
            "Epoch: 6, Batch: 216, Loss: 0.3814, Accuracy: 71.05%, Precision: 68.36%\n",
            "Epoch: 6, Batch: 217, Loss: 0.4029, Accuracy: 71.06%, Precision: 68.36%\n",
            "Epoch: 6, Batch: 218, Loss: 0.3006, Accuracy: 71.07%, Precision: 68.37%\n",
            "Epoch: 6, Batch: 219, Loss: 0.3596, Accuracy: 71.07%, Precision: 68.38%\n",
            "Epoch: 6, Batch: 220, Loss: 0.4502, Accuracy: 71.08%, Precision: 68.38%\n",
            "Epoch: 6, Batch: 221, Loss: 0.3660, Accuracy: 71.09%, Precision: 68.39%\n",
            "Epoch: 6, Batch: 222, Loss: 0.3879, Accuracy: 71.09%, Precision: 68.39%\n",
            "Epoch: 6, Batch: 223, Loss: 0.4766, Accuracy: 71.10%, Precision: 68.40%\n",
            "Epoch: 6, Batch: 224, Loss: 0.4241, Accuracy: 71.10%, Precision: 68.41%\n",
            "Epoch: 6, Batch: 225, Loss: 0.4520, Accuracy: 71.10%, Precision: 68.40%\n",
            "Epoch: 6, Batch: 226, Loss: 0.3371, Accuracy: 71.11%, Precision: 68.40%\n",
            "Epoch: 6, Batch: 227, Loss: 0.5241, Accuracy: 71.11%, Precision: 68.41%\n",
            "Epoch: 6, Batch: 228, Loss: 0.3449, Accuracy: 71.12%, Precision: 68.42%\n",
            "Epoch: 6, Batch: 229, Loss: 0.3240, Accuracy: 71.13%, Precision: 68.43%\n",
            "Epoch: 6, Batch: 230, Loss: 0.4336, Accuracy: 71.14%, Precision: 68.43%\n",
            "Epoch: 6, Batch: 231, Loss: 0.3419, Accuracy: 71.15%, Precision: 68.43%\n",
            "Epoch: 6, Batch: 232, Loss: 0.3667, Accuracy: 71.15%, Precision: 68.44%\n",
            "Epoch: 6, Batch: 233, Loss: 0.4350, Accuracy: 71.16%, Precision: 68.44%\n",
            "Epoch: 6, Batch: 234, Loss: 0.3394, Accuracy: 71.17%, Precision: 68.45%\n",
            "Epoch: 6, Batch: 235, Loss: 0.4332, Accuracy: 71.17%, Precision: 68.46%\n",
            "Epoch: 6, Batch: 236, Loss: 0.3798, Accuracy: 71.17%, Precision: 68.46%\n",
            "Epoch: 6, Batch: 237, Loss: 0.3982, Accuracy: 71.18%, Precision: 68.47%\n",
            "Epoch: 6, Batch: 238, Loss: 0.5180, Accuracy: 71.18%, Precision: 68.47%\n",
            "Epoch: 6, Batch: 239, Loss: 0.6474, Accuracy: 71.18%, Precision: 68.47%\n",
            "Epoch: 6, Batch: 240, Loss: 0.3769, Accuracy: 71.19%, Precision: 68.47%\n",
            "Epoch: 6, Batch: 241, Loss: 0.3892, Accuracy: 71.19%, Precision: 68.48%\n",
            "Epoch: 6, Batch: 242, Loss: 0.4794, Accuracy: 71.19%, Precision: 68.48%\n",
            "Epoch: 6, Batch: 243, Loss: 0.3035, Accuracy: 71.19%, Precision: 68.48%\n",
            "Epoch: 7, Batch: 1, Loss: 0.4315, Accuracy: 71.20%, Precision: 68.49%\n",
            "Epoch: 7, Batch: 2, Loss: 0.3665, Accuracy: 71.21%, Precision: 68.50%\n",
            "Epoch: 7, Batch: 3, Loss: 0.3553, Accuracy: 71.22%, Precision: 68.51%\n",
            "Epoch: 7, Batch: 4, Loss: 0.3471, Accuracy: 71.23%, Precision: 68.52%\n",
            "Epoch: 7, Batch: 5, Loss: 0.3086, Accuracy: 71.24%, Precision: 68.53%\n",
            "Epoch: 7, Batch: 6, Loss: 0.3128, Accuracy: 71.24%, Precision: 68.54%\n",
            "Epoch: 7, Batch: 7, Loss: 0.3120, Accuracy: 71.26%, Precision: 68.55%\n",
            "Epoch: 7, Batch: 8, Loss: 0.3458, Accuracy: 71.26%, Precision: 68.56%\n",
            "Epoch: 7, Batch: 9, Loss: 0.4367, Accuracy: 71.27%, Precision: 68.56%\n",
            "Epoch: 7, Batch: 10, Loss: 0.4069, Accuracy: 71.27%, Precision: 68.56%\n",
            "Epoch: 7, Batch: 11, Loss: 0.3169, Accuracy: 71.28%, Precision: 68.57%\n",
            "Epoch: 7, Batch: 12, Loss: 0.3115, Accuracy: 71.29%, Precision: 68.57%\n",
            "Epoch: 7, Batch: 13, Loss: 0.3462, Accuracy: 71.29%, Precision: 68.58%\n",
            "Epoch: 7, Batch: 14, Loss: 0.2692, Accuracy: 71.30%, Precision: 68.59%\n",
            "Epoch: 7, Batch: 15, Loss: 0.3924, Accuracy: 71.31%, Precision: 68.60%\n",
            "Epoch: 7, Batch: 16, Loss: 0.2435, Accuracy: 71.32%, Precision: 68.62%\n",
            "Epoch: 7, Batch: 17, Loss: 0.2569, Accuracy: 71.33%, Precision: 68.62%\n",
            "Epoch: 7, Batch: 18, Loss: 0.4096, Accuracy: 71.34%, Precision: 68.63%\n",
            "Epoch: 7, Batch: 19, Loss: 0.2346, Accuracy: 71.35%, Precision: 68.64%\n",
            "Epoch: 7, Batch: 20, Loss: 0.2266, Accuracy: 71.36%, Precision: 68.64%\n",
            "Epoch: 7, Batch: 21, Loss: 0.2932, Accuracy: 71.36%, Precision: 68.65%\n",
            "Epoch: 7, Batch: 22, Loss: 0.2400, Accuracy: 71.37%, Precision: 68.66%\n",
            "Epoch: 7, Batch: 23, Loss: 0.3819, Accuracy: 71.38%, Precision: 68.67%\n",
            "Epoch: 7, Batch: 24, Loss: 0.3939, Accuracy: 71.38%, Precision: 68.67%\n",
            "Epoch: 7, Batch: 25, Loss: 0.4061, Accuracy: 71.39%, Precision: 68.68%\n",
            "Epoch: 7, Batch: 26, Loss: 0.3072, Accuracy: 71.40%, Precision: 68.69%\n",
            "Epoch: 7, Batch: 27, Loss: 0.2674, Accuracy: 71.41%, Precision: 68.71%\n",
            "Epoch: 7, Batch: 28, Loss: 0.2349, Accuracy: 71.42%, Precision: 68.72%\n",
            "Epoch: 7, Batch: 29, Loss: 0.2135, Accuracy: 71.43%, Precision: 68.73%\n",
            "Epoch: 7, Batch: 30, Loss: 0.5740, Accuracy: 71.43%, Precision: 68.73%\n",
            "Epoch: 7, Batch: 31, Loss: 0.2707, Accuracy: 71.44%, Precision: 68.74%\n",
            "Epoch: 7, Batch: 32, Loss: 0.3919, Accuracy: 71.45%, Precision: 68.74%\n",
            "Epoch: 7, Batch: 33, Loss: 0.2884, Accuracy: 71.45%, Precision: 68.75%\n",
            "Epoch: 7, Batch: 34, Loss: 0.2869, Accuracy: 71.47%, Precision: 68.76%\n",
            "Epoch: 7, Batch: 35, Loss: 0.4724, Accuracy: 71.47%, Precision: 68.76%\n",
            "Epoch: 7, Batch: 36, Loss: 0.4277, Accuracy: 71.47%, Precision: 68.77%\n",
            "Epoch: 7, Batch: 37, Loss: 0.2681, Accuracy: 71.48%, Precision: 68.78%\n",
            "Epoch: 7, Batch: 38, Loss: 0.2870, Accuracy: 71.49%, Precision: 68.79%\n",
            "Epoch: 7, Batch: 39, Loss: 0.2931, Accuracy: 71.50%, Precision: 68.80%\n",
            "Epoch: 7, Batch: 40, Loss: 0.3370, Accuracy: 71.50%, Precision: 68.81%\n",
            "Epoch: 7, Batch: 41, Loss: 0.2647, Accuracy: 71.51%, Precision: 68.82%\n",
            "Epoch: 7, Batch: 42, Loss: 0.3158, Accuracy: 71.52%, Precision: 68.82%\n",
            "Epoch: 7, Batch: 43, Loss: 0.3070, Accuracy: 71.53%, Precision: 68.82%\n",
            "Epoch: 7, Batch: 44, Loss: 0.4064, Accuracy: 71.53%, Precision: 68.83%\n",
            "Epoch: 7, Batch: 45, Loss: 0.2526, Accuracy: 71.54%, Precision: 68.84%\n",
            "Epoch: 7, Batch: 46, Loss: 0.4735, Accuracy: 71.55%, Precision: 68.85%\n",
            "Epoch: 7, Batch: 47, Loss: 0.3310, Accuracy: 71.56%, Precision: 68.86%\n",
            "Epoch: 7, Batch: 48, Loss: 0.3518, Accuracy: 71.56%, Precision: 68.86%\n",
            "Epoch: 7, Batch: 49, Loss: 0.2752, Accuracy: 71.57%, Precision: 68.87%\n",
            "Epoch: 7, Batch: 50, Loss: 0.4257, Accuracy: 71.57%, Precision: 68.88%\n",
            "Epoch: 7, Batch: 51, Loss: 0.3287, Accuracy: 71.58%, Precision: 68.89%\n",
            "Epoch: 7, Batch: 52, Loss: 0.4158, Accuracy: 71.58%, Precision: 68.89%\n",
            "Epoch: 7, Batch: 53, Loss: 0.2902, Accuracy: 71.59%, Precision: 68.90%\n",
            "Epoch: 7, Batch: 54, Loss: 0.3905, Accuracy: 71.60%, Precision: 68.91%\n",
            "Epoch: 7, Batch: 55, Loss: 0.2420, Accuracy: 71.61%, Precision: 68.92%\n",
            "Epoch: 7, Batch: 56, Loss: 0.4195, Accuracy: 71.61%, Precision: 68.91%\n",
            "Epoch: 7, Batch: 57, Loss: 0.3340, Accuracy: 71.62%, Precision: 68.92%\n",
            "Epoch: 7, Batch: 58, Loss: 0.3467, Accuracy: 71.62%, Precision: 68.92%\n",
            "Epoch: 7, Batch: 59, Loss: 0.2350, Accuracy: 71.63%, Precision: 68.94%\n",
            "Epoch: 7, Batch: 60, Loss: 0.3104, Accuracy: 71.64%, Precision: 68.94%\n",
            "Epoch: 7, Batch: 61, Loss: 0.3149, Accuracy: 71.65%, Precision: 68.95%\n",
            "Epoch: 7, Batch: 62, Loss: 0.4227, Accuracy: 71.65%, Precision: 68.96%\n",
            "Epoch: 7, Batch: 63, Loss: 0.4607, Accuracy: 71.66%, Precision: 68.96%\n",
            "Epoch: 7, Batch: 64, Loss: 0.3064, Accuracy: 71.66%, Precision: 68.97%\n",
            "Epoch: 7, Batch: 65, Loss: 0.3005, Accuracy: 71.67%, Precision: 68.98%\n",
            "Epoch: 7, Batch: 66, Loss: 0.4309, Accuracy: 71.67%, Precision: 68.98%\n",
            "Epoch: 7, Batch: 67, Loss: 0.2602, Accuracy: 71.68%, Precision: 68.99%\n",
            "Epoch: 7, Batch: 68, Loss: 0.4616, Accuracy: 71.68%, Precision: 68.99%\n",
            "Epoch: 7, Batch: 69, Loss: 0.3359, Accuracy: 71.69%, Precision: 69.00%\n",
            "Epoch: 7, Batch: 70, Loss: 0.3515, Accuracy: 71.70%, Precision: 69.01%\n",
            "Epoch: 7, Batch: 71, Loss: 0.3309, Accuracy: 71.71%, Precision: 69.01%\n",
            "Epoch: 7, Batch: 72, Loss: 0.2566, Accuracy: 71.72%, Precision: 69.03%\n",
            "Epoch: 7, Batch: 73, Loss: 0.3365, Accuracy: 71.73%, Precision: 69.03%\n",
            "Epoch: 7, Batch: 74, Loss: 0.3884, Accuracy: 71.73%, Precision: 69.04%\n",
            "Epoch: 7, Batch: 75, Loss: 0.3748, Accuracy: 71.74%, Precision: 69.04%\n",
            "Epoch: 7, Batch: 76, Loss: 0.3471, Accuracy: 71.74%, Precision: 69.04%\n",
            "Epoch: 7, Batch: 77, Loss: 0.3425, Accuracy: 71.75%, Precision: 69.05%\n",
            "Epoch: 7, Batch: 78, Loss: 0.2735, Accuracy: 71.75%, Precision: 69.05%\n",
            "Epoch: 7, Batch: 79, Loss: 0.4118, Accuracy: 71.76%, Precision: 69.06%\n",
            "Epoch: 7, Batch: 80, Loss: 0.2568, Accuracy: 71.77%, Precision: 69.07%\n",
            "Epoch: 7, Batch: 81, Loss: 0.3755, Accuracy: 71.77%, Precision: 69.08%\n",
            "Epoch: 7, Batch: 82, Loss: 0.3309, Accuracy: 71.78%, Precision: 69.08%\n",
            "Epoch: 7, Batch: 83, Loss: 0.3915, Accuracy: 71.78%, Precision: 69.08%\n",
            "Epoch: 7, Batch: 84, Loss: 0.2462, Accuracy: 71.79%, Precision: 69.10%\n",
            "Epoch: 7, Batch: 85, Loss: 0.2926, Accuracy: 71.80%, Precision: 69.11%\n",
            "Epoch: 7, Batch: 86, Loss: 0.2891, Accuracy: 71.81%, Precision: 69.12%\n",
            "Epoch: 7, Batch: 87, Loss: 0.2928, Accuracy: 71.82%, Precision: 69.13%\n",
            "Epoch: 7, Batch: 88, Loss: 0.3171, Accuracy: 71.83%, Precision: 69.14%\n",
            "Epoch: 7, Batch: 89, Loss: 0.3886, Accuracy: 71.84%, Precision: 69.14%\n",
            "Epoch: 7, Batch: 90, Loss: 0.3377, Accuracy: 71.84%, Precision: 69.15%\n",
            "Epoch: 7, Batch: 91, Loss: 0.3729, Accuracy: 71.85%, Precision: 69.15%\n",
            "Epoch: 7, Batch: 92, Loss: 0.3958, Accuracy: 71.86%, Precision: 69.16%\n",
            "Epoch: 7, Batch: 93, Loss: 0.2510, Accuracy: 71.87%, Precision: 69.17%\n",
            "Epoch: 7, Batch: 94, Loss: 0.4149, Accuracy: 71.87%, Precision: 69.18%\n",
            "Epoch: 7, Batch: 95, Loss: 0.3887, Accuracy: 71.87%, Precision: 69.19%\n",
            "Epoch: 7, Batch: 96, Loss: 0.3794, Accuracy: 71.88%, Precision: 69.19%\n",
            "Epoch: 7, Batch: 97, Loss: 0.3813, Accuracy: 71.88%, Precision: 69.20%\n",
            "Epoch: 7, Batch: 98, Loss: 0.4112, Accuracy: 71.89%, Precision: 69.20%\n",
            "Epoch: 7, Batch: 99, Loss: 0.3065, Accuracy: 71.90%, Precision: 69.21%\n",
            "Epoch: 7, Batch: 100, Loss: 0.5956, Accuracy: 71.90%, Precision: 69.21%\n",
            "Epoch: 7, Batch: 101, Loss: 0.3451, Accuracy: 71.91%, Precision: 69.21%\n",
            "Epoch: 7, Batch: 102, Loss: 0.3295, Accuracy: 71.91%, Precision: 69.22%\n",
            "Epoch: 7, Batch: 103, Loss: 0.3585, Accuracy: 71.92%, Precision: 69.23%\n",
            "Epoch: 7, Batch: 104, Loss: 0.4536, Accuracy: 71.92%, Precision: 69.23%\n",
            "Epoch: 7, Batch: 105, Loss: 0.4755, Accuracy: 71.92%, Precision: 69.24%\n",
            "Epoch: 7, Batch: 106, Loss: 0.3100, Accuracy: 71.93%, Precision: 69.24%\n",
            "Epoch: 7, Batch: 107, Loss: 0.4680, Accuracy: 71.93%, Precision: 69.24%\n",
            "Epoch: 7, Batch: 108, Loss: 0.5153, Accuracy: 71.93%, Precision: 69.24%\n",
            "Epoch: 7, Batch: 109, Loss: 0.3185, Accuracy: 71.94%, Precision: 69.25%\n",
            "Epoch: 7, Batch: 110, Loss: 0.3194, Accuracy: 71.95%, Precision: 69.26%\n",
            "Epoch: 7, Batch: 111, Loss: 0.4184, Accuracy: 71.95%, Precision: 69.26%\n",
            "Epoch: 7, Batch: 112, Loss: 0.3726, Accuracy: 71.96%, Precision: 69.27%\n",
            "Epoch: 7, Batch: 113, Loss: 0.4167, Accuracy: 71.96%, Precision: 69.28%\n",
            "Epoch: 7, Batch: 114, Loss: 0.5299, Accuracy: 71.97%, Precision: 69.29%\n",
            "Epoch: 7, Batch: 115, Loss: 0.4884, Accuracy: 71.97%, Precision: 69.30%\n",
            "Epoch: 7, Batch: 116, Loss: 0.2717, Accuracy: 71.98%, Precision: 69.31%\n",
            "Epoch: 7, Batch: 117, Loss: 0.4095, Accuracy: 71.98%, Precision: 69.32%\n",
            "Epoch: 7, Batch: 118, Loss: 0.4100, Accuracy: 71.99%, Precision: 69.32%\n",
            "Epoch: 7, Batch: 119, Loss: 0.4082, Accuracy: 71.99%, Precision: 69.33%\n",
            "Epoch: 7, Batch: 120, Loss: 0.4171, Accuracy: 71.99%, Precision: 69.33%\n",
            "Epoch: 7, Batch: 121, Loss: 0.3143, Accuracy: 72.00%, Precision: 69.34%\n",
            "Epoch: 7, Batch: 122, Loss: 0.3509, Accuracy: 72.01%, Precision: 69.35%\n",
            "Epoch: 7, Batch: 123, Loss: 0.3813, Accuracy: 72.01%, Precision: 69.35%\n",
            "Epoch: 7, Batch: 124, Loss: 0.4494, Accuracy: 72.02%, Precision: 69.35%\n",
            "Epoch: 7, Batch: 125, Loss: 0.3672, Accuracy: 72.02%, Precision: 69.36%\n",
            "Epoch: 7, Batch: 126, Loss: 0.4408, Accuracy: 72.03%, Precision: 69.36%\n",
            "Epoch: 7, Batch: 127, Loss: 0.4062, Accuracy: 72.03%, Precision: 69.36%\n",
            "Epoch: 7, Batch: 128, Loss: 0.3303, Accuracy: 72.04%, Precision: 69.37%\n",
            "Epoch: 7, Batch: 129, Loss: 0.4304, Accuracy: 72.04%, Precision: 69.38%\n",
            "Epoch: 7, Batch: 130, Loss: 0.3745, Accuracy: 72.05%, Precision: 69.39%\n",
            "Epoch: 7, Batch: 131, Loss: 0.3863, Accuracy: 72.06%, Precision: 69.40%\n",
            "Epoch: 7, Batch: 132, Loss: 0.3936, Accuracy: 72.06%, Precision: 69.40%\n",
            "Epoch: 7, Batch: 133, Loss: 0.4167, Accuracy: 72.07%, Precision: 69.40%\n",
            "Epoch: 7, Batch: 134, Loss: 0.3630, Accuracy: 72.07%, Precision: 69.40%\n",
            "Epoch: 7, Batch: 135, Loss: 0.3849, Accuracy: 72.08%, Precision: 69.41%\n",
            "Epoch: 7, Batch: 136, Loss: 0.4602, Accuracy: 72.09%, Precision: 69.42%\n",
            "Epoch: 7, Batch: 137, Loss: 0.4021, Accuracy: 72.09%, Precision: 69.42%\n",
            "Epoch: 7, Batch: 138, Loss: 0.3136, Accuracy: 72.10%, Precision: 69.43%\n",
            "Epoch: 7, Batch: 139, Loss: 0.4362, Accuracy: 72.10%, Precision: 69.44%\n",
            "Epoch: 7, Batch: 140, Loss: 0.2934, Accuracy: 72.11%, Precision: 69.45%\n",
            "Epoch: 7, Batch: 141, Loss: 0.3604, Accuracy: 72.12%, Precision: 69.45%\n",
            "Epoch: 7, Batch: 142, Loss: 0.3223, Accuracy: 72.13%, Precision: 69.46%\n",
            "Epoch: 7, Batch: 143, Loss: 0.3466, Accuracy: 72.13%, Precision: 69.47%\n",
            "Epoch: 7, Batch: 144, Loss: 0.2556, Accuracy: 72.14%, Precision: 69.48%\n",
            "Epoch: 7, Batch: 145, Loss: 0.2909, Accuracy: 72.15%, Precision: 69.49%\n",
            "Epoch: 7, Batch: 146, Loss: 0.2735, Accuracy: 72.16%, Precision: 69.51%\n",
            "Epoch: 7, Batch: 147, Loss: 0.3484, Accuracy: 72.17%, Precision: 69.51%\n",
            "Epoch: 7, Batch: 148, Loss: 0.2747, Accuracy: 72.18%, Precision: 69.52%\n",
            "Epoch: 7, Batch: 149, Loss: 0.3255, Accuracy: 72.18%, Precision: 69.52%\n",
            "Epoch: 7, Batch: 150, Loss: 0.3115, Accuracy: 72.19%, Precision: 69.53%\n",
            "Epoch: 7, Batch: 151, Loss: 0.2861, Accuracy: 72.20%, Precision: 69.55%\n",
            "Epoch: 7, Batch: 152, Loss: 0.4055, Accuracy: 72.20%, Precision: 69.55%\n",
            "Epoch: 7, Batch: 153, Loss: 0.4015, Accuracy: 72.20%, Precision: 69.55%\n",
            "Epoch: 7, Batch: 154, Loss: 0.3587, Accuracy: 72.21%, Precision: 69.55%\n",
            "Epoch: 7, Batch: 155, Loss: 0.6362, Accuracy: 72.20%, Precision: 69.54%\n",
            "Epoch: 7, Batch: 156, Loss: 0.3827, Accuracy: 72.21%, Precision: 69.55%\n",
            "Epoch: 7, Batch: 157, Loss: 0.3350, Accuracy: 72.21%, Precision: 69.56%\n",
            "Epoch: 7, Batch: 158, Loss: 0.4056, Accuracy: 72.22%, Precision: 69.56%\n",
            "Epoch: 7, Batch: 159, Loss: 0.3142, Accuracy: 72.22%, Precision: 69.57%\n",
            "Epoch: 7, Batch: 160, Loss: 0.4050, Accuracy: 72.23%, Precision: 69.58%\n",
            "Epoch: 7, Batch: 161, Loss: 0.3428, Accuracy: 72.24%, Precision: 69.59%\n",
            "Epoch: 7, Batch: 162, Loss: 0.4233, Accuracy: 72.24%, Precision: 69.59%\n",
            "Epoch: 7, Batch: 163, Loss: 0.4424, Accuracy: 72.24%, Precision: 69.59%\n",
            "Epoch: 7, Batch: 164, Loss: 0.3995, Accuracy: 72.25%, Precision: 69.59%\n",
            "Epoch: 7, Batch: 165, Loss: 0.4894, Accuracy: 72.25%, Precision: 69.59%\n",
            "Epoch: 7, Batch: 166, Loss: 0.3866, Accuracy: 72.26%, Precision: 69.60%\n",
            "Epoch: 7, Batch: 167, Loss: 0.3708, Accuracy: 72.26%, Precision: 69.60%\n",
            "Epoch: 7, Batch: 168, Loss: 0.4260, Accuracy: 72.27%, Precision: 69.61%\n",
            "Epoch: 7, Batch: 169, Loss: 0.3317, Accuracy: 72.27%, Precision: 69.62%\n",
            "Epoch: 7, Batch: 170, Loss: 0.5002, Accuracy: 72.27%, Precision: 69.62%\n",
            "Epoch: 7, Batch: 171, Loss: 0.3536, Accuracy: 72.27%, Precision: 69.62%\n",
            "Epoch: 7, Batch: 172, Loss: 0.2645, Accuracy: 72.28%, Precision: 69.64%\n",
            "Epoch: 7, Batch: 173, Loss: 0.4178, Accuracy: 72.28%, Precision: 69.63%\n",
            "Epoch: 7, Batch: 174, Loss: 0.4164, Accuracy: 72.29%, Precision: 69.64%\n",
            "Epoch: 7, Batch: 175, Loss: 0.3874, Accuracy: 72.30%, Precision: 69.65%\n",
            "Epoch: 7, Batch: 176, Loss: 0.4870, Accuracy: 72.30%, Precision: 69.65%\n",
            "Epoch: 7, Batch: 177, Loss: 0.3826, Accuracy: 72.30%, Precision: 69.65%\n",
            "Epoch: 7, Batch: 178, Loss: 0.3917, Accuracy: 72.30%, Precision: 69.65%\n",
            "Epoch: 7, Batch: 179, Loss: 0.3450, Accuracy: 72.31%, Precision: 69.66%\n",
            "Epoch: 7, Batch: 180, Loss: 0.3703, Accuracy: 72.32%, Precision: 69.66%\n",
            "Epoch: 7, Batch: 181, Loss: 0.3174, Accuracy: 72.33%, Precision: 69.67%\n",
            "Epoch: 7, Batch: 182, Loss: 0.3662, Accuracy: 72.33%, Precision: 69.68%\n",
            "Epoch: 7, Batch: 183, Loss: 0.3793, Accuracy: 72.34%, Precision: 69.68%\n",
            "Epoch: 7, Batch: 184, Loss: 0.2551, Accuracy: 72.34%, Precision: 69.69%\n",
            "Epoch: 7, Batch: 185, Loss: 0.4469, Accuracy: 72.35%, Precision: 69.70%\n",
            "Epoch: 7, Batch: 186, Loss: 0.2864, Accuracy: 72.35%, Precision: 69.70%\n",
            "Epoch: 7, Batch: 187, Loss: 0.3217, Accuracy: 72.36%, Precision: 69.71%\n",
            "Epoch: 7, Batch: 188, Loss: 0.3656, Accuracy: 72.36%, Precision: 69.72%\n",
            "Epoch: 7, Batch: 189, Loss: 0.3343, Accuracy: 72.38%, Precision: 69.73%\n",
            "Epoch: 7, Batch: 190, Loss: 0.3279, Accuracy: 72.38%, Precision: 69.73%\n",
            "Epoch: 7, Batch: 191, Loss: 0.3730, Accuracy: 72.39%, Precision: 69.74%\n",
            "Epoch: 7, Batch: 192, Loss: 0.3140, Accuracy: 72.40%, Precision: 69.74%\n",
            "Epoch: 7, Batch: 193, Loss: 0.3213, Accuracy: 72.40%, Precision: 69.75%\n",
            "Epoch: 7, Batch: 194, Loss: 0.5283, Accuracy: 72.41%, Precision: 69.76%\n",
            "Epoch: 7, Batch: 195, Loss: 0.5035, Accuracy: 72.41%, Precision: 69.76%\n",
            "Epoch: 7, Batch: 196, Loss: 0.3936, Accuracy: 72.41%, Precision: 69.76%\n",
            "Epoch: 7, Batch: 197, Loss: 0.2713, Accuracy: 72.42%, Precision: 69.76%\n",
            "Epoch: 7, Batch: 198, Loss: 0.2210, Accuracy: 72.43%, Precision: 69.77%\n",
            "Epoch: 7, Batch: 199, Loss: 0.3431, Accuracy: 72.44%, Precision: 69.78%\n",
            "Epoch: 7, Batch: 200, Loss: 0.4197, Accuracy: 72.44%, Precision: 69.79%\n",
            "Epoch: 7, Batch: 201, Loss: 0.2882, Accuracy: 72.45%, Precision: 69.80%\n",
            "Epoch: 7, Batch: 202, Loss: 0.5009, Accuracy: 72.46%, Precision: 69.80%\n",
            "Epoch: 7, Batch: 203, Loss: 0.2739, Accuracy: 72.47%, Precision: 69.82%\n",
            "Epoch: 7, Batch: 204, Loss: 0.4533, Accuracy: 72.47%, Precision: 69.82%\n",
            "Epoch: 7, Batch: 205, Loss: 0.2905, Accuracy: 72.48%, Precision: 69.83%\n",
            "Epoch: 7, Batch: 206, Loss: 0.3147, Accuracy: 72.48%, Precision: 69.83%\n",
            "Epoch: 7, Batch: 207, Loss: 0.3525, Accuracy: 72.49%, Precision: 69.84%\n",
            "Epoch: 7, Batch: 208, Loss: 0.4699, Accuracy: 72.49%, Precision: 69.84%\n",
            "Epoch: 7, Batch: 209, Loss: 0.4208, Accuracy: 72.50%, Precision: 69.84%\n",
            "Epoch: 7, Batch: 210, Loss: 0.4065, Accuracy: 72.50%, Precision: 69.84%\n",
            "Epoch: 7, Batch: 211, Loss: 0.3580, Accuracy: 72.51%, Precision: 69.85%\n",
            "Epoch: 7, Batch: 212, Loss: 0.4422, Accuracy: 72.51%, Precision: 69.85%\n",
            "Epoch: 7, Batch: 213, Loss: 0.3867, Accuracy: 72.51%, Precision: 69.85%\n",
            "Epoch: 7, Batch: 214, Loss: 0.3679, Accuracy: 72.52%, Precision: 69.86%\n",
            "Epoch: 7, Batch: 215, Loss: 0.4011, Accuracy: 72.52%, Precision: 69.87%\n",
            "Epoch: 7, Batch: 216, Loss: 0.3750, Accuracy: 72.52%, Precision: 69.87%\n",
            "Epoch: 7, Batch: 217, Loss: 0.3959, Accuracy: 72.53%, Precision: 69.88%\n",
            "Epoch: 7, Batch: 218, Loss: 0.4920, Accuracy: 72.53%, Precision: 69.87%\n",
            "Epoch: 7, Batch: 219, Loss: 0.3697, Accuracy: 72.53%, Precision: 69.88%\n",
            "Epoch: 7, Batch: 220, Loss: 0.2730, Accuracy: 72.54%, Precision: 69.88%\n",
            "Epoch: 7, Batch: 221, Loss: 0.5209, Accuracy: 72.54%, Precision: 69.89%\n",
            "Epoch: 7, Batch: 222, Loss: 0.3784, Accuracy: 72.55%, Precision: 69.90%\n",
            "Epoch: 7, Batch: 223, Loss: 0.3060, Accuracy: 72.56%, Precision: 69.91%\n",
            "Epoch: 7, Batch: 224, Loss: 0.3536, Accuracy: 72.56%, Precision: 69.91%\n",
            "Epoch: 7, Batch: 225, Loss: 0.3068, Accuracy: 72.57%, Precision: 69.91%\n",
            "Epoch: 7, Batch: 226, Loss: 0.3178, Accuracy: 72.57%, Precision: 69.92%\n",
            "Epoch: 7, Batch: 227, Loss: 0.3044, Accuracy: 72.58%, Precision: 69.93%\n",
            "Epoch: 7, Batch: 228, Loss: 0.3500, Accuracy: 72.59%, Precision: 69.93%\n",
            "Epoch: 7, Batch: 229, Loss: 0.3969, Accuracy: 72.59%, Precision: 69.94%\n",
            "Epoch: 7, Batch: 230, Loss: 0.4138, Accuracy: 72.60%, Precision: 69.95%\n",
            "Epoch: 7, Batch: 231, Loss: 0.3868, Accuracy: 72.60%, Precision: 69.95%\n",
            "Epoch: 7, Batch: 232, Loss: 0.3806, Accuracy: 72.61%, Precision: 69.95%\n",
            "Epoch: 7, Batch: 233, Loss: 0.3018, Accuracy: 72.62%, Precision: 69.96%\n",
            "Epoch: 7, Batch: 234, Loss: 0.3528, Accuracy: 72.62%, Precision: 69.96%\n",
            "Epoch: 7, Batch: 235, Loss: 0.2984, Accuracy: 72.63%, Precision: 69.97%\n",
            "Epoch: 7, Batch: 236, Loss: 0.3406, Accuracy: 72.63%, Precision: 69.98%\n",
            "Epoch: 7, Batch: 237, Loss: 0.3255, Accuracy: 72.64%, Precision: 69.99%\n",
            "Epoch: 7, Batch: 238, Loss: 0.4365, Accuracy: 72.64%, Precision: 70.00%\n",
            "Epoch: 7, Batch: 239, Loss: 0.3930, Accuracy: 72.65%, Precision: 70.00%\n",
            "Epoch: 7, Batch: 240, Loss: 0.3286, Accuracy: 72.65%, Precision: 70.01%\n",
            "Epoch: 7, Batch: 241, Loss: 0.4395, Accuracy: 72.66%, Precision: 70.01%\n",
            "Epoch: 7, Batch: 242, Loss: 0.4769, Accuracy: 72.66%, Precision: 70.01%\n",
            "Epoch: 7, Batch: 243, Loss: 0.4462, Accuracy: 72.66%, Precision: 70.01%\n",
            "Epoch: 8, Batch: 1, Loss: 0.3915, Accuracy: 72.67%, Precision: 70.01%\n",
            "Epoch: 8, Batch: 2, Loss: 0.2074, Accuracy: 72.68%, Precision: 70.02%\n",
            "Epoch: 8, Batch: 3, Loss: 0.3921, Accuracy: 72.68%, Precision: 70.03%\n",
            "Epoch: 8, Batch: 4, Loss: 0.5096, Accuracy: 72.68%, Precision: 70.03%\n",
            "Epoch: 8, Batch: 5, Loss: 0.3050, Accuracy: 72.69%, Precision: 70.04%\n",
            "Epoch: 8, Batch: 6, Loss: 0.3591, Accuracy: 72.69%, Precision: 70.05%\n",
            "Epoch: 8, Batch: 7, Loss: 0.2564, Accuracy: 72.70%, Precision: 70.05%\n",
            "Epoch: 8, Batch: 8, Loss: 0.2841, Accuracy: 72.71%, Precision: 70.06%\n",
            "Epoch: 8, Batch: 9, Loss: 0.2003, Accuracy: 72.72%, Precision: 70.07%\n",
            "Epoch: 8, Batch: 10, Loss: 0.2597, Accuracy: 72.73%, Precision: 70.07%\n",
            "Epoch: 8, Batch: 11, Loss: 0.4251, Accuracy: 72.73%, Precision: 70.08%\n",
            "Epoch: 8, Batch: 12, Loss: 0.3163, Accuracy: 72.74%, Precision: 70.09%\n",
            "Epoch: 8, Batch: 13, Loss: 0.3308, Accuracy: 72.75%, Precision: 70.09%\n",
            "Epoch: 8, Batch: 14, Loss: 0.2629, Accuracy: 72.76%, Precision: 70.10%\n",
            "Epoch: 8, Batch: 15, Loss: 0.2576, Accuracy: 72.77%, Precision: 70.11%\n",
            "Epoch: 8, Batch: 16, Loss: 0.2907, Accuracy: 72.78%, Precision: 70.11%\n",
            "Epoch: 8, Batch: 17, Loss: 0.2540, Accuracy: 72.78%, Precision: 70.12%\n",
            "Epoch: 8, Batch: 18, Loss: 0.2309, Accuracy: 72.79%, Precision: 70.13%\n",
            "Epoch: 8, Batch: 19, Loss: 0.2990, Accuracy: 72.80%, Precision: 70.14%\n",
            "Epoch: 8, Batch: 20, Loss: 0.3464, Accuracy: 72.81%, Precision: 70.15%\n",
            "Epoch: 8, Batch: 21, Loss: 0.2556, Accuracy: 72.82%, Precision: 70.16%\n",
            "Epoch: 8, Batch: 22, Loss: 0.3439, Accuracy: 72.82%, Precision: 70.16%\n",
            "Epoch: 8, Batch: 23, Loss: 0.3728, Accuracy: 72.83%, Precision: 70.17%\n",
            "Epoch: 8, Batch: 24, Loss: 0.2646, Accuracy: 72.84%, Precision: 70.17%\n",
            "Epoch: 8, Batch: 25, Loss: 0.1978, Accuracy: 72.85%, Precision: 70.19%\n",
            "Epoch: 8, Batch: 26, Loss: 0.3938, Accuracy: 72.85%, Precision: 70.19%\n",
            "Epoch: 8, Batch: 27, Loss: 0.2187, Accuracy: 72.86%, Precision: 70.21%\n",
            "Epoch: 8, Batch: 28, Loss: 0.1868, Accuracy: 72.87%, Precision: 70.22%\n",
            "Epoch: 8, Batch: 29, Loss: 0.2732, Accuracy: 72.88%, Precision: 70.23%\n",
            "Epoch: 8, Batch: 30, Loss: 0.2249, Accuracy: 72.89%, Precision: 70.23%\n",
            "Epoch: 8, Batch: 31, Loss: 0.2429, Accuracy: 72.90%, Precision: 70.24%\n",
            "Epoch: 8, Batch: 32, Loss: 0.2635, Accuracy: 72.91%, Precision: 70.25%\n",
            "Epoch: 8, Batch: 33, Loss: 0.2559, Accuracy: 72.92%, Precision: 70.26%\n",
            "Epoch: 8, Batch: 34, Loss: 0.3017, Accuracy: 72.92%, Precision: 70.26%\n",
            "Epoch: 8, Batch: 35, Loss: 0.2644, Accuracy: 72.93%, Precision: 70.27%\n",
            "Epoch: 8, Batch: 36, Loss: 0.2376, Accuracy: 72.94%, Precision: 70.28%\n",
            "Epoch: 8, Batch: 37, Loss: 0.2052, Accuracy: 72.95%, Precision: 70.29%\n",
            "Epoch: 8, Batch: 38, Loss: 0.2248, Accuracy: 72.96%, Precision: 70.30%\n",
            "Epoch: 8, Batch: 39, Loss: 0.2235, Accuracy: 72.97%, Precision: 70.31%\n",
            "Epoch: 8, Batch: 40, Loss: 0.2327, Accuracy: 72.98%, Precision: 70.32%\n",
            "Epoch: 8, Batch: 41, Loss: 0.3560, Accuracy: 72.98%, Precision: 70.33%\n",
            "Epoch: 8, Batch: 42, Loss: 0.4311, Accuracy: 72.99%, Precision: 70.33%\n",
            "Epoch: 8, Batch: 43, Loss: 0.4755, Accuracy: 72.99%, Precision: 70.33%\n",
            "Epoch: 8, Batch: 44, Loss: 0.3077, Accuracy: 73.00%, Precision: 70.34%\n",
            "Epoch: 8, Batch: 45, Loss: 0.3030, Accuracy: 73.00%, Precision: 70.35%\n",
            "Epoch: 8, Batch: 46, Loss: 0.2282, Accuracy: 73.01%, Precision: 70.35%\n",
            "Epoch: 8, Batch: 47, Loss: 0.2497, Accuracy: 73.02%, Precision: 70.36%\n",
            "Epoch: 8, Batch: 48, Loss: 0.2460, Accuracy: 73.02%, Precision: 70.37%\n",
            "Epoch: 8, Batch: 49, Loss: 0.4660, Accuracy: 73.03%, Precision: 70.37%\n",
            "Epoch: 8, Batch: 50, Loss: 0.2449, Accuracy: 73.03%, Precision: 70.38%\n",
            "Epoch: 8, Batch: 51, Loss: 0.2631, Accuracy: 73.04%, Precision: 70.38%\n",
            "Epoch: 8, Batch: 52, Loss: 0.2960, Accuracy: 73.05%, Precision: 70.39%\n",
            "Epoch: 8, Batch: 53, Loss: 0.2419, Accuracy: 73.06%, Precision: 70.40%\n",
            "Epoch: 8, Batch: 54, Loss: 0.2786, Accuracy: 73.07%, Precision: 70.41%\n",
            "Epoch: 8, Batch: 55, Loss: 0.3549, Accuracy: 73.07%, Precision: 70.41%\n",
            "Epoch: 8, Batch: 56, Loss: 0.3021, Accuracy: 73.08%, Precision: 70.42%\n",
            "Epoch: 8, Batch: 57, Loss: 0.1811, Accuracy: 73.09%, Precision: 70.43%\n",
            "Epoch: 8, Batch: 58, Loss: 0.3294, Accuracy: 73.09%, Precision: 70.44%\n",
            "Epoch: 8, Batch: 59, Loss: 0.2574, Accuracy: 73.10%, Precision: 70.45%\n",
            "Epoch: 8, Batch: 60, Loss: 0.3175, Accuracy: 73.11%, Precision: 70.45%\n",
            "Epoch: 8, Batch: 61, Loss: 0.3188, Accuracy: 73.12%, Precision: 70.46%\n",
            "Epoch: 8, Batch: 62, Loss: 0.2026, Accuracy: 73.13%, Precision: 70.47%\n",
            "Epoch: 8, Batch: 63, Loss: 0.2728, Accuracy: 73.13%, Precision: 70.48%\n",
            "Epoch: 8, Batch: 64, Loss: 0.3018, Accuracy: 73.14%, Precision: 70.49%\n",
            "Epoch: 8, Batch: 65, Loss: 0.2086, Accuracy: 73.15%, Precision: 70.50%\n",
            "Epoch: 8, Batch: 66, Loss: 0.2733, Accuracy: 73.15%, Precision: 70.50%\n",
            "Epoch: 8, Batch: 67, Loss: 0.2395, Accuracy: 73.16%, Precision: 70.51%\n",
            "Epoch: 8, Batch: 68, Loss: 0.2566, Accuracy: 73.17%, Precision: 70.52%\n",
            "Epoch: 8, Batch: 69, Loss: 0.3124, Accuracy: 73.18%, Precision: 70.53%\n",
            "Epoch: 8, Batch: 70, Loss: 0.2849, Accuracy: 73.19%, Precision: 70.54%\n",
            "Epoch: 8, Batch: 71, Loss: 0.2428, Accuracy: 73.20%, Precision: 70.55%\n",
            "Epoch: 8, Batch: 72, Loss: 0.3476, Accuracy: 73.20%, Precision: 70.55%\n",
            "Epoch: 8, Batch: 73, Loss: 0.2326, Accuracy: 73.21%, Precision: 70.56%\n",
            "Epoch: 8, Batch: 74, Loss: 0.2287, Accuracy: 73.22%, Precision: 70.57%\n",
            "Epoch: 8, Batch: 75, Loss: 0.3144, Accuracy: 73.23%, Precision: 70.57%\n",
            "Epoch: 8, Batch: 76, Loss: 0.2541, Accuracy: 73.23%, Precision: 70.58%\n",
            "Epoch: 8, Batch: 77, Loss: 0.2971, Accuracy: 73.24%, Precision: 70.59%\n",
            "Epoch: 8, Batch: 78, Loss: 0.4125, Accuracy: 73.25%, Precision: 70.59%\n",
            "Epoch: 8, Batch: 79, Loss: 0.1724, Accuracy: 73.26%, Precision: 70.60%\n",
            "Epoch: 8, Batch: 80, Loss: 0.2937, Accuracy: 73.26%, Precision: 70.61%\n",
            "Epoch: 8, Batch: 81, Loss: 0.2960, Accuracy: 73.27%, Precision: 70.62%\n",
            "Epoch: 8, Batch: 82, Loss: 0.1826, Accuracy: 73.28%, Precision: 70.63%\n",
            "Epoch: 8, Batch: 83, Loss: 0.3142, Accuracy: 73.28%, Precision: 70.64%\n",
            "Epoch: 8, Batch: 84, Loss: 0.2967, Accuracy: 73.29%, Precision: 70.64%\n",
            "Epoch: 8, Batch: 85, Loss: 0.2446, Accuracy: 73.30%, Precision: 70.65%\n",
            "Epoch: 8, Batch: 86, Loss: 0.3172, Accuracy: 73.30%, Precision: 70.66%\n",
            "Epoch: 8, Batch: 87, Loss: 0.2636, Accuracy: 73.31%, Precision: 70.66%\n",
            "Epoch: 8, Batch: 88, Loss: 0.3416, Accuracy: 73.32%, Precision: 70.67%\n",
            "Epoch: 8, Batch: 89, Loss: 0.2836, Accuracy: 73.32%, Precision: 70.67%\n",
            "Epoch: 8, Batch: 90, Loss: 0.2075, Accuracy: 73.33%, Precision: 70.68%\n",
            "Epoch: 8, Batch: 91, Loss: 0.3259, Accuracy: 73.34%, Precision: 70.69%\n",
            "Epoch: 8, Batch: 92, Loss: 0.3806, Accuracy: 73.34%, Precision: 70.69%\n",
            "Epoch: 8, Batch: 93, Loss: 0.2930, Accuracy: 73.34%, Precision: 70.70%\n",
            "Epoch: 8, Batch: 94, Loss: 0.3709, Accuracy: 73.35%, Precision: 70.70%\n",
            "Epoch: 8, Batch: 95, Loss: 0.2928, Accuracy: 73.35%, Precision: 70.71%\n",
            "Epoch: 8, Batch: 96, Loss: 0.3056, Accuracy: 73.36%, Precision: 70.71%\n",
            "Epoch: 8, Batch: 97, Loss: 0.3472, Accuracy: 73.36%, Precision: 70.72%\n",
            "Epoch: 8, Batch: 98, Loss: 0.2341, Accuracy: 73.37%, Precision: 70.73%\n",
            "Epoch: 8, Batch: 99, Loss: 0.2503, Accuracy: 73.38%, Precision: 70.73%\n",
            "Epoch: 8, Batch: 100, Loss: 0.2822, Accuracy: 73.39%, Precision: 70.74%\n",
            "Epoch: 8, Batch: 101, Loss: 0.3816, Accuracy: 73.39%, Precision: 70.75%\n",
            "Epoch: 8, Batch: 102, Loss: 0.2224, Accuracy: 73.40%, Precision: 70.76%\n",
            "Epoch: 8, Batch: 103, Loss: 0.2275, Accuracy: 73.41%, Precision: 70.77%\n",
            "Epoch: 8, Batch: 104, Loss: 0.3194, Accuracy: 73.41%, Precision: 70.77%\n",
            "Epoch: 8, Batch: 105, Loss: 0.3167, Accuracy: 73.42%, Precision: 70.78%\n",
            "Epoch: 8, Batch: 106, Loss: 0.2430, Accuracy: 73.43%, Precision: 70.79%\n",
            "Epoch: 8, Batch: 107, Loss: 0.2512, Accuracy: 73.43%, Precision: 70.80%\n",
            "Epoch: 8, Batch: 108, Loss: 0.2314, Accuracy: 73.44%, Precision: 70.81%\n",
            "Epoch: 8, Batch: 109, Loss: 0.2560, Accuracy: 73.45%, Precision: 70.82%\n",
            "Epoch: 8, Batch: 110, Loss: 0.2916, Accuracy: 73.45%, Precision: 70.82%\n",
            "Epoch: 8, Batch: 111, Loss: 0.2637, Accuracy: 73.46%, Precision: 70.83%\n",
            "Epoch: 8, Batch: 112, Loss: 0.2980, Accuracy: 73.46%, Precision: 70.84%\n",
            "Epoch: 8, Batch: 113, Loss: 0.2018, Accuracy: 73.47%, Precision: 70.85%\n",
            "Epoch: 8, Batch: 114, Loss: 0.3777, Accuracy: 73.48%, Precision: 70.85%\n",
            "Epoch: 8, Batch: 115, Loss: 0.3000, Accuracy: 73.48%, Precision: 70.86%\n",
            "Epoch: 8, Batch: 116, Loss: 0.3926, Accuracy: 73.49%, Precision: 70.86%\n",
            "Epoch: 8, Batch: 117, Loss: 0.2824, Accuracy: 73.49%, Precision: 70.87%\n",
            "Epoch: 8, Batch: 118, Loss: 0.3592, Accuracy: 73.50%, Precision: 70.87%\n",
            "Epoch: 8, Batch: 119, Loss: 0.2449, Accuracy: 73.51%, Precision: 70.89%\n",
            "Epoch: 8, Batch: 120, Loss: 0.1743, Accuracy: 73.52%, Precision: 70.90%\n",
            "Epoch: 8, Batch: 121, Loss: 0.3714, Accuracy: 73.52%, Precision: 70.90%\n",
            "Epoch: 8, Batch: 122, Loss: 0.3733, Accuracy: 73.52%, Precision: 70.91%\n",
            "Epoch: 8, Batch: 123, Loss: 0.3915, Accuracy: 73.53%, Precision: 70.91%\n",
            "Epoch: 8, Batch: 124, Loss: 0.1918, Accuracy: 73.54%, Precision: 70.92%\n",
            "Epoch: 8, Batch: 125, Loss: 0.2834, Accuracy: 73.54%, Precision: 70.93%\n",
            "Epoch: 8, Batch: 126, Loss: 0.2966, Accuracy: 73.55%, Precision: 70.93%\n",
            "Epoch: 8, Batch: 127, Loss: 0.4029, Accuracy: 73.55%, Precision: 70.93%\n",
            "Epoch: 8, Batch: 128, Loss: 0.2866, Accuracy: 73.56%, Precision: 70.94%\n",
            "Epoch: 8, Batch: 129, Loss: 0.3969, Accuracy: 73.56%, Precision: 70.94%\n",
            "Epoch: 8, Batch: 130, Loss: 0.3094, Accuracy: 73.57%, Precision: 70.95%\n",
            "Epoch: 8, Batch: 131, Loss: 0.2697, Accuracy: 73.58%, Precision: 70.95%\n",
            "Epoch: 8, Batch: 132, Loss: 0.2773, Accuracy: 73.58%, Precision: 70.96%\n",
            "Epoch: 8, Batch: 133, Loss: 0.3659, Accuracy: 73.59%, Precision: 70.97%\n",
            "Epoch: 8, Batch: 134, Loss: 0.2627, Accuracy: 73.60%, Precision: 70.98%\n",
            "Epoch: 8, Batch: 135, Loss: 0.2260, Accuracy: 73.61%, Precision: 70.98%\n",
            "Epoch: 8, Batch: 136, Loss: 0.3223, Accuracy: 73.61%, Precision: 70.99%\n",
            "Epoch: 8, Batch: 137, Loss: 0.4720, Accuracy: 73.61%, Precision: 70.99%\n",
            "Epoch: 8, Batch: 138, Loss: 0.2653, Accuracy: 73.62%, Precision: 71.00%\n",
            "Epoch: 8, Batch: 139, Loss: 0.3273, Accuracy: 73.63%, Precision: 71.01%\n",
            "Epoch: 8, Batch: 140, Loss: 0.3603, Accuracy: 73.63%, Precision: 71.02%\n",
            "Epoch: 8, Batch: 141, Loss: 0.2583, Accuracy: 73.64%, Precision: 71.02%\n",
            "Epoch: 8, Batch: 142, Loss: 0.2814, Accuracy: 73.64%, Precision: 71.03%\n",
            "Epoch: 8, Batch: 143, Loss: 0.2871, Accuracy: 73.65%, Precision: 71.04%\n",
            "Epoch: 8, Batch: 144, Loss: 0.2863, Accuracy: 73.66%, Precision: 71.04%\n",
            "Epoch: 8, Batch: 145, Loss: 0.2756, Accuracy: 73.66%, Precision: 71.05%\n",
            "Epoch: 8, Batch: 146, Loss: 0.2576, Accuracy: 73.67%, Precision: 71.05%\n",
            "Epoch: 8, Batch: 147, Loss: 0.3237, Accuracy: 73.68%, Precision: 71.06%\n",
            "Epoch: 8, Batch: 148, Loss: 0.2984, Accuracy: 73.68%, Precision: 71.07%\n",
            "Epoch: 8, Batch: 149, Loss: 0.3238, Accuracy: 73.69%, Precision: 71.07%\n",
            "Epoch: 8, Batch: 150, Loss: 0.2422, Accuracy: 73.69%, Precision: 71.08%\n",
            "Epoch: 8, Batch: 151, Loss: 0.2524, Accuracy: 73.70%, Precision: 71.08%\n",
            "Epoch: 8, Batch: 152, Loss: 0.2779, Accuracy: 73.71%, Precision: 71.09%\n",
            "Epoch: 8, Batch: 153, Loss: 0.2899, Accuracy: 73.71%, Precision: 71.09%\n",
            "Epoch: 8, Batch: 154, Loss: 0.2661, Accuracy: 73.72%, Precision: 71.10%\n",
            "Epoch: 8, Batch: 155, Loss: 0.3102, Accuracy: 73.73%, Precision: 71.10%\n",
            "Epoch: 8, Batch: 156, Loss: 0.2819, Accuracy: 73.73%, Precision: 71.11%\n",
            "Epoch: 8, Batch: 157, Loss: 0.2046, Accuracy: 73.74%, Precision: 71.12%\n",
            "Epoch: 8, Batch: 158, Loss: 0.3406, Accuracy: 73.75%, Precision: 71.13%\n",
            "Epoch: 8, Batch: 159, Loss: 0.2735, Accuracy: 73.75%, Precision: 71.14%\n",
            "Epoch: 8, Batch: 160, Loss: 0.3993, Accuracy: 73.76%, Precision: 71.14%\n",
            "Epoch: 8, Batch: 161, Loss: 0.3073, Accuracy: 73.76%, Precision: 71.15%\n",
            "Epoch: 8, Batch: 162, Loss: 0.3220, Accuracy: 73.77%, Precision: 71.15%\n",
            "Epoch: 8, Batch: 163, Loss: 0.3573, Accuracy: 73.77%, Precision: 71.15%\n",
            "Epoch: 8, Batch: 164, Loss: 0.2028, Accuracy: 73.78%, Precision: 71.16%\n",
            "Epoch: 8, Batch: 165, Loss: 0.4344, Accuracy: 73.78%, Precision: 71.17%\n",
            "Epoch: 8, Batch: 166, Loss: 0.3277, Accuracy: 73.79%, Precision: 71.18%\n",
            "Epoch: 8, Batch: 167, Loss: 0.4164, Accuracy: 73.79%, Precision: 71.18%\n",
            "Epoch: 8, Batch: 168, Loss: 0.2849, Accuracy: 73.80%, Precision: 71.19%\n",
            "Epoch: 8, Batch: 169, Loss: 0.3377, Accuracy: 73.81%, Precision: 71.20%\n",
            "Epoch: 8, Batch: 170, Loss: 0.4471, Accuracy: 73.81%, Precision: 71.19%\n",
            "Epoch: 8, Batch: 171, Loss: 0.3412, Accuracy: 73.81%, Precision: 71.20%\n",
            "Epoch: 8, Batch: 172, Loss: 0.3354, Accuracy: 73.81%, Precision: 71.20%\n",
            "Epoch: 8, Batch: 173, Loss: 0.2879, Accuracy: 73.82%, Precision: 71.21%\n",
            "Epoch: 8, Batch: 174, Loss: 0.3203, Accuracy: 73.82%, Precision: 71.21%\n",
            "Epoch: 8, Batch: 175, Loss: 0.2133, Accuracy: 73.83%, Precision: 71.23%\n",
            "Epoch: 8, Batch: 176, Loss: 0.2843, Accuracy: 73.84%, Precision: 71.23%\n",
            "Epoch: 8, Batch: 177, Loss: 0.3228, Accuracy: 73.84%, Precision: 71.23%\n",
            "Epoch: 8, Batch: 178, Loss: 0.2608, Accuracy: 73.85%, Precision: 71.24%\n",
            "Epoch: 8, Batch: 179, Loss: 0.2712, Accuracy: 73.86%, Precision: 71.25%\n",
            "Epoch: 8, Batch: 180, Loss: 0.3778, Accuracy: 73.86%, Precision: 71.25%\n",
            "Epoch: 8, Batch: 181, Loss: 0.1991, Accuracy: 73.86%, Precision: 71.26%\n",
            "Epoch: 8, Batch: 182, Loss: 0.2816, Accuracy: 73.87%, Precision: 71.27%\n",
            "Epoch: 8, Batch: 183, Loss: 0.2049, Accuracy: 73.88%, Precision: 71.27%\n",
            "Epoch: 8, Batch: 184, Loss: 0.3619, Accuracy: 73.88%, Precision: 71.28%\n",
            "Epoch: 8, Batch: 185, Loss: 0.1567, Accuracy: 73.89%, Precision: 71.29%\n",
            "Epoch: 8, Batch: 186, Loss: 0.2953, Accuracy: 73.90%, Precision: 71.29%\n",
            "Epoch: 8, Batch: 187, Loss: 0.3527, Accuracy: 73.91%, Precision: 71.30%\n",
            "Epoch: 8, Batch: 188, Loss: 0.3763, Accuracy: 73.91%, Precision: 71.30%\n",
            "Epoch: 8, Batch: 189, Loss: 0.4003, Accuracy: 73.91%, Precision: 71.31%\n",
            "Epoch: 8, Batch: 190, Loss: 0.2413, Accuracy: 73.92%, Precision: 71.31%\n",
            "Epoch: 8, Batch: 191, Loss: 0.2106, Accuracy: 73.93%, Precision: 71.32%\n",
            "Epoch: 8, Batch: 192, Loss: 0.3059, Accuracy: 73.93%, Precision: 71.32%\n",
            "Epoch: 8, Batch: 193, Loss: 0.2228, Accuracy: 73.94%, Precision: 71.33%\n",
            "Epoch: 8, Batch: 194, Loss: 0.3318, Accuracy: 73.95%, Precision: 71.33%\n",
            "Epoch: 8, Batch: 195, Loss: 0.4104, Accuracy: 73.95%, Precision: 71.33%\n",
            "Epoch: 8, Batch: 196, Loss: 0.3248, Accuracy: 73.95%, Precision: 71.34%\n",
            "Epoch: 8, Batch: 197, Loss: 0.3341, Accuracy: 73.96%, Precision: 71.35%\n",
            "Epoch: 8, Batch: 198, Loss: 0.3138, Accuracy: 73.96%, Precision: 71.35%\n",
            "Epoch: 8, Batch: 199, Loss: 0.4100, Accuracy: 73.97%, Precision: 71.36%\n",
            "Epoch: 8, Batch: 200, Loss: 0.2078, Accuracy: 73.98%, Precision: 71.37%\n",
            "Epoch: 8, Batch: 201, Loss: 0.2610, Accuracy: 73.98%, Precision: 71.38%\n",
            "Epoch: 8, Batch: 202, Loss: 0.2596, Accuracy: 73.99%, Precision: 71.39%\n",
            "Epoch: 8, Batch: 203, Loss: 0.3410, Accuracy: 73.99%, Precision: 71.39%\n",
            "Epoch: 8, Batch: 204, Loss: 0.4706, Accuracy: 73.99%, Precision: 71.39%\n",
            "Epoch: 8, Batch: 205, Loss: 0.2654, Accuracy: 74.00%, Precision: 71.40%\n",
            "Epoch: 8, Batch: 206, Loss: 0.3300, Accuracy: 74.01%, Precision: 71.40%\n",
            "Epoch: 8, Batch: 207, Loss: 0.3741, Accuracy: 74.01%, Precision: 71.40%\n",
            "Epoch: 8, Batch: 208, Loss: 0.2909, Accuracy: 74.02%, Precision: 71.41%\n",
            "Epoch: 8, Batch: 209, Loss: 0.3420, Accuracy: 74.02%, Precision: 71.41%\n",
            "Epoch: 8, Batch: 210, Loss: 0.3734, Accuracy: 74.02%, Precision: 71.42%\n",
            "Epoch: 8, Batch: 211, Loss: 0.2499, Accuracy: 74.03%, Precision: 71.43%\n",
            "Epoch: 8, Batch: 212, Loss: 0.3025, Accuracy: 74.03%, Precision: 71.43%\n",
            "Epoch: 8, Batch: 213, Loss: 0.3044, Accuracy: 74.04%, Precision: 71.43%\n",
            "Epoch: 8, Batch: 214, Loss: 0.3482, Accuracy: 74.04%, Precision: 71.43%\n",
            "Epoch: 8, Batch: 215, Loss: 0.3174, Accuracy: 74.05%, Precision: 71.44%\n",
            "Epoch: 8, Batch: 216, Loss: 0.4033, Accuracy: 74.05%, Precision: 71.44%\n",
            "Epoch: 8, Batch: 217, Loss: 0.2380, Accuracy: 74.06%, Precision: 71.45%\n",
            "Epoch: 8, Batch: 218, Loss: 0.3589, Accuracy: 74.06%, Precision: 71.45%\n",
            "Epoch: 8, Batch: 219, Loss: 0.2004, Accuracy: 74.07%, Precision: 71.46%\n",
            "Epoch: 8, Batch: 220, Loss: 0.3207, Accuracy: 74.07%, Precision: 71.46%\n",
            "Epoch: 8, Batch: 221, Loss: 0.3324, Accuracy: 74.08%, Precision: 71.47%\n",
            "Epoch: 8, Batch: 222, Loss: 0.2603, Accuracy: 74.08%, Precision: 71.47%\n",
            "Epoch: 8, Batch: 223, Loss: 0.4310, Accuracy: 74.08%, Precision: 71.48%\n",
            "Epoch: 8, Batch: 224, Loss: 0.2790, Accuracy: 74.09%, Precision: 71.49%\n",
            "Epoch: 8, Batch: 225, Loss: 0.2997, Accuracy: 74.09%, Precision: 71.49%\n",
            "Epoch: 8, Batch: 226, Loss: 0.3350, Accuracy: 74.10%, Precision: 71.50%\n",
            "Epoch: 8, Batch: 227, Loss: 0.3727, Accuracy: 74.10%, Precision: 71.50%\n",
            "Epoch: 8, Batch: 228, Loss: 0.4500, Accuracy: 74.11%, Precision: 71.50%\n",
            "Epoch: 8, Batch: 229, Loss: 0.2646, Accuracy: 74.11%, Precision: 71.51%\n",
            "Epoch: 8, Batch: 230, Loss: 0.2356, Accuracy: 74.12%, Precision: 71.51%\n",
            "Epoch: 8, Batch: 231, Loss: 0.3009, Accuracy: 74.13%, Precision: 71.52%\n",
            "Epoch: 8, Batch: 232, Loss: 0.4139, Accuracy: 74.13%, Precision: 71.51%\n",
            "Epoch: 8, Batch: 233, Loss: 0.3387, Accuracy: 74.13%, Precision: 71.52%\n",
            "Epoch: 8, Batch: 234, Loss: 0.2895, Accuracy: 74.13%, Precision: 71.52%\n",
            "Epoch: 8, Batch: 235, Loss: 0.3056, Accuracy: 74.14%, Precision: 71.53%\n",
            "Epoch: 8, Batch: 236, Loss: 0.2735, Accuracy: 74.14%, Precision: 71.54%\n",
            "Epoch: 8, Batch: 237, Loss: 0.4055, Accuracy: 74.15%, Precision: 71.54%\n",
            "Epoch: 8, Batch: 238, Loss: 0.2883, Accuracy: 74.15%, Precision: 71.55%\n",
            "Epoch: 8, Batch: 239, Loss: 0.2379, Accuracy: 74.16%, Precision: 71.56%\n",
            "Epoch: 8, Batch: 240, Loss: 0.2348, Accuracy: 74.16%, Precision: 71.56%\n",
            "Epoch: 8, Batch: 241, Loss: 0.3756, Accuracy: 74.17%, Precision: 71.57%\n",
            "Epoch: 8, Batch: 242, Loss: 0.3321, Accuracy: 74.17%, Precision: 71.57%\n",
            "Epoch: 8, Batch: 243, Loss: 0.3252, Accuracy: 74.17%, Precision: 71.57%\n",
            "Epoch: 9, Batch: 1, Loss: 0.1747, Accuracy: 74.18%, Precision: 71.58%\n",
            "Epoch: 9, Batch: 2, Loss: 0.2079, Accuracy: 74.18%, Precision: 71.58%\n",
            "Epoch: 9, Batch: 3, Loss: 0.1999, Accuracy: 74.19%, Precision: 71.59%\n",
            "Epoch: 9, Batch: 4, Loss: 0.2057, Accuracy: 74.20%, Precision: 71.60%\n",
            "Epoch: 9, Batch: 5, Loss: 0.1816, Accuracy: 74.21%, Precision: 71.61%\n",
            "Epoch: 9, Batch: 6, Loss: 0.2207, Accuracy: 74.22%, Precision: 71.62%\n",
            "Epoch: 9, Batch: 7, Loss: 0.2807, Accuracy: 74.23%, Precision: 71.63%\n",
            "Epoch: 9, Batch: 8, Loss: 0.1919, Accuracy: 74.23%, Precision: 71.63%\n",
            "Epoch: 9, Batch: 9, Loss: 0.1844, Accuracy: 74.24%, Precision: 71.64%\n",
            "Epoch: 9, Batch: 10, Loss: 0.1773, Accuracy: 74.25%, Precision: 71.65%\n",
            "Epoch: 9, Batch: 11, Loss: 0.1611, Accuracy: 74.26%, Precision: 71.66%\n",
            "Epoch: 9, Batch: 12, Loss: 0.1662, Accuracy: 74.27%, Precision: 71.67%\n",
            "Epoch: 9, Batch: 13, Loss: 0.1178, Accuracy: 74.28%, Precision: 71.68%\n",
            "Epoch: 9, Batch: 14, Loss: 0.2746, Accuracy: 74.28%, Precision: 71.69%\n",
            "Epoch: 9, Batch: 15, Loss: 0.2639, Accuracy: 74.28%, Precision: 71.69%\n",
            "Epoch: 9, Batch: 16, Loss: 0.1734, Accuracy: 74.29%, Precision: 71.70%\n",
            "Epoch: 9, Batch: 17, Loss: 0.1703, Accuracy: 74.30%, Precision: 71.71%\n",
            "Epoch: 9, Batch: 18, Loss: 0.2683, Accuracy: 74.31%, Precision: 71.72%\n",
            "Epoch: 9, Batch: 19, Loss: 0.2098, Accuracy: 74.31%, Precision: 71.73%\n",
            "Epoch: 9, Batch: 20, Loss: 0.1820, Accuracy: 74.32%, Precision: 71.73%\n",
            "Epoch: 9, Batch: 21, Loss: 0.2079, Accuracy: 74.33%, Precision: 71.74%\n",
            "Epoch: 9, Batch: 22, Loss: 0.2036, Accuracy: 74.34%, Precision: 71.75%\n",
            "Epoch: 9, Batch: 23, Loss: 0.2990, Accuracy: 74.34%, Precision: 71.76%\n",
            "Epoch: 9, Batch: 24, Loss: 0.2522, Accuracy: 74.35%, Precision: 71.77%\n",
            "Epoch: 9, Batch: 25, Loss: 0.3042, Accuracy: 74.35%, Precision: 71.77%\n",
            "Epoch: 9, Batch: 26, Loss: 0.2029, Accuracy: 74.36%, Precision: 71.78%\n",
            "Epoch: 9, Batch: 27, Loss: 0.1498, Accuracy: 74.37%, Precision: 71.79%\n",
            "Epoch: 9, Batch: 28, Loss: 0.1688, Accuracy: 74.38%, Precision: 71.80%\n",
            "Epoch: 9, Batch: 29, Loss: 0.3661, Accuracy: 74.38%, Precision: 71.80%\n",
            "Epoch: 9, Batch: 30, Loss: 0.2923, Accuracy: 74.39%, Precision: 71.80%\n",
            "Epoch: 9, Batch: 31, Loss: 0.3133, Accuracy: 74.39%, Precision: 71.81%\n",
            "Epoch: 9, Batch: 32, Loss: 0.1523, Accuracy: 74.40%, Precision: 71.82%\n",
            "Epoch: 9, Batch: 33, Loss: 0.1436, Accuracy: 74.41%, Precision: 71.83%\n",
            "Epoch: 9, Batch: 34, Loss: 0.1860, Accuracy: 74.42%, Precision: 71.84%\n",
            "Epoch: 9, Batch: 35, Loss: 0.1609, Accuracy: 74.43%, Precision: 71.85%\n",
            "Epoch: 9, Batch: 36, Loss: 0.2311, Accuracy: 74.43%, Precision: 71.85%\n",
            "Epoch: 9, Batch: 37, Loss: 0.1989, Accuracy: 74.44%, Precision: 71.86%\n",
            "Epoch: 9, Batch: 38, Loss: 0.2822, Accuracy: 74.45%, Precision: 71.86%\n",
            "Epoch: 9, Batch: 39, Loss: 0.1461, Accuracy: 74.45%, Precision: 71.87%\n",
            "Epoch: 9, Batch: 40, Loss: 0.2310, Accuracy: 74.46%, Precision: 71.88%\n",
            "Epoch: 9, Batch: 41, Loss: 0.1989, Accuracy: 74.47%, Precision: 71.89%\n",
            "Epoch: 9, Batch: 42, Loss: 0.1510, Accuracy: 74.48%, Precision: 71.90%\n",
            "Epoch: 9, Batch: 43, Loss: 0.2661, Accuracy: 74.49%, Precision: 71.91%\n",
            "Epoch: 9, Batch: 44, Loss: 0.2468, Accuracy: 74.49%, Precision: 71.92%\n",
            "Epoch: 9, Batch: 45, Loss: 0.3566, Accuracy: 74.50%, Precision: 71.92%\n",
            "Epoch: 9, Batch: 46, Loss: 0.2577, Accuracy: 74.50%, Precision: 71.93%\n",
            "Epoch: 9, Batch: 47, Loss: 0.2369, Accuracy: 74.51%, Precision: 71.94%\n",
            "Epoch: 9, Batch: 48, Loss: 0.2360, Accuracy: 74.52%, Precision: 71.95%\n",
            "Epoch: 9, Batch: 49, Loss: 0.1849, Accuracy: 74.53%, Precision: 71.95%\n",
            "Epoch: 9, Batch: 50, Loss: 0.3267, Accuracy: 74.53%, Precision: 71.96%\n",
            "Epoch: 9, Batch: 51, Loss: 0.3114, Accuracy: 74.53%, Precision: 71.96%\n",
            "Epoch: 9, Batch: 52, Loss: 0.2532, Accuracy: 74.54%, Precision: 71.97%\n",
            "Epoch: 9, Batch: 53, Loss: 0.2256, Accuracy: 74.55%, Precision: 71.98%\n",
            "Epoch: 9, Batch: 54, Loss: 0.1704, Accuracy: 74.56%, Precision: 71.99%\n",
            "Epoch: 9, Batch: 55, Loss: 0.1918, Accuracy: 74.57%, Precision: 72.00%\n",
            "Epoch: 9, Batch: 56, Loss: 0.2236, Accuracy: 74.57%, Precision: 72.01%\n",
            "Epoch: 9, Batch: 57, Loss: 0.1987, Accuracy: 74.58%, Precision: 72.02%\n",
            "Epoch: 9, Batch: 58, Loss: 0.2596, Accuracy: 74.58%, Precision: 72.02%\n",
            "Epoch: 9, Batch: 59, Loss: 0.1940, Accuracy: 74.59%, Precision: 72.02%\n",
            "Epoch: 9, Batch: 60, Loss: 0.1725, Accuracy: 74.60%, Precision: 72.02%\n",
            "Epoch: 9, Batch: 61, Loss: 0.3262, Accuracy: 74.60%, Precision: 72.03%\n",
            "Epoch: 9, Batch: 62, Loss: 0.3063, Accuracy: 74.61%, Precision: 72.03%\n",
            "Epoch: 9, Batch: 63, Loss: 0.2373, Accuracy: 74.61%, Precision: 72.04%\n",
            "Epoch: 9, Batch: 64, Loss: 0.1708, Accuracy: 74.62%, Precision: 72.05%\n",
            "Epoch: 9, Batch: 65, Loss: 0.2099, Accuracy: 74.63%, Precision: 72.05%\n",
            "Epoch: 9, Batch: 66, Loss: 0.1890, Accuracy: 74.64%, Precision: 72.06%\n",
            "Epoch: 9, Batch: 67, Loss: 0.1999, Accuracy: 74.64%, Precision: 72.07%\n",
            "Epoch: 9, Batch: 68, Loss: 0.1874, Accuracy: 74.65%, Precision: 72.08%\n",
            "Epoch: 9, Batch: 69, Loss: 0.4002, Accuracy: 74.65%, Precision: 72.09%\n",
            "Epoch: 9, Batch: 70, Loss: 0.2572, Accuracy: 74.66%, Precision: 72.09%\n",
            "Epoch: 9, Batch: 71, Loss: 0.2682, Accuracy: 74.67%, Precision: 72.10%\n",
            "Epoch: 9, Batch: 72, Loss: 0.2916, Accuracy: 74.67%, Precision: 72.10%\n",
            "Epoch: 9, Batch: 73, Loss: 0.3226, Accuracy: 74.68%, Precision: 72.11%\n",
            "Epoch: 9, Batch: 74, Loss: 0.2105, Accuracy: 74.68%, Precision: 72.11%\n",
            "Epoch: 9, Batch: 75, Loss: 0.1755, Accuracy: 74.69%, Precision: 72.12%\n",
            "Epoch: 9, Batch: 76, Loss: 0.1847, Accuracy: 74.70%, Precision: 72.13%\n",
            "Epoch: 9, Batch: 77, Loss: 0.2490, Accuracy: 74.70%, Precision: 72.13%\n",
            "Epoch: 9, Batch: 78, Loss: 0.3913, Accuracy: 74.71%, Precision: 72.14%\n",
            "Epoch: 9, Batch: 79, Loss: 0.1506, Accuracy: 74.72%, Precision: 72.15%\n",
            "Epoch: 9, Batch: 80, Loss: 0.1920, Accuracy: 74.72%, Precision: 72.16%\n",
            "Epoch: 9, Batch: 81, Loss: 0.1636, Accuracy: 74.73%, Precision: 72.17%\n",
            "Epoch: 9, Batch: 82, Loss: 0.2738, Accuracy: 74.73%, Precision: 72.17%\n",
            "Epoch: 9, Batch: 83, Loss: 0.2010, Accuracy: 74.74%, Precision: 72.17%\n",
            "Epoch: 9, Batch: 84, Loss: 0.1286, Accuracy: 74.75%, Precision: 72.18%\n",
            "Epoch: 9, Batch: 85, Loss: 0.1467, Accuracy: 74.76%, Precision: 72.19%\n",
            "Epoch: 9, Batch: 86, Loss: 0.1451, Accuracy: 74.77%, Precision: 72.20%\n",
            "Epoch: 9, Batch: 87, Loss: 0.3123, Accuracy: 74.77%, Precision: 72.20%\n",
            "Epoch: 9, Batch: 88, Loss: 0.2602, Accuracy: 74.78%, Precision: 72.21%\n",
            "Epoch: 9, Batch: 89, Loss: 0.3094, Accuracy: 74.78%, Precision: 72.22%\n",
            "Epoch: 9, Batch: 90, Loss: 0.2688, Accuracy: 74.79%, Precision: 72.22%\n",
            "Epoch: 9, Batch: 91, Loss: 0.2516, Accuracy: 74.79%, Precision: 72.23%\n",
            "Epoch: 9, Batch: 92, Loss: 0.2474, Accuracy: 74.80%, Precision: 72.23%\n",
            "Epoch: 9, Batch: 93, Loss: 0.3387, Accuracy: 74.80%, Precision: 72.24%\n",
            "Epoch: 9, Batch: 94, Loss: 0.3296, Accuracy: 74.81%, Precision: 72.24%\n",
            "Epoch: 9, Batch: 95, Loss: 0.2846, Accuracy: 74.81%, Precision: 72.24%\n",
            "Epoch: 9, Batch: 96, Loss: 0.3565, Accuracy: 74.81%, Precision: 72.25%\n",
            "Epoch: 9, Batch: 97, Loss: 0.2292, Accuracy: 74.82%, Precision: 72.25%\n",
            "Epoch: 9, Batch: 98, Loss: 0.2556, Accuracy: 74.83%, Precision: 72.26%\n",
            "Epoch: 9, Batch: 99, Loss: 0.2765, Accuracy: 74.83%, Precision: 72.26%\n",
            "Epoch: 9, Batch: 100, Loss: 0.2552, Accuracy: 74.84%, Precision: 72.27%\n",
            "Epoch: 9, Batch: 101, Loss: 0.4904, Accuracy: 74.84%, Precision: 72.27%\n",
            "Epoch: 9, Batch: 102, Loss: 0.2059, Accuracy: 74.85%, Precision: 72.28%\n",
            "Epoch: 9, Batch: 103, Loss: 0.3198, Accuracy: 74.86%, Precision: 72.29%\n",
            "Epoch: 9, Batch: 104, Loss: 0.2560, Accuracy: 74.86%, Precision: 72.30%\n",
            "Epoch: 9, Batch: 105, Loss: 0.3220, Accuracy: 74.87%, Precision: 72.30%\n",
            "Epoch: 9, Batch: 106, Loss: 0.3267, Accuracy: 74.87%, Precision: 72.31%\n",
            "Epoch: 9, Batch: 107, Loss: 0.3503, Accuracy: 74.88%, Precision: 72.31%\n",
            "Epoch: 9, Batch: 108, Loss: 0.3304, Accuracy: 74.88%, Precision: 72.31%\n",
            "Epoch: 9, Batch: 109, Loss: 0.2237, Accuracy: 74.89%, Precision: 72.32%\n",
            "Epoch: 9, Batch: 110, Loss: 0.2056, Accuracy: 74.89%, Precision: 72.33%\n",
            "Epoch: 9, Batch: 111, Loss: 0.2017, Accuracy: 74.90%, Precision: 72.33%\n",
            "Epoch: 9, Batch: 112, Loss: 0.2588, Accuracy: 74.91%, Precision: 72.34%\n",
            "Epoch: 9, Batch: 113, Loss: 0.2303, Accuracy: 74.91%, Precision: 72.35%\n",
            "Epoch: 9, Batch: 114, Loss: 0.3538, Accuracy: 74.92%, Precision: 72.35%\n",
            "Epoch: 9, Batch: 115, Loss: 0.1942, Accuracy: 74.92%, Precision: 72.36%\n",
            "Epoch: 9, Batch: 116, Loss: 0.2288, Accuracy: 74.93%, Precision: 72.37%\n",
            "Epoch: 9, Batch: 117, Loss: 0.3337, Accuracy: 74.94%, Precision: 72.37%\n",
            "Epoch: 9, Batch: 118, Loss: 0.2236, Accuracy: 74.94%, Precision: 72.37%\n",
            "Epoch: 9, Batch: 119, Loss: 0.2747, Accuracy: 74.95%, Precision: 72.38%\n",
            "Epoch: 9, Batch: 120, Loss: 0.3175, Accuracy: 74.95%, Precision: 72.38%\n",
            "Epoch: 9, Batch: 121, Loss: 0.1944, Accuracy: 74.96%, Precision: 72.39%\n",
            "Epoch: 9, Batch: 122, Loss: 0.2678, Accuracy: 74.97%, Precision: 72.40%\n",
            "Epoch: 9, Batch: 123, Loss: 0.3653, Accuracy: 74.97%, Precision: 72.41%\n",
            "Epoch: 9, Batch: 124, Loss: 0.1411, Accuracy: 74.98%, Precision: 72.41%\n",
            "Epoch: 9, Batch: 125, Loss: 0.2374, Accuracy: 74.98%, Precision: 72.42%\n",
            "Epoch: 9, Batch: 126, Loss: 0.2289, Accuracy: 74.99%, Precision: 72.42%\n",
            "Epoch: 9, Batch: 127, Loss: 0.2559, Accuracy: 75.00%, Precision: 72.43%\n",
            "Epoch: 9, Batch: 128, Loss: 0.2646, Accuracy: 75.00%, Precision: 72.44%\n",
            "Epoch: 9, Batch: 129, Loss: 0.3101, Accuracy: 75.01%, Precision: 72.44%\n",
            "Epoch: 9, Batch: 130, Loss: 0.2674, Accuracy: 75.01%, Precision: 72.44%\n",
            "Epoch: 9, Batch: 131, Loss: 0.1573, Accuracy: 75.02%, Precision: 72.45%\n",
            "Epoch: 9, Batch: 132, Loss: 0.2346, Accuracy: 75.03%, Precision: 72.46%\n",
            "Epoch: 9, Batch: 133, Loss: 0.2864, Accuracy: 75.03%, Precision: 72.47%\n",
            "Epoch: 9, Batch: 134, Loss: 0.2422, Accuracy: 75.04%, Precision: 72.48%\n",
            "Epoch: 9, Batch: 135, Loss: 0.2917, Accuracy: 75.04%, Precision: 72.48%\n",
            "Epoch: 9, Batch: 136, Loss: 0.2796, Accuracy: 75.05%, Precision: 72.49%\n",
            "Epoch: 9, Batch: 137, Loss: 0.2446, Accuracy: 75.05%, Precision: 72.49%\n",
            "Epoch: 9, Batch: 138, Loss: 0.3215, Accuracy: 75.06%, Precision: 72.49%\n",
            "Epoch: 9, Batch: 139, Loss: 0.3095, Accuracy: 75.06%, Precision: 72.49%\n",
            "Epoch: 9, Batch: 140, Loss: 0.3067, Accuracy: 75.06%, Precision: 72.50%\n",
            "Epoch: 9, Batch: 141, Loss: 0.2454, Accuracy: 75.07%, Precision: 72.50%\n",
            "Epoch: 9, Batch: 142, Loss: 0.2100, Accuracy: 75.08%, Precision: 72.50%\n",
            "Epoch: 9, Batch: 143, Loss: 0.1755, Accuracy: 75.09%, Precision: 72.51%\n",
            "Epoch: 9, Batch: 144, Loss: 0.1961, Accuracy: 75.09%, Precision: 72.52%\n",
            "Epoch: 9, Batch: 145, Loss: 0.2639, Accuracy: 75.10%, Precision: 72.53%\n",
            "Epoch: 9, Batch: 146, Loss: 0.2172, Accuracy: 75.11%, Precision: 72.54%\n",
            "Epoch: 9, Batch: 147, Loss: 0.3115, Accuracy: 75.11%, Precision: 72.54%\n",
            "Epoch: 9, Batch: 148, Loss: 0.1823, Accuracy: 75.12%, Precision: 72.55%\n",
            "Epoch: 9, Batch: 149, Loss: 0.1866, Accuracy: 75.13%, Precision: 72.55%\n",
            "Epoch: 9, Batch: 150, Loss: 0.2636, Accuracy: 75.13%, Precision: 72.56%\n",
            "Epoch: 9, Batch: 151, Loss: 0.1784, Accuracy: 75.14%, Precision: 72.56%\n",
            "Epoch: 9, Batch: 152, Loss: 0.2501, Accuracy: 75.14%, Precision: 72.57%\n",
            "Epoch: 9, Batch: 153, Loss: 0.1744, Accuracy: 75.15%, Precision: 72.58%\n",
            "Epoch: 9, Batch: 154, Loss: 0.2300, Accuracy: 75.16%, Precision: 72.58%\n",
            "Epoch: 9, Batch: 155, Loss: 0.2001, Accuracy: 75.16%, Precision: 72.59%\n",
            "Epoch: 9, Batch: 156, Loss: 0.2207, Accuracy: 75.17%, Precision: 72.60%\n",
            "Epoch: 9, Batch: 157, Loss: 0.3652, Accuracy: 75.17%, Precision: 72.60%\n",
            "Epoch: 9, Batch: 158, Loss: 0.2444, Accuracy: 75.18%, Precision: 72.61%\n",
            "Epoch: 9, Batch: 159, Loss: 0.2814, Accuracy: 75.18%, Precision: 72.62%\n",
            "Epoch: 9, Batch: 160, Loss: 0.2650, Accuracy: 75.19%, Precision: 72.62%\n",
            "Epoch: 9, Batch: 161, Loss: 0.2615, Accuracy: 75.19%, Precision: 72.63%\n",
            "Epoch: 9, Batch: 162, Loss: 0.2614, Accuracy: 75.20%, Precision: 72.64%\n",
            "Epoch: 9, Batch: 163, Loss: 0.1418, Accuracy: 75.21%, Precision: 72.65%\n",
            "Epoch: 9, Batch: 164, Loss: 0.4736, Accuracy: 75.21%, Precision: 72.64%\n",
            "Epoch: 9, Batch: 165, Loss: 0.2656, Accuracy: 75.21%, Precision: 72.65%\n",
            "Epoch: 9, Batch: 166, Loss: 0.2722, Accuracy: 75.22%, Precision: 72.65%\n",
            "Epoch: 9, Batch: 167, Loss: 0.1433, Accuracy: 75.23%, Precision: 72.66%\n",
            "Epoch: 9, Batch: 168, Loss: 0.2707, Accuracy: 75.23%, Precision: 72.67%\n",
            "Epoch: 9, Batch: 169, Loss: 0.2572, Accuracy: 75.24%, Precision: 72.68%\n",
            "Epoch: 9, Batch: 170, Loss: 0.1753, Accuracy: 75.24%, Precision: 72.69%\n",
            "Epoch: 9, Batch: 171, Loss: 0.3224, Accuracy: 75.25%, Precision: 72.69%\n",
            "Epoch: 9, Batch: 172, Loss: 0.2327, Accuracy: 75.25%, Precision: 72.70%\n",
            "Epoch: 9, Batch: 173, Loss: 0.2697, Accuracy: 75.26%, Precision: 72.71%\n",
            "Epoch: 9, Batch: 174, Loss: 0.2926, Accuracy: 75.26%, Precision: 72.71%\n",
            "Epoch: 9, Batch: 175, Loss: 0.3250, Accuracy: 75.27%, Precision: 72.71%\n",
            "Epoch: 9, Batch: 176, Loss: 0.2778, Accuracy: 75.27%, Precision: 72.72%\n",
            "Epoch: 9, Batch: 177, Loss: 0.3503, Accuracy: 75.28%, Precision: 72.72%\n",
            "Epoch: 9, Batch: 178, Loss: 0.3795, Accuracy: 75.28%, Precision: 72.73%\n",
            "Epoch: 9, Batch: 179, Loss: 0.1943, Accuracy: 75.29%, Precision: 72.73%\n",
            "Epoch: 9, Batch: 180, Loss: 0.3480, Accuracy: 75.29%, Precision: 72.74%\n",
            "Epoch: 9, Batch: 181, Loss: 0.2768, Accuracy: 75.30%, Precision: 72.75%\n",
            "Epoch: 9, Batch: 182, Loss: 0.2842, Accuracy: 75.30%, Precision: 72.76%\n",
            "Epoch: 9, Batch: 183, Loss: 0.1970, Accuracy: 75.31%, Precision: 72.76%\n",
            "Epoch: 9, Batch: 184, Loss: 0.2554, Accuracy: 75.32%, Precision: 72.77%\n",
            "Epoch: 9, Batch: 185, Loss: 0.3200, Accuracy: 75.32%, Precision: 72.77%\n",
            "Epoch: 9, Batch: 186, Loss: 0.2380, Accuracy: 75.32%, Precision: 72.77%\n",
            "Epoch: 9, Batch: 187, Loss: 0.4000, Accuracy: 75.32%, Precision: 72.77%\n",
            "Epoch: 9, Batch: 188, Loss: 0.2260, Accuracy: 75.33%, Precision: 72.78%\n",
            "Epoch: 9, Batch: 189, Loss: 0.3107, Accuracy: 75.34%, Precision: 72.79%\n",
            "Epoch: 9, Batch: 190, Loss: 0.2201, Accuracy: 75.34%, Precision: 72.79%\n",
            "Epoch: 9, Batch: 191, Loss: 0.2495, Accuracy: 75.35%, Precision: 72.79%\n",
            "Epoch: 9, Batch: 192, Loss: 0.3776, Accuracy: 75.35%, Precision: 72.80%\n",
            "Epoch: 9, Batch: 193, Loss: 0.2120, Accuracy: 75.36%, Precision: 72.81%\n",
            "Epoch: 9, Batch: 194, Loss: 0.4006, Accuracy: 75.36%, Precision: 72.81%\n",
            "Epoch: 9, Batch: 195, Loss: 0.2438, Accuracy: 75.36%, Precision: 72.82%\n",
            "Epoch: 9, Batch: 196, Loss: 0.2852, Accuracy: 75.37%, Precision: 72.82%\n",
            "Epoch: 9, Batch: 197, Loss: 0.3012, Accuracy: 75.37%, Precision: 72.82%\n",
            "Epoch: 9, Batch: 198, Loss: 0.3186, Accuracy: 75.38%, Precision: 72.82%\n",
            "Epoch: 9, Batch: 199, Loss: 0.2641, Accuracy: 75.38%, Precision: 72.83%\n",
            "Epoch: 9, Batch: 200, Loss: 0.1729, Accuracy: 75.39%, Precision: 72.84%\n",
            "Epoch: 9, Batch: 201, Loss: 0.1920, Accuracy: 75.39%, Precision: 72.85%\n",
            "Epoch: 9, Batch: 202, Loss: 0.2257, Accuracy: 75.40%, Precision: 72.86%\n",
            "Epoch: 9, Batch: 203, Loss: 0.2757, Accuracy: 75.41%, Precision: 72.86%\n",
            "Epoch: 9, Batch: 204, Loss: 0.2271, Accuracy: 75.41%, Precision: 72.87%\n",
            "Epoch: 9, Batch: 205, Loss: 0.2025, Accuracy: 75.42%, Precision: 72.88%\n",
            "Epoch: 9, Batch: 206, Loss: 0.2439, Accuracy: 75.43%, Precision: 72.88%\n",
            "Epoch: 9, Batch: 207, Loss: 0.3218, Accuracy: 75.43%, Precision: 72.89%\n",
            "Epoch: 9, Batch: 208, Loss: 0.3011, Accuracy: 75.44%, Precision: 72.89%\n",
            "Epoch: 9, Batch: 209, Loss: 0.3067, Accuracy: 75.44%, Precision: 72.90%\n",
            "Epoch: 9, Batch: 210, Loss: 0.1671, Accuracy: 75.45%, Precision: 72.91%\n",
            "Epoch: 9, Batch: 211, Loss: 0.2582, Accuracy: 75.45%, Precision: 72.91%\n",
            "Epoch: 9, Batch: 212, Loss: 0.2944, Accuracy: 75.46%, Precision: 72.92%\n",
            "Epoch: 9, Batch: 213, Loss: 0.2700, Accuracy: 75.47%, Precision: 72.93%\n",
            "Epoch: 9, Batch: 214, Loss: 0.2567, Accuracy: 75.47%, Precision: 72.93%\n",
            "Epoch: 9, Batch: 215, Loss: 0.3036, Accuracy: 75.48%, Precision: 72.94%\n",
            "Epoch: 9, Batch: 216, Loss: 0.2172, Accuracy: 75.48%, Precision: 72.95%\n",
            "Epoch: 9, Batch: 217, Loss: 0.3018, Accuracy: 75.49%, Precision: 72.96%\n",
            "Epoch: 9, Batch: 218, Loss: 0.3609, Accuracy: 75.49%, Precision: 72.95%\n",
            "Epoch: 9, Batch: 219, Loss: 0.3109, Accuracy: 75.49%, Precision: 72.96%\n",
            "Epoch: 9, Batch: 220, Loss: 0.1933, Accuracy: 75.50%, Precision: 72.97%\n",
            "Epoch: 9, Batch: 221, Loss: 0.2779, Accuracy: 75.51%, Precision: 72.97%\n",
            "Epoch: 9, Batch: 222, Loss: 0.1737, Accuracy: 75.51%, Precision: 72.98%\n",
            "Epoch: 9, Batch: 223, Loss: 0.2231, Accuracy: 75.52%, Precision: 72.98%\n",
            "Epoch: 9, Batch: 224, Loss: 0.3213, Accuracy: 75.52%, Precision: 72.98%\n",
            "Epoch: 9, Batch: 225, Loss: 0.2378, Accuracy: 75.53%, Precision: 72.98%\n",
            "Epoch: 9, Batch: 226, Loss: 0.2522, Accuracy: 75.53%, Precision: 72.99%\n",
            "Epoch: 9, Batch: 227, Loss: 0.1532, Accuracy: 75.54%, Precision: 73.00%\n",
            "Epoch: 9, Batch: 228, Loss: 0.2521, Accuracy: 75.54%, Precision: 73.00%\n",
            "Epoch: 9, Batch: 229, Loss: 0.2089, Accuracy: 75.55%, Precision: 73.01%\n",
            "Epoch: 9, Batch: 230, Loss: 0.1908, Accuracy: 75.56%, Precision: 73.02%\n",
            "Epoch: 9, Batch: 231, Loss: 0.3275, Accuracy: 75.56%, Precision: 73.02%\n",
            "Epoch: 9, Batch: 232, Loss: 0.2223, Accuracy: 75.57%, Precision: 73.03%\n",
            "Epoch: 9, Batch: 233, Loss: 0.2867, Accuracy: 75.57%, Precision: 73.03%\n",
            "Epoch: 9, Batch: 234, Loss: 0.3820, Accuracy: 75.58%, Precision: 73.04%\n",
            "Epoch: 9, Batch: 235, Loss: 0.1736, Accuracy: 75.58%, Precision: 73.05%\n",
            "Epoch: 9, Batch: 236, Loss: 0.2968, Accuracy: 75.58%, Precision: 73.05%\n",
            "Epoch: 9, Batch: 237, Loss: 0.2538, Accuracy: 75.59%, Precision: 73.06%\n",
            "Epoch: 9, Batch: 238, Loss: 0.2343, Accuracy: 75.60%, Precision: 73.06%\n",
            "Epoch: 9, Batch: 239, Loss: 0.2187, Accuracy: 75.60%, Precision: 73.07%\n",
            "Epoch: 9, Batch: 240, Loss: 0.3006, Accuracy: 75.61%, Precision: 73.07%\n",
            "Epoch: 9, Batch: 241, Loss: 0.2216, Accuracy: 75.61%, Precision: 73.08%\n",
            "Epoch: 9, Batch: 242, Loss: 0.1964, Accuracy: 75.62%, Precision: 73.09%\n",
            "Epoch: 9, Batch: 243, Loss: 0.3265, Accuracy: 75.62%, Precision: 73.09%\n",
            "7530598\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [2, 64, 216, 160]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pp\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_n_params(model))\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m216\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# for name, param in model.named_parameters():\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#     print(name, param.data)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/prog/lib/python3.10/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
            "File \u001b[0;32m~/miniconda3/envs/prog/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[9], line 28\u001b[0m, in \u001b[0;36mMyConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Convolutional layers with ReLU activation\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n",
            "File \u001b[0;32m~/miniconda3/envs/prog/lib/python3.10/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
            "File \u001b[0;32m~/miniconda3/envs/prog/lib/python3.10/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/prog/lib/python3.10/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [2, 64, 216, 160]"
          ]
        }
      ],
      "source": [
        "print(model)\n",
        "device = get_device()\n",
        "model.to(device)\n",
        "model.train()\n",
        "loss_hist = []\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "true_positives = 0\n",
        "false_positives = 0\n",
        "true_negatives = 0\n",
        "false_negatives = 0\n",
        "epochs = 15\n",
        "\n",
        "for e in range(epochs):\n",
        "    i = 1\n",
        "    loss_per_epoch = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        # Perform forward pass, compute loss, and update the model\n",
        "        inputs = inputs.unsqueeze(1)\n",
        "        labels = labels.long()\n",
        "        bs, c, time, feats = inputs.shape\n",
        "        inputs = inputs.reshape(bs, time, feats)\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_per_epoch += loss.item()\n",
        "        # Calculate accuracy\n",
        "        # print('a',outputs.data)\n",
        "        # print('b',torch.max(outputs.data, 1))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate precision\n",
        "        true_positives += ((predicted == 1) & (labels == 1)).sum().item()\n",
        "        false_positives += ((predicted == 1) & (labels == 0)).sum().item()\n",
        "        true_negatives += ((predicted == 0) & (labels == 0)).sum().item()\n",
        "        false_negatives += ((predicted == 0) & (labels == 1)).sum().item()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print metrics\n",
        "        print(\"Epoch: {}, Batch: {}, Loss: {:.4f}, Accuracy: {:.2f}%, Precision: {:.2f}%\".format(\n",
        "            e, i, loss.item(), (correct_predictions / total_predictions) * 100,\n",
        "            (true_positives / (true_positives + false_positives + 1e-12)) * 100  # Add small epsilon to avoid division by zero\n",
        "        ))\n",
        "\n",
        "        i += 1\n",
        "    loss_hist.append(loss_per_epoch/i)\n",
        "torch.save(model.state_dict(), \"./model/model_conv1d_norm_3_dropout.pt\")\n",
        "\n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp\n",
        "print(get_n_params(model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "y5NIwYzNFc3n"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABerklEQVR4nO3deVwV9f7H8dcBZFVwQTbDPfeF3Ag1tSRxydSs1DSVFvupmUZWenO3JC2t65JbmWtmmWu5k5rmmkta11xzF1wSUFRQOL8/5nq6JCogMMB5Px+PeVzOnJk5nxGv5913vovFarVaEREREbEjDmYXICIiIpLTFIBERETE7igAiYiIiN1RABIRERG7owAkIiIidkcBSEREROyOApCIiIjYHQUgERERsTsKQCIiImJ3FIBEhO7du1O6dOlMnTts2DAsFkvWFmTnNmzYgMViYcOGDWaXIpJvKQCJ5GIWiyVdm71+UXbv3p2CBQuaXcZ9NWnShGrVqqX53vHjx7FYLHz88ccP/DmjRo1iyZIlD3wdEXvgZHYBInJ3c+bMSfV69uzZrF279o79lStXfqDPmT59OikpKZk6d9CgQQwYMOCBPl9Sa9SoEdevX8fZ2TlD540aNYpnn32Wtm3bZk9hIvmIApBILtalS5dUr7dt28batWvv2P9P165dw93dPd2fU6BAgUzVB+Dk5ISTk/4pyUoODg64urqaXQYACQkJeHh4mF2GSJbTIzCRPO7245Vdu3bRqFEj3N3d+de//gXA0qVLadWqFQEBAbi4uFCuXDlGjhxJcnJyqmv8sw/Q/z6WmTZtGuXKlcPFxYW6deuyc+fOVOem1QfIYrHw+uuvs2TJEqpVq4aLiwtVq1Zl1apVd9S/YcMG6tSpg6urK+XKlWPq1KlZ3q/o22+/pXbt2ri5ueHt7U2XLl04c+ZMqmOio6MJDw/noYcewsXFBX9/f9q0acPx48dtx/zyyy+EhYXh7e2Nm5sbZcqU4aWXXsqyOm9Lqw/Q4cOHad++PX5+fri6uvLQQw/RsWNH4uLiAOPPPCEhgVmzZtkejXbv3t12/p49e2jRogWenp4ULFiQpk2bsm3btlSfO3PmTCwWCxs3bqRXr174+Pjw0EMPsX79eiwWC4sXL76j1q+++gqLxcLWrVuz/M9BJDvpP9tE8oFLly7RokULOnbsSJcuXfD19QWML7SCBQsSERFBwYIF+fHHHxkyZAjx8fF89NFH973uV199xZUrV3jttdewWCyMGTOGZ555hmPHjt231Wjz5s0sWrSIXr16UahQIcaPH0/79u05efIkxYoVA4wv5ebNm+Pv78/w4cNJTk5mxIgRFC9e/MH/UP5r5syZhIeHU7duXSIjI4mJieHf//43P//8M3v27KFw4cIAtG/fnt9//50+ffpQunRpzp8/z9q1azl58qTtdbNmzShevDgDBgygcOHCHD9+nEWLFqWrjuTkZC5evHjH/suXL9/33KSkJMLCwkhMTKRPnz74+flx5swZvv/+e2JjY/Hy8mLOnDm88sor1KtXjx49egBQrlw5AH7//Xcee+wxPD09eeeddyhQoABTp06lSZMmbNy4keDg4FSf16tXL4oXL86QIUNISEigSZMmBAYGMm/ePNq1a5fq2Hnz5lGuXDlCQkLS9ecgkmtYRSTP6N27t/Wf/7dt3LixFbBOmTLljuOvXbt2x77XXnvN6u7ubr1x44ZtX7du3aylSpWyvf7zzz+tgLVYsWLWv/76y7Z/6dKlVsC6fPly276hQ4feURNgdXZ2th45csS279dff7UC1gkTJtj2tW7d2uru7m49c+aMbd/hw4etTk5Od1wzLd26dbN6eHjc9f2kpCSrj4+PtVq1atbr16/b9n///fdWwDpkyBCr1Wq1Xr582QpYP/roo7tea/HixVbAunPnzvvW9U+3f0f32v73s9evX28FrOvXr7darVbrnj17rID122+/vefneHh4WLt163bH/rZt21qdnZ2tR48ete07e/astVChQtZGjRrZ9n355ZdWwNqwYUPrrVu3Ul1j4MCBVhcXF2tsbKxt3/nz561OTk7WoUOHZuBPQyR30CMwkXzAxcWF8PDwO/a7ubnZfr5y5QoXL17kscce49q1a/zxxx/3vW6HDh0oUqSI7fVjjz0GwLFjx+57bmhoqK0FAqBGjRp4enrazk1OTmbdunW0bduWgIAA23Hly5enRYsW971+evzyyy+cP3+eXr16pepT06pVKypVqsQPP/wAGH9Ozs7ObNiw4a4tMrdbir7//ntu3ryZ4VpKly7N2rVr79jmzp1733O9vLwAWL16NdeuXcvQ5yYnJ7NmzRratm1L2bJlbfv9/f154YUX2Lx5M/Hx8anOefXVV3F0dEy1r2vXriQmJrJw4ULbvgULFnDr1q379kkTyY0UgETygRIlSqQ5Yuj333+nXbt2eHl54enpSfHixW1fVrf7jtxLyZIlU72+HYbS89jmn+fePv/2uefPn+f69euUL1/+juPS2pcZJ06cAKBixYp3vFepUiXb+y4uLowePZqVK1fi6+tLo0aNGDNmDNHR0bbjGzduTPv27Rk+fDje3t60adOGL7/8ksTExHTV4uHhQWho6B1bgwYN7ntumTJliIiI4PPPP8fb25uwsDAmTZqUrt/hhQsXuHbtWpp/BpUrVyYlJYVTp07d8Xn/VKlSJerWrcu8efNs++bNm8ejjz6aZb8vkZykACSSD/xvS89tsbGxNG7cmF9//ZURI0awfPly1q5dy+jRowHSNez9n60At1mt1mw91wz9+vXj0KFDREZG4urqyuDBg6lcuTJ79uwBjE7GCxcuZOvWrbz++uucOXOGl156idq1a3P16tVsr2/s2LHs27ePf/3rX1y/fp033niDqlWrcvr06Sz/rLT+PoHRCrRx40ZOnz7N0aNH2bZtm1p/JM9SABLJpzZs2MClS5eYOXMmffv25amnniI0NDTVIy0z+fj44OrqypEjR+54L619mVGqVCkADh48eMd7Bw8etL1/W7ly5XjrrbdYs2YNv/32G0lJSYwdOzbVMY8++igffPABv/zyC/PmzeP333/n66+/zpJ676d69eoMGjSIn376iU2bNnHmzBmmTJliez+tkXPFixfH3d09zT+DP/74AwcHBwIDA9P1+R07dsTR0ZH58+czb948ChQoQIcOHTJ/QyImUgASyadut8D8b4tLUlISn332mVklpeLo6EhoaChLlizh7Nmztv1Hjhxh5cqVWfIZderUwcfHhylTpqR6VLVy5UoOHDhAq1atAGPepBs3bqQ6t1y5chQqVMh23uXLl+9ovQoKCgJI92OwzIqPj+fWrVup9lWvXh0HB4dUn+3h4UFsbGyq4xwdHWnWrBlLly5NNaQ/JiaGr776ioYNG+Lp6ZmuOry9vWnRogVz585l3rx5NG/eHG9v70zfl4iZNAxeJJ+qX78+RYoUoVu3brzxxhtYLBbmzJmTqx5BDRs2jDVr1tCgQQN69uxJcnIyEydOpFq1auzduzdd17h58ybvv//+HfuLFi1Kr169GD16NOHh4TRu3JhOnTrZhsGXLl2aN998E4BDhw7RtGlTnn/+eapUqYKTkxOLFy8mJiaGjh07AjBr1iw+++wz2rVrR7ly5bhy5QrTp0/H09OTli1bZtmfSVp+/PFHXn/9dZ577jkqVKjArVu3mDNnDo6OjrRv3952XO3atVm3bh3jxo0jICCAMmXKEBwczPvvv8/atWtp2LAhvXr1wsnJialTp5KYmMiYMWMyVEvXrl159tlnARg5cmSW3qdITlIAEsmnihUrxvfff89bb73FoEGDKFKkCF26dKFp06aEhYWZXR5gfGGvXLmS/v37M3jwYAIDAxkxYgQHDhxI1yg1MFq1Bg8efMf+cuXK0atXL7p37467uzsffvgh7777Lh4eHrRr147Ro0fbRnYFBgbSqVMnoqKimDNnDk5OTlSqVIlvvvnGFjAaN27Mjh07+Prrr4mJicHLy4t69eoxb968NDsNZ6WaNWsSFhbG8uXLOXPmDO7u7tSsWZOVK1fy6KOP2o4bN24cPXr0YNCgQVy/fp1u3boRHBxM1apV2bRpEwMHDiQyMpKUlBSCg4OZO3fuHXMA3U/r1q0pUqQIKSkpPP3001l9qyI5xmLNTf85KCICtG3blt9//53Dhw+bXYr8w61btwgICKB169Z88cUXZpcjkmnqAyQiprp+/Xqq14cPH2bFihU0adLEnILknpYsWcKFCxfo2rWr2aWIPBC1AImIqfz9/enevTtly5blxIkTTJ48mcTERPbs2cPDDz9sdnnyX9u3b2ffvn2MHDkSb29vdu/ebXZJIg9EfYBExFTNmzdn/vz5REdH4+LiQkhICKNGjVL4yWUmT57M3LlzCQoKYubMmWaXI/LA1AIkIiIidkd9gERERMTuKACJiIiI3VEfoDSkpKRw9uxZChUqlObU8iIiIpL7WK1Wrly5QkBAAA4O927jUQBKw9mzZ9O9No6IiIjkLqdOneKhhx665zEKQGkoVKgQYPwBpneNHBERETFXfHw8gYGBtu/xe1EASsPtx16enp4KQCIiInlMerqvqBO0iIiI2B0FIBEREbE7pgegSZMmUbp0aVxdXQkODmbHjh33PD42NpbevXvj7++Pi4sLFSpUYMWKFbb3hw0bhsViSbVVqlQpu29DRERE8hBT+wAtWLCAiIgIpkyZQnBwMJ9++ilhYWEcPHgQHx+fO45PSkriySefxMfHh4ULF1KiRAlOnDhB4cKFUx1XtWpV1q1bZ3vt5KSuTiIieVFycjI3b940uwzJJQoUKICjo2OWXMvUZDBu3DheffVVwsPDAZgyZQo//PADM2bMYMCAAXccP2PGDP766y+2bNlCgQIFAChduvQdxzk5OeHn55ettYuISPaxWq1ER0cTGxtrdimSyxQuXBg/P78HnqfPtACUlJTErl27GDhwoG2fg4MDoaGhbN26Nc1zli1bRkhICL1792bp0qUUL16cF154gXfffTdVIjx8+DABAQG4uroSEhJCZGQkJUuWzPZ7EhGRrHE7/Pj4+ODu7q5JaQWr1cq1a9c4f/48AP7+/g90PdMC0MWLF0lOTsbX1zfVfl9fX/744480zzl27Bg//vgjnTt3ZsWKFRw5coRevXpx8+ZNhg4dCkBwcDAzZ86kYsWKnDt3juHDh/PYY4/x22+/3XVegMTERBITE22v4+Pjs+guRUQko5KTk23hp1ixYmaXI7mIm5sbAOfPn8fHx+eBHoflqc4xKSkp+Pj4MG3aNBwdHalduzZnzpzho48+sgWgFi1a2I6vUaMGwcHBlCpVim+++YaXX345zetGRkYyfPjwHLkHERG5t9t9ftzd3U2uRHKj238vbt68+UAByLRRYN7e3jg6OhITE5Nqf0xMzF377/j7+1OhQoVUN1y5cmWio6NJSkpK85zChQtToUIFjhw5ctdaBg4cSFxcnG07depUJu5IRESykh57SVqy6u+FaQHI2dmZ2rVrExUVZduXkpJCVFQUISEhaZ7ToEEDjhw5QkpKim3foUOH8Pf3x9nZOc1zrl69ytGjR+/5rNDFxcU267NmfxYREcn/TJ0HKCIigunTpzNr1iwOHDhAz549SUhIsI0K69q1a6pO0j179uSvv/6ib9++HDp0iB9++IFRo0bRu3dv2zH9+/dn48aNHD9+nC1bttCuXTscHR3p1KlTjt+fiIjIgypdujSffvppuo/fsGEDFosl20fQzZw5845paPISU/sAdejQgQsXLjBkyBCio6MJCgpi1apVto7RJ0+eTLWcfWBgIKtXr+bNN9+kRo0alChRgr59+/Luu+/ajjl9+jSdOnXi0qVLFC9enIYNG7Jt2zaKFy+e4/cnIiL2436PZoYOHcqwYcMyfN2dO3fi4eGR7uPr16/PuXPn8PLyyvBn2ROL1Wq1ml1EbhMfH4+XlxdxcXFZ+jjMaoXvv4enngI92hYRSduNGzf4888/KVOmDK6urmaXk27R0dG2nxcsWMCQIUM4ePCgbV/BggUpWLAgYAzpTk5OztMT9c6cOZN+/frl+FxN9/r7kZHvb9OXwrAnkybB00/Ds8+CRtqLiOQvfn5+ts3LywuLxWJ7/ccff1CoUCFWrlxJ7dq1cXFxYfPmzRw9epQ2bdrg6+tLwYIFqVu3bqqVDODOR2AWi4XPP/+cdu3a4e7uzsMPP8yyZcts7//zEdjtR1WrV6+mcuXKFCxYkObNm3Pu3DnbObdu3eKNN96gcOHCFCtWjHfffZdu3brRtm3bDP0ZTJ48mXLlyuHs7EzFihWZM2eO7T2r1cqwYcMoWbIkLi4uBAQE8MYbb9je/+yzz3j44YdxdXXF19eXZ599NkOfnVEKQDnI2RkKFIBFi6BePfjPf8yuSEQkb7BaISEh57esfkYyYMAAPvzwQw4cOECNGjW4evUqLVu2JCoqij179tC8eXNat27NyZMn73md4cOH8/zzz7Nv3z5atmxJ586d+euvv+56/LVr1/j444+ZM2cOP/30EydPnqR///6290ePHs28efP48ssv+fnnn4mPj2fJkiUZurfFixfTt29f3nrrLX777Tdee+01wsPDWb9+PQDfffcdn3zyCVOnTuXw4cMsWbKE6tWrA/DLL7/wxhtvMGLECA4ePMiqVato1KhRhj4/w6xyh7i4OCtgjYuLy/Jrb9tmtZYoYbWC1erhYbUuWJDlHyEikqddv37d+p///Md6/fp1276rV41/N3N6u3o1c/fw5ZdfWr28vGyv169fbwWsS5Ysue+5VatWtU6YMMH2ulSpUtZPPvnE9hqwDho06H/+bK5aAevKlStTfdbly5dttQDWI0eO2M6ZNGmS1dfX1/ba19fX+tFHH9le37p1y1qyZElrmzZt0n2P9evXt7766qupjnnuueesLVu2tFqtVuvYsWOtFSpUsCYlJd1xre+++87q6elpjY+Pv+vn3ZbW34/bMvL9rRagHBYcDLt3w+OPG/910aEDvPUW3LpldmUiIpLd6tSpk+r11atX6d+/P5UrV6Zw4cIULFiQAwcO3LcFqEaNGrafPTw88PT0tC0RkRZ3d3fKlStne+3v7287Pi4ujpiYGOrVq2d7//Zkwxlx4MABGjRokGpfgwYNOHDgAADPPfcc169fp2zZsrz66qssXryYW//98nvyyScpVaoUZcuW5cUXX2TevHlcu3YtQ5+fUQpAJvDxgTVr4J13jNfjxkFoKPxP/zkREfkf7u5w9WrOb1k9GfU/R3P179+fxYsXM2rUKDZt2sTevXupXr36XSf3ve32guC3WSyWVHPkped4aw6PgQoMDOTgwYN89tlnuLm50atXLxo1asTNmzcpVKgQu3fvZv78+fj7+zNkyBBq1qyZrR2sFYBM4uQEo0fDwoVQqBBs3Ai1a8OWLWZXJiKS+1gs4OGR81t2j9j9+eef6d69O+3ataN69er4+flx/Pjx7P3Qf/Dy8sLX15edO3fa9iUnJ7N79+4MXady5cr8/PPPqfb9/PPPVKlSxfbazc2N1q1bM378eDZs2MDWrVvZv38/AE5OToSGhjJmzBj27dvH8ePH+fHHHx/gzu4t746/yyfat4eqVeGZZ+DAAWjcGD75BHr31lB5EZH87uGHH2bRokW0bt0ai8XC4MGD79mSk1369OlDZGQk5cuXp1KlSkyYMIHLly9naNmJt99+m+eff55HHnmE0NBQli9fzqJFi2yj2mbOnElycjLBwcG4u7szd+5c3NzcKFWqFN9//z3Hjh2jUaNGFClShBUrVpCSkkLFihWz65bVApQbVKoEO3bA888bfYH69IEXXzT6CImISP41btw4ihQpQv369WndujVhYWHUqlUrx+t499136dSpE127diUkJISCBQsSFhaWoXmY2rZty7///W8+/vhjqlatytSpU/nyyy9p0qQJYKzNOX36dBo0aECNGjVYt24dy5cvp1ixYhQuXJhFixbxxBNPULlyZaZMmcL8+fOpWrVqNt2xJkJMU3ZNhHg/Vit8+im8/TYkJ0P16saQ+fLlc6wEERHT5dWJEPOTlJQUKleuzPPPP8/IkSPNLicVTYSYD1ks8Oab8OOP4OsL+/dDnTqwfLnZlYmISH524sQJpk+fzqFDh9i/fz89e/bkzz//5IUXXjC7tGyjAJQLNWpkDJWvXx/i4ozZowcNMlqFREREspqDgwMzZ86kbt26NGjQgP3797Nu3ToqV65sdmnZRp2gc6mAAFi/Hvr3hwkT4IMPYOdO+OorKFbM7OpERCQ/CQwMvGMEV36nFqBczNkZxo+HuXPBzc2YO6h2bfjlF7MrExERydsUgPKAzp1h2zajM/SJE9CwIXzxhdlViYhkL43RkbRk1d8LBaA8okYN4xFY69aQmAivvAKvvgo3bphdmYhI1ro9a3F2L4UgedPtvxf/nN06o9QHKA8pXBiWLIEPPzQ6RX/+OezZA999B6VKmV2diEjWcHR0pHDhwra1qtzd3TM0IZ/kT1arlWvXrnH+/HkKFy6Mo6PjA11P8wClwax5gDJizRp44QW4dAmKFoX586FZM7OrEhHJGlarlejo6GxdC0rypsKFC+Pn55dmKM7I97cCUBryQgACoz/Qs88anaItFhg5EgYOBAc92BSRfCI5OZmbN2+aXYbkEgUKFLhny48C0APKKwEIjD5AffoYj8PA6CM0e7bxuExERMSeaCZoO+LqCtOnG5uLizFrdN26xizSIiIikjYFoHzilVdg82YoWRKOHIHgYJg3z+yqREREcicFoHykTh3YtcvoDH39OnTpYjweS0oyuzIREZHcRQEon/H2hhUrjGHyABMnwuOPw9mz5tYlIiKSmygA5UOOjsaIsGXLwMsLtmyBWrVg40azKxMREckdFIDysdatjSHy1atDTAw0bQrjxoHG/YmIiL1TAMrnypeHrVuN9cSSk+Gtt6BDB7hyxezKREREzKMAZAc8PGDOHJgwAZyc4NtvjVFif/xhdmUiIiLmUACyExYLvP660Q8oIAAOHDDmC/ruO7MrExERyXkKQHamfn1jqHyjRnD1qrGUxjvvwK1bZlcmIiKScxSA7JCfH6xbZ/QHAvjoI2PuoP8uvCwiIpLvKQDZqQIF4OOP4ZtvjD5C69cbQ+W3bTO7MhERkeynAGTnnnsOduyAihXhzBnj0djkyRoqLyIi+ZvpAWjSpEmULl0aV1dXgoOD2bFjxz2Pj42NpXfv3vj7++Pi4kKFChVYsWLFA13T3lWpYoSg9u3h5k3o1Qu6d4dr18yuTEREJHuYGoAWLFhAREQEQ4cOZffu3dSsWZOwsDDO36UzSlJSEk8++STHjx9n4cKFHDx4kOnTp1OiRIlMX1MMnp7G8PgxY8DBAWbPNjpMHz1qdmUiIiJZz2K1mvewIzg4mLp16zJx4kQAUlJSCAwMpE+fPgwYMOCO46dMmcJHH33EH3/8QYECBbLkmmmJj4/Hy8uLuLg4PD09M3l3edf69cZkiRcuQOHCMHcutGpldlUiIiL3lpHvb9NagJKSkti1axehoaF/F+PgQGhoKFu3bk3znGXLlhESEkLv3r3x9fWlWrVqjBo1iuTk5ExfEyAxMZH4+PhUmz17/HHYvduYLDE2Fp56CoYONWaSFhERyQ9MC0AXL14kOTkZX1/fVPt9fX2Jjo5O85xjx46xcOFCkpOTWbFiBYMHD2bs2LG8//77mb4mQGRkJF5eXrYtMDDwAe8u73voIWPSxF69jNcjRhhB6K+/zK1LREQkK5jeCTojUlJS8PHxYdq0adSuXZsOHTrw3nvvMWXKlAe67sCBA4mLi7Ntp06dyqKK8zYXF5g0CWbNAldXWLUKateGPXvMrkxEROTBmBaAvL29cXR0JCYmJtX+mJgY/Pz80jzH39+fChUq4OjoaNtXuXJloqOjSUpKytQ1AVxcXPD09Ey1yd+6djUWVC1bFo4fNzpHz5xpdlUiIiKZZ1oAcnZ2pnbt2kRFRdn2paSkEBUVRUhISJrnNGjQgCNHjpCSkmLbd+jQIfz9/XF2ds7UNSV9goLgl1+MztA3bkB4uPFIbP16zRkkIiJ5j6mPwCIiIpg+fTqzZs3iwIED9OzZk4SEBMLDwwHo2rUrAwcOtB3fs2dP/vrrL/r27cuhQ4f44YcfGDVqFL179073NSXzihSBZcuM/kAWC/zwAzzxBDzyiNEilJhodoUiIiLp42Tmh3fo0IELFy4wZMgQoqOjCQoKYtWqVbZOzCdPnsTB4e+MFhgYyOrVq3nzzTepUaMGJUqUoG/fvrz77rvpvqY8GAcHGDzYmEF6/Hgj+Pz6q9Ei9O67Rqfpnj3Bx8fsSkVERO7O1HmAcit7nwcoI/76C6ZPhwkTjKU0wOg83bkz9OsH1aubWp6IiNiRPDEPkOQPRYsaLT9//glffQV16xqPwmbMgBo1IDQUvv8e/qfbloiIiOkUgCRLFCgAnTrB9u3w88/GIzIHB4iKgtatoXJl+OwzSEgwu1IREREFIMliFosxTP6bb+DYMXjrLWOdsUOHoHdvY4LFd98FTbUkIiJmUgCSbFOqFHz8MZw+bXSYLlfOWFpjzBgoUwY6djRajERERHKaApBku0KFoE8fOHgQli6FJk2MdcUWLIBHH4WQEKPF6NYtsysVERF7oQAkOcbREZ5+2pg8cc8e6NYNnJ1h2zZj9fmyZeGjj+DyZbMrFRGR/E4BSEwRFGTMIXTiBAwZAsWLG/2C3nkHAgPh9deNfkMiIiLZQQFITOXnB8OHw8mT8MUXUK2aMVJs0iSoVMkYQfbjj1puQ0REspYCkOQKrq7w0kuwbx+sW2esOWa1GnMINW1qtBh9+aWxDpmIiMiDUgCSXMViMQLP998bnaZ79QJ3dyMYvfSSMbJs2DCIiTG7UhERycsUgCTXqlDBeBR26hSMHm3MIXT+vPHIrGRJY/2xX381u0oREcmLFIAk1yta1OgcfewYfP01BAdDUpLRiTooyFiRfvlyLbchIiLppwAkeUaBAsZw+W3bYMsWeP55Y2j9+vXG8PqKFWHiRLh61exKRUQkt1MAkjwpJMSYSPHYMXj7bfDygiNHjAkXH3rI2HfypNlViohIbqUAJHlayZLG0hqnT8OECVC+PMTFGUtwlC1rtBJt3Wp2lSIiktsoAEm+ULCgMXniwYOwbJnRLyg5Gb791lic9dFHjf5DN2+aXamIiOQGCkCSrzg4GJMnRkXB3r3Qvbux3Mb27dCpk9EqNHq0ltsQEbF3CkCSb9WsaUyeePIkDB1qLLdx+jQMGGD0E+rd22gxEhER+6MAJPmer68xeeLJkzBjBlSvDteuwWefGcttPPWUMfu0ltsQEbEfCkBiN1xd/548MSrKCD4AP/wATz4JNWoY65FpuQ0RkfxPAUjsjsXy9+SJBw8aj8Lc3eG33+CVV4yRZUOGQHS02ZWKiEh2UQASu1ahgjF54unTxnD6wEC4cAFGjjTWHeve3ehMLSIi+YsCkAhQpIgxeeKxY8YEiyEhxnIbs2bBI4/A44/D0qXG0HoREcn7FIBE/oeTkzF54pYtxpIbHTsay21s2ABt2xrLbYwfD1eumF2piIg8CAUgkbsIDob58+HPP+Hdd41WoqNHoW9fYxj9W2/B8eNmVykiIpmhACRyH4GB8OGHcOqUMXS+QgWIj4dx46BcOXj2Wfj5Zw2jFxHJSxSARNLJwwN69oQDB4yh86GhkJIC330HDRtCvXrw1VdabkNEJC9QABLJIAcHaNkS1q6F/fvh5ZfBxQV++QU6d4bSpSEyEi5dMrtSERG5GwUgkQdQrRp8/rkxy/SIEcas02fPwr/+ZTw6+7//M1qMREQkd1EAEskCPj4weDCcOGEMnQ8KguvXYepUqFIFWrSA1avVT0hEJLdQABLJQi4u0LUr7N5tDJ1v08aYeXrVKmje3GgxmjbNCEciImIeBSCRbGCxQOPGsGQJHD4Mb7wBBQvCf/4Dr71mPB4bNMh4XCYiIjkvVwSgSZMmUbp0aVxdXQkODmbHjh13PXbmzJlYLJZUm6ura6pjunfvfscxzZs3z+7bEElTuXLw738by22MHWsssXHpEnzwgdFh+sUXYdcus6sUEbEvpgegBQsWEBERwdChQ9m9ezc1a9YkLCyM8+fP3/UcT09Pzp07Z9tOnDhxxzHNmzdPdcz8+fOz8zZE7svLCyIi4MgRWLjQGDp/8ybMnQt16kCjRrBokZbbEBHJCaYHoHHjxvHqq68SHh5OlSpVmDJlCu7u7syYMeOu51gsFvz8/Gybr6/vHce4uLikOqZIkSLZeRsi6ebkBO3bw6ZNsHOnMXTeycl43b49lC8Pn3xiTLYoIiLZw9QAlJSUxK5duwgNDbXtc3BwIDQ0lK1bt971vKtXr1KqVCkCAwNp06YNv//++x3HbNiwAR8fHypWrEjPnj25dI9JWRITE4mPj0+1ieSEOnWMFqDjx42h80WLGj9HRBjLbbz5prFAq4iIZC1TA9DFixdJTk6+owXH19eX6OjoNM+pWLEiM2bMYOnSpcydO5eUlBTq16/P6dOnbcc0b96c2bNnExUVxejRo9m4cSMtWrQg+S7PFiIjI/Hy8rJtgYGBWXeTIulQooTRJ+jUKWPofOXKxoKrn35qtAi1awc//aRh9CIiWcVitZr3T+rZs2cpUaIEW7ZsISQkxLb/nXfeYePGjWzfvv2+17h58yaVK1emU6dOjBw5Ms1jjh07Rrly5Vi3bh1Nmza94/3ExEQSExNtr+Pj4wkMDCQuLg5PT89M3JnIg0lJMWaa/uQTY/6g22rVgn79oEMHcHY2rTwRkVwpPj4eLy+vdH1/m9oC5O3tjaOjIzExMan2x8TE4Ofnl65rFChQgEceeYQjR47c9ZiyZcvi7e1912NcXFzw9PRMtYmYycEBwsKM+YN+/x169ABXV2N+oa5djZFk778PFy+aXamISN5kagBydnamdu3aREVF2falpKQQFRWVqkXoXpKTk9m/fz/+/v53Peb06dNcunTpnseI5FZVqhiPxU6dMh6T+ftDdLQx83RgILz6qhGSREQk/UwfBRYREcH06dOZNWsWBw4coGfPniQkJBAeHg5A165dGThwoO34ESNGsGbNGo4dO8bu3bvp0qULJ06c4JVXXgGMDtJvv/0227Zt4/jx40RFRdGmTRvKly9PWFiYKfcokhW8vY2O0sePGx2na9eGGzeMtciqVYOnnzYmXRQRkfszPQB16NCBjz/+mCFDhhAUFMTevXtZtWqVrWP0yZMnOXfunO34y5cv8+qrr1K5cmVatmxJfHw8W7ZsoUqVKgA4Ojqyb98+nn76aSpUqMDLL79M7dq12bRpEy4uLqbco0hWcnY2hs7v3Pn30HkHB1i+HKpWhXffNTpQi4jI3ZnaCTq3ykgnKpHc4OBBo3P0qlXGaz8/GD0aunQxwpGIiD3IM52gRSRrVKwIK1YYrUDlyxt9hLp1gwYNjJYiERFJTQFIJJ+wWOCpp+C33+DDD43FV7dtg3r14OWX4R+DLUVE7JoCkEg+4+Ji9AM6eNBYaBVgxgyoUAHGjYOkJHPrExHJDRSARPKpgACYPRu2bDFGjMXHw1tvQY0af/cVEhGxVwpAIvlcSAjs2AFffAE+PkbLUIsWxrD5e8wfKiKSrykAidgBBwd46SU4dMhYaNXJ6e9h8wMHwtWrZlcoIpKzFIBE7IiXF4wdC/v2QbNmRn+gDz80RpHNnavFVkXEfigAidihypWNfkBLl0LZsnD2rNFhumFD2LXL7OpERLKfApCInbJYjH5Av/8Oo0aBh4fRYbpuXWN9sfPnza5QRCT7KACJ2DlXV6Mf0MGDxhIbVquxvliFCvDpp3DzptkViohkPQUgEQGgRAmjH9DmzVCrFsTFwZtvQs2asHat2dWJiGQtBSARSaVBA2PY/LRpxgr0Bw4YHabbtYNjx8yuTkQkaygAicgdHB2NfkCHDxuLrDo6wpIlUKUKDBoECQlmVygi8mAUgETkrgoXhk8+MYbNh4ZCYiJ88IExbH7+fA2bF5G8SwFIRO6rShVYswYWL4YyZeDMGXjhBWjUCPbsMbs6EZGMUwASkXSxWKBtW/jPf+D998Hd3egwXbs2vPYaXLhgdoUiIumnACQiGeLqCu+9Zwyb79TJeAw2bZoxbH78eA2bF5G8QQFIRDLloYfgq6/gp58gKAhiY6FvX3jkEYiKMrs6EZF7UwASkQfy2GPwyy8wZQoUK2bMLB0aCu3bw/HjZlcnIpI2BSAReWCOjkY/oMOHoU8f4/WiRVCpEgwZomHzIpL7KACJSJYpUsToB7R3LzzxhDFsfuRIIwgtWKBh8yKSeygAiUiWq1YN1q2D776DUqXg9Gno2BGaNIFffzW7OhERBSARySYWCzzzjLGUxogR4OZmdJiuVQt69oSLF82uUETsmQKQiGQrNzcYPBj++AM6dICUFKPDdIUKMHEi3LpldoUiYo8UgEQkR5QsCV9/DRs2QI0acPmy0WH6kUdg/XqzqxMRe6MAJCI5qnFj2LULPvsMihaF334zOkw/9xycOGF2dSJiLxSARCTHOTkZ/YAOH4bevcHBARYuNEaLffCB8ZhMRCQ7KQCJiGmKFjX6Ae3ZY4wQu3EDBg0y1hyLjze7OhHJzxSARMR0NWrAjz/Cl1+CiwssXw4hIXDkiNmViUh+pQAkIrmCxQLduxtD5QMCjFXn69Uz5hMSEclqCkAikqvUqwc7d0JwsDFSLCwM/v1vzSItIllLAUhEcp2AAGO4fLduRofofv3g5ZeNpTVERLJCrghAkyZNonTp0ri6uhIcHMyOHTvueuzMmTOxWCypNldX11THWK1WhgwZgr+/P25uboSGhnL48OHsvg0RyUKurkafoHHjjFFiX34Jjz8O0dFmVyYi+YHpAWjBggVEREQwdOhQdu/eTc2aNQkLC+P8+fN3PcfT05Nz587ZthP/mDxkzJgxjB8/nilTprB9+3Y8PDwICwvjxo0b2X07IpKFLBZ4801YsQIKF4atW6FOHfjlF7MrE5G8zvQANG7cOF599VXCw8OpUqUKU6ZMwd3dnRkzZtz1HIvFgp+fn23z9fW1vWe1Wvn0008ZNGgQbdq0oUaNGsyePZuzZ8+yZMmSHLgjEclqYWGwY4cxT9CZM/DYY/DVV2ZXJSJ5makBKCkpiV27dhEaGmrb5+DgQGhoKFu3br3reVevXqVUqVIEBgbSpk0bfv/9d9t7f/75J9HR0amu6eXlRXBw8F2vmZiYSHx8fKpNRHKXhx+GbdugVStjvqDOneHddyE52ezKRCQvMjUAXbx4keTk5FQtOAC+vr5E3+VBf8WKFZkxYwZLly5l7ty5pKSkUL9+fU6fPg1gOy8j14yMjMTLy8u2BQYGPuitiUg28PKCpUthwADj9Zgx8PTTEBdnbl0ikveY/ggso0JCQujatStBQUE0btyYRYsWUbx4caZOnZrpaw4cOJC4uDjbdurUqSysWESykqMjREYaj8BcXY3+QcHBcOiQ2ZWJSF5iagDy9vbG0dGRmJiYVPtjYmLw8/NL1zUKFCjAI488wpH/Thl7+7yMXNPFxQVPT89Um4jkbp06waZNUKIEHDxozB+0erXZVYlIXmFqAHJ2dqZ27dpERUXZ9qWkpBAVFUVISEi6rpGcnMz+/fvx9/cHoEyZMvj5+aW6Znx8PNu3b0/3NUUkb7g9IiwkxHgM1rIljB2rSRNF5P5MfwQWERHB9OnTmTVrFgcOHKBnz54kJCQQHh4OQNeuXRk4cKDt+BEjRrBmzRqOHTvG7t276dKlCydOnOCVV14BjBFi/fr14/3332fZsmXs37+frl27EhAQQNu2bc24RRHJRn5+sH49vPSSMWli//7GBIqa9UJE7sXJ7AI6dOjAhQsXGDJkCNHR0QQFBbFq1SpbJ+aTJ0/i4PB3Trt8+TKvvvoq0dHRFClShNq1a7NlyxaqVKliO+add94hISGBHj16EBsbS8OGDVm1atUdEyaKSP7g4gKffw41a0JEBMyZYzwWW7zYmFVaROSfLFarGov/KT4+Hi8vL+Li4tQfSCSPWbcOnn/eWEfM398IQcHBZlclIjkhI9/fpj8CExHJSqGhxmKqVarAuXPQuDHMnm12VSKS2ygAiUi+U66csWzG008bC6h262b0Dbp1y+zKRCS3UAASkXzJ09N4/DVokPF67Fh46inj0ZiIiAKQiORbDg4wciQsWABubsY8QcHB8McfZlcmImZTABKRfO/55+HnnyEwEA4fNkLQihVmVyUiZlIAEhG78MgjxqSJDRtCfLzxOGz0aE2aKGKvFIBExG74+EBUFLz6qhF8BgyALl3g+nWzKxORnKYAJCJ2xdkZpk6FiRONhVW/+goeewxOnza7MhHJSQpAImJ3LBbo3RvWroVixWDXLmNdsa1bza5MRHKKApCI2K3HHzcmTaxWDWJioEkT+PJLs6sSkZygACQidq1MGaPlp107SEoyFlXt10+TJorkdwpAImL3ChaEhQth6FDj9b//DS1awF9/mVuXiGQfBSAREYxJE4cNM4KQu7uxqGq9evD772ZXJiLZQQFIROR/tG8PW7ZAqVJw9Cg8+igsX252VSKS1RSARET+oWZNo3N048Zw9Sq0aQOjRmnSRJH8RAFIRCQNxYsbw+R79jSCz3vvQadOcO2a2ZWJSFZQABIRuYsCBeCzz2DKFHByMhZVbdgQTp40uzIReVAKQCIi9/Haa8YSGt7esGcP1K0LmzebXZWIPAgFIBGRdGjUyOgXVLMmnD8PTzwB06ebXZWIZJYCkIhIOpUuDT//DM8+CzdvQo8e0KeP8bOI5C0KQCIiGeDhAd98AyNGGK8nToSwMLh0ydy6RCRjFIBERDLIYoHBg2HxYmMW6fXrjX5B+/ebXZmIpJcCkIhIJrVta6wjVqYM/PknhIQYoUhEcj8FIBGRB1CtmtE5+vHHISEBnnnGeDyWkmJ2ZSJyLwpAIiIPqFgxWL0aXn/deD10KHTooEkTRXIzBSARkSxQoABMmGAMjS9QwFhU9fHHISbG7MpEJC0KQCIiWeiVV4yV5IsWhR07jMVUDxwwuyoR+ScFIBGRLNaokdE5ulw5OH4c6tc3RoqJSO6RqQB06tQpTp8+bXu9Y8cO+vXrx7Rp07KsMBGRvKxCBSMEhYRAbKwxV9Ds2WZXJSK3ZSoAvfDCC6z/73/OREdH8+STT7Jjxw7ee+89RtyeHUxExM4VL26sIfbcc8Zs0d26wbBhxuryImKuTAWg3377jXr16gHwzTffUK1aNbZs2cK8efOYOXNmVtYnIpKnubnB11/Du+8ar4cPN4JQUpK5dYnYu0wFoJs3b+Li4gLAunXrePrppwGoVKkS586dy/D1Jk2aROnSpXF1dSU4OJgdO3ak67yvv/4ai8VC27ZtU+3v3r07Fosl1da8efMM1yUikhUcHODDD2HqVHB0hDlzjEdily+bXZmI/cpUAKpatSpTpkxh06ZNrF271hYuzp49S7FixTJ0rQULFhAREcHQoUPZvXs3NWvWJCwsjPPnz9/zvOPHj9O/f38ee+yxNN9v3rw5586ds23z58/PUF0iIlmtRw/44QcoVAg2bDD6Bx07ZnZVIvYpUwFo9OjRTJ06lSZNmtCpUydq1qwJwLJly2yPxtJr3LhxvPrqq4SHh1OlShWmTJmCu7s7M2bMuOs5ycnJdO7cmeHDh1O2bNk0j3FxccHPz8+2FSlSJEN1iYhkh7Aw2LwZHnoIDh40hslv3252VSL2J1MBqEmTJly8eJGLFy+mCio9evRgypQp6b5OUlISu3btIjQ09O+CHBwIDQ1l69atdz1vxIgR+Pj48PLLL9/1mA0bNuDj40PFihXp2bMnl7RUs4jkEjVqwLZtEBQEFy5Akybw3XdmVyViXzIVgK5fv05iYqKtVeXEiRN8+umnHDx4EB8fn3Rf5+LFiyQnJ+Pr65tqv6+vL9HR0Wmes3nzZr744gumT59+1+s2b96c2bNnExUVxejRo9m4cSMtWrQgOTk5zeMTExOJj49PtYmIZKcSJeCnn6BlS7hxwxgpNnasRoiJ5JRMBaA2bdow+78TWsTGxhIcHMzYsWNp27YtkydPztIC/9eVK1d48cUXmT59Ot7e3nc9rmPHjjz99NNUr16dtm3b8v3337Nz5042bNiQ5vGRkZF4eXnZtsDAwGy6AxGRvxUqBEuXQq9eRvDp3x9694Zbt8yuTCT/y1QA2r17t63z8cKFC/H19eXEiRPMnj2b8ePHp/s63t7eODo6EvOPxXJiYmLw8/O74/ijR49y/PhxWrdujZOTE05OTsyePZtly5bh5OTE0aNH0/ycsmXL4u3tzZEjR9J8f+DAgcTFxdm2U6dOpfseREQehJMTTJxotP5YLDB5MrRpA1eumF2ZSP6WqQB07do1ChUqBMCaNWt45plncHBw4NFHH+XEiRPpvo6zszO1a9cmKirKti8lJYWoqChCQkLuOL5SpUrs37+fvXv32rann36axx9/nL1799615eb06dNcunQJf3//NN93cXHB09Mz1SYiklMsFoiIMBZQdXWFFSuM5TTOnDG7MpH8K1MBqHz58ixZsoRTp06xevVqmjVrBsD58+czHB4iIiKYPn06s2bN4sCBA/Ts2ZOEhATCw8MB6Nq1KwMHDgTA1dWVatWqpdoKFy5MoUKFqFatGs7Ozly9epW3336bbdu2cfz4caKiomjTpg3ly5cnLCwsM7crIpIjnnnGGB5fvDjs3QvBwfDrr2ZXJZI/ZSoADRkyhP79+1O6dGnq1atna61Zs2YNjzzySIau1aFDBz7++GOGDBlCUFAQe/fuZdWqVbaO0SdPnszQ5IqOjo7s27ePp59+mgoVKvDyyy9Tu3ZtNm3aZJu8UUQktwoONobFV6pktAA1bAirVpldlUj+Y7FaMzfmIDo6mnPnzlGzZk0cHIwctWPHDjw9PalUqVKWFpnT4uPj8fLyIi4uTo/DRMQUly//3SLk6AiffWZMpCgid5eR7+9MB6Dbbq8K/9BDDz3IZXIVBSARyQ2SkuCVV4ylMwDeeQciI42lNUTkThn5/s7U/41SUlIYMWIEXl5elCpVilKlSlG4cGFGjhxJSkpKpooWEZHUnJ1h1ixjBXmAMWOgY0e4ft3UskTyBafMnPTee+/xxRdf8OGHH9KgQQPAmKBw2LBh3Lhxgw8++CBLixQRsVcWCwwdCmXKGK1B334Lp08b8wcVL252dSJ5V6YegQUEBDBlyhTbKvC3LV26lF69enEmj4/d1CMwEcmNNmyAdu0gNhbKljWGy1esaHZVIrlHtj8C++uvv9Ls6FypUiX++uuvzFxSRETuo0kT2LLFaA06dsxYTf6nn8yuSiRvylQAqlmzJhMnTrxj/8SJE6lRo8YDFyUiImmrXNlYSDU42Bgp9uST8NVXZlclkvdkqg/QmDFjaNWqFevWrbPNAbR161ZOnTrFihUrsrRAERFJzccHfvwRXnwRFi2Czp2NFqH33jP6DInI/WWqBahx48YcOnSIdu3aERsbS2xsLM888wy///47c26P1xQRkWzj7m50iH7rLeP14MHw8svG0HkRub8Hngfof/3666/UqlWL5OTkrLqkKdQJWkTyksmT4fXXISUFnngCvvsOChc2uyqRnJftnaBFRCT36NkTli8HDw/j0ViDBpCBdalF7JICkIhIPtCyJWzaBAEB8J//GJ2kf/nF7KpEci8FIBGRfOKRR4yFVGvUgJgYaNzYmDBRRO6UoVFgzzzzzD3fj42NfZBaRETkAT30kNES1KGDsYp8u3bwySfQt6/ZlYnkLhkKQF5eXvd9v2vXrg9UkIiIPBhPT6NP0Ouvw9Sp0K8fHD1qBCFHR7OrE8kdsnQUWH6hUWAikh9YrfDxx8Yq8gCtW8P8+UZnaZH8SKPAREQEiwXefhu++QZcXIxWocaN4dw5sysTMZ8CkIhIPvfcc8bweG9v2LULHn0UfvvN7KpEzKUAJCJiB+rXN9YQq1ABTp405gpat87sqkTMowAkImInypWDrVvhsccgPh5atIAZM8yuSsQcCkAiInakaFFYuxZeeAFu3TLWD3vvPWMZDRF7ogAkImJnXFxg7lxjAVWAUaOMFeVv3DC3LpGcpAAkImKHLBYYMcJ4BObkBF9/DU8+CZcumV2ZSM5QABIRsWPh4caM0V5esHkzhITAkSNmVyWS/RSARETsXNOmsGULlCoFhw8bw+S3bDG7KpHspQAkIiJUqWIMk69Tx3gM9sQTsGCB2VWJZB8FIBERAcDPDzZsgDZtIDEROnaEDz80ltQQyW8UgERExMbDA777zlhAFWDgQOjRA27eNLUskSynACQiIqk4Ohorx0+YAA4O8Pnn0KoVxMWZXZlI1lEAEhGRNL3+OixZAu7uxuSJDRsay2iI5AcKQCIicletW8OmTeDvbyygGhxsLKgqktcpAImIyD3VqgXbt0P16hAdDY0awdKlZlcl8mAUgERE5L4CA42JEsPC4No1aNcO/v1vs6sSybxcEYAmTZpE6dKlcXV1JTg4mB07dqTrvK+//hqLxULbtm1T7bdarQwZMgR/f3/c3NwIDQ3l8OHD2VC5iIj98PSE5cuNUWFWqzFS7I03IDnZ7MpEMs70ALRgwQIiIiIYOnQou3fvpmbNmoSFhXH+/Pl7nnf8+HH69+/PY489dsd7Y8aMYfz48UyZMoXt27fj4eFBWFgYN7TSn4jIAylQAKZMgTFjjNcTJhitQVevmluXSEZZrFZzp7gKDg6mbt26TJw4EYCUlBQCAwPp06cPAwYMSPOc5ORkGjVqxEsvvcSmTZuIjY1lyZIlgNH6ExAQwFtvvUX//v0BiIuLw9fXl5kzZ9KxY8f71hQfH4+XlxdxcXF4enpmzY2KiOQzCxfCiy8aq8jXqmW0DgUEmF2V2LOMfH+b2gKUlJTErl27CA0Nte1zcHAgNDSUrVu33vW8ESNG4OPjw8svv3zHe3/++SfR0dGprunl5UVwcPBdr5mYmEh8fHyqTURE7u3ZZ2H9eiheHHbvNtYQ27/f7KpE0sfUAHTx4kWSk5Px9fVNtd/X15fo6Og0z9m8eTNffPEF06dPT/P92+dl5JqRkZF4eXnZtsDAwIzeioiIXXr0UWMNsYoV4dQpaNAAVq82uyqR+zO9D1BGXLlyhRdffJHp06fj7e2dZdcdOHAgcXFxtu3UqVNZdm0RkfyubFlj9fjGjeHKFWPW6GnTzK5K5N6czPxwb29vHB0diYmJSbU/JiYGPz+/O44/evQox48fp3Xr1rZ9KSkpADg5OXHw4EHbeTExMfj7+6e6ZlBQUJp1uLi44OLi8qC3IyJit4oWhTVr4JVXYM4ceO01OHoUIiON5TREchtT/1o6OztTu3ZtoqKibPtSUlKIiooiJCTkjuMrVarE/v372bt3r217+umnefzxx9m7dy+BgYGUKVMGPz+/VNeMj49n+/btaV5TRESyhrMzzJoFw4YZr8eMgQ4d4Pp1U8sSSZOpLUAAERERdOvWjTp16lCvXj0+/fRTEhISCA8PB6Br166UKFGCyMhIXF1dqVatWqrzCxcuDJBqf79+/Xj//fd5+OGHKVOmDIMHDyYgIOCO+YJERCRrWSwwdCiUKwcvvWSMFDt92pg52sfH7OpE/mZ6AOrQoQMXLlxgyJAhREdHExQUxKpVq2ydmE+ePIlDBttP33nnHRISEujRowexsbE0bNiQVatW4erqmh23ICIi/9ClizF7dLt2RifpRx+FFSugUiWzKxMxmD4PUG6keYBERLLGwYPQsiUcOwaFC8PixdCkidlVSX6VZ+YBEhGR/K1iRaMFKCQEYmOhWTOYPdvsqkQUgEREJJsVLw5RUfDcc3DzJnTrZnSU1vMHMZMCkIiIZDs3N/j6a7i9wtHw4dC1KyQmmluX2C8FIBERyREODsa8QNOmgaMjzJ1rPBL76y+zKxN7pAAkIiI56tVXYeVK8PSEn36C+vWNSRNFcpICkIiI5Lgnn4TNm42h8gcPGsPk77EGtkiWUwASERFTVK9ujBCrVQsuXoTHH4dvvzW7KrEXCkAiImKagADjMVjr1kaH6Oefh9GjNUJMsp8CkIiImMrDw5gg8Y03jNcDBhiLqd68aW5dkr8pAImIiOkcHeHf/zY2BweYPh2eegri4syuTPIrBSAREck13ngDliwBd3dYswYaNoSTJ82uSvIjBSAREclVWrc2+gX5+cFvv0FwMOzaZXZVkt8oAImISK5TuzZs326MFIuOhkaNYNkys6uS/EQBSEREcqWSJY25gpo1g2vXoG1bGD/e7Kokv1AAEhGRXMvTE77/3pg92mqFvn2NfkLJyWZXJnmdApCIiORqBQrA1KnG/EAAEyZAu3Zw9aq5dUnepgAkIiK5nsUC77wD33wDLi6wfDk0bgxnz5pdmeRVCkAiIpJnPPccrF8P3t6we7exhtj+/WZXJXmRApCIiOQpISHGGmIVK8KpU9CggTFnkEhGKACJiEieU64cbNliPAa7cgVatjRmjxZJLwUgERHJk4oWhdWr4cUXjVFhPXoY64ilpJhdmeQFCkAiIpJnubjArFkwbJjxevRo6NgRrl83tSzJAxSAREQkT7NYYOhQmD3bGDL/7bfQtClcuGB2ZZKbKQCJiEi+8OKLRmfowoVh61ZjhNgff5hdleRWCkAiIpJvNGlihJ+yZeHYMahfHzZuNLsqyY0UgEREJF+pVMkYJv/oo3D5Mjz5JMyZY3ZVktsoAImISL5TvDj8+KMxceLNm9C1q7GchshtCkAiIpIvubnB119DRITxumdPWLDA3Jok91AAEhGRfMvBAT7+2Ag/Vit06QIrV5pdleQGCkAiIpKvWSwwcSJ06gS3bkH79rB5s9lVidkUgEREJN9zcDAmTGzZ0pgk8amnYO9es6sSMykAiYiIXbg9SeJjj0FcHDRrBocOmV2VmCVXBKBJkyZRunRpXF1dCQ4OZseOHXc9dtGiRdSpU4fChQvj4eFBUFAQc/4xvrF79+5YLJZUW/PmzbP7NkREJJdzd4flyyEoyJgp+sknjRXlxf6YHoAWLFhAREQEQ4cOZffu3dSsWZOwsDDOnz+f5vFFixblvffeY+vWrezbt4/w8HDCw8NZvXp1quOaN2/OuXPnbNv8+fNz4nZERCSX8/IyFlGtUAFOnjRagrRshv2xWK1Wq5kFBAcHU7duXSZOnAhASkoKgYGB9OnThwEDBqTrGrVq1aJVq1aMHDkSMFqAYmNjWbJkSaZqio+Px8vLi7i4ODw9PTN1DRERyd1OnoQGDeD0aahd25g3SP/k520Z+f42tQUoKSmJXbt2ERoaatvn4OBAaGgoW7duve/5VquVqKgoDh48SKNGjVK9t2HDBnx8fKhYsSI9e/bk0qVLd71OYmIi8fHxqTYREcnfSpaEtWvB2xt27YKnn9Yq8vbE1AB08eJFkpOT8fX1TbXf19eX6Ojou54XFxdHwYIFcXZ2plWrVkyYMIEnn3zS9n7z5s2ZPXs2UVFRjB49mo0bN9KiRQuSk5PTvF5kZCReXl62LTAwMGtuUEREcrVKlYzHYYUKGWuGdehgzBwt+Z+T2QVkRqFChdi7dy9Xr14lKiqKiIgIypYtS5MmTQDo2LGj7djq1atTo0YNypUrx4YNG2jatOkd1xs4cCARt6cKxWhCUwgSEbEPtWrB999DWJjRQfqll4wh8w6m95KV7GRqAPL29sbR0ZGYmJhU+2NiYvDz87vreQ4ODpQvXx6AoKAgDhw4QGRkpC0A/VPZsmXx9vbmyJEjaQYgFxcXXFxcMn8jIiKSpzVqBAsXQtu2MHcuFC4M48cbkyhK/mRqvnV2dqZ27dpERUXZ9qWkpBAVFUVISEi6r5OSkkJiYuJd3z99+jSXLl3C39//geoVEZH8q1Uro+Xn9szRQ4eaXZFkJ9MfgUVERNCtWzfq1KlDvXr1+PTTT0lISCA8PByArl27UqJECSIjIwGjv06dOnUoV64ciYmJrFixgjlz5jB58mQArl69yvDhw2nfvj1+fn4cPXqUd955h/LlyxMWFmbafYqISO73wgsQGwu9e8PIkVCkCLz5ptlVSXYwPQB16NCBCxcuMGTIEKKjowkKCmLVqlW2jtEnT57E4X8exCYkJNCrVy9Onz6Nm5sblSpVYu7cuXTo0AEAR0dH9u3bx6xZs4iNjSUgIIBmzZoxcuRIPeYSEZH76tULLl+GQYOMleQLF4b//je55COmzwOUG2keIBER+2a1Qv/+MG6c0Rl64UJo187squR+8sw8QCIiIrmRxQIff2yMCEtJgY4d4X+6q0o+oAAkIiKSBosFpk6FZ56BpCRo0wa2bze7KskqCkAiIiJ34eQEX30FoaGQkAAtWsBvv5ldlWQFBSAREZF7cHGBxYshONjoHN2sGRw7ZnZV8qAUgERERO6jYEFYsQKqVYNz5+DJJ43/lbxLAUhERCQdihaFNWugbFmjBahZM/jrL7OrksxSABIREUknf39jBXl/f6MvUKtWcPWq2VVJZigAiYiIZEDZskZLUJEisG2bMUrsHqsxSS6lACQiIpJB1arBypXg4WG0CHXuDLdumV2VZIQCkIiISCYEB8OSJeDsDN99B6+9ZswgLXmDApCIiEgmhYbC/PnGchkzZsDbbysE5RUKQCIiIg/gmWfg88+Nn8eOhchIc+uR9FEAEhEReUDh4cbCqQDvvQeTJ5tbj9yfApCIiEgWePNNGDTI+Ll3b2MJDcm9FIBERESyyIgRRvixWqFbN/jhB7MrkrtRABIREckiFguMH//3sPhnn4WffjK7KkmLApCIiEgWcnCAL7+Ep56CGzegdWvYvdvsquSfFIBERESyWIEC8M030LgxxMdD8+Zw8KDZVcn/UgASERHJBm5usGwZ1KoFFy4YK8ifPGl2VXKbApCIiEg28fSEVaugYkU4dcoIQefPm12VgAKQiIhItipe3FgvrGRJOHTIeBwWF2d2VaIAJCIiks0CA40QVLw47NljdIy+ds3squybApCIiEgOqFABVq82Hott2gTPPQc3b5pdlf1SABIREckhjzxiTI7o5gYrVhiTJaakmF2VfVIAEhERyUENG8LCheDkZKwk//rrWkHeDApAIiIiOaxlS5gzx5g5evJkGDzY7IrsjwKQiIiICTp2hM8+M37+4AMYO9bceuyNApCIiIhJ/u//YNQo4+f+/eGLL8ytx54oAImIiJhowAB4+23j5x49jP5Bkv0UgERERExkscDo0fDKK8aIsBdeMOYMkuylACQiImIyiwWmTPl7bqC2bWHrVrOryt8UgERERHIBR0djZFizZsYs0S1bwv79ZleVf+WKADRp0iRKly6Nq6srwcHB7Nix467HLlq0iDp16lC4cGE8PDwICgpizpw5qY6xWq0MGTIEf39/3NzcCA0N5fDhw9l9GyIiIg/ExQUWLYKQEIiNNcLQ0aNmV5U/mR6AFixYQEREBEOHDmX37t3UrFmTsLAwzt9ludyiRYvy3nvvsXXrVvbt20d4eDjh4eGsXr3adsyYMWMYP348U6ZMYfv27Xh4eBAWFsaNGzdy6rZEREQyxcPDmC26Rg2IjjZWkD971uyq8h+L1Wru/JPBwcHUrVuXiRMnApCSkkJgYCB9+vRhwIAB6bpGrVq1aNWqFSNHjsRqtRIQEMBbb71F//79AYiLi8PX15eZM2fSsWPH+14vPj4eLy8v4uLi8PT0zPzNiYiIZFJ0tDFr9NGjUKUK/PQTFCtmdlW5W0a+v01tAUpKSmLXrl2Ehoba9jk4OBAaGsrWdPT+slqtREVFcfDgQRo1agTAn3/+SXR0dKprenl5ERwcfNdrJiYmEh8fn2oTERExk5+fMRosIAD+8x+jT9CVK2ZXlX+YGoAuXrxIcnIyvr6+qfb7+voSHR191/Pi4uIoWLAgzs7OtGrVigkTJvDkk08C2M7LyDUjIyPx8vKybYGBgQ9yWyIiIlmiTBlYswaKFoUdO4zRYerNkTVM7wOUGYUKFWLv3r3s3LmTDz74gIiICDZs2JDp6w0cOJC4uDjbdurUqawrVkRE5AFUrQorV0LBgvDjj/DEE3CXbrKSAU5mfri3tzeOjo7ExMSk2h8TE4Ofn99dz3NwcKB8+fIABAUFceDAASIjI2nSpIntvJiYGPz9/VNdMygoKM3rubi44OLi8oB3IyIikj3q1TM6RrdpY8wPFBwMy5dDtWpmV5Z3mdoC5OzsTO3atYmKirLtS0lJISoqipCQkHRfJyUlhcTERADKlCmDn59fqmvGx8ezffv2DF1TREQkN2nUCLZtg/Ll4fhxqF8fVq0yu6q8y/RHYBEREUyfPp1Zs2Zx4MABevbsSUJCAuHh4QB07dqVgQMH2o6PjIxk7dq1HDt2jAMHDjB27FjmzJlDly5dALBYLPTr14/333+fZcuWsX//frp27UpAQABt27Y14xZFRESyRMWKRghq3NjoEN2qFUyYYHZVeZOpj8AAOnTowIULFxgyZAjR0dEEBQWxatUqWyfmkydP4uDwd05LSEigV69enD59Gjc3NypVqsTcuXPp0KGD7Zh33nmHhIQEevToQWxsLA0bNmTVqlW4urrm+P2JiIhkpWLFjI7RPXvCjBnwxhtw8CB8+ik4mf6tnneYPg9QbqR5gEREJLezWuHjj+Hdd42fmzWDb74BLy+zKzNPnpkHSERERDLHYoG33zaWznB3N1qF6teHY8fMrixvUAASERHJw9q2hU2b/p4wMTgYfv7Z7KpyPwUgERGRPK5WLWOixFq14OJFY66guXPNrip3UwASERHJB0qUMNYLe+YZSEqCF1+EQYMgJcXsynInBSAREZF8wsMDvv0Wbs8e88EH0LEjXLtmbl25kQKQiIhIPuLgAKNGwcyZUKCAEYiaNIFz58yuLHdRABIREcmHunWDdeuMhVR37jQ6R//6q9lV5R4KQCIiIvlUo0awfbsxg/SpU9CggbGGmCgAiYiI5GvlyxsLqDZtCgkJxoKqY8cakyfaMwUgERGRfK5IEVi5El57zQg+/fsbP9+8aXZl5lEAEhERsQMFCsDkyfDJJ0ZH6enToXlzuHzZ7MrMoQAkIiJiJywW6NcPli2DggXhxx/h0Ufh8GGzK8t5CkAiIiJ2plUrY7mMkiXh0CFjhNiGDWZXlbMUgEREROxQjRrGCLHgYOMx2JNPwowZZleVcxSARERE7JSfH6xfDx06wK1b8PLL8M479rF8hgKQiIiIHXNzg6++giFDjNcffQTt2xtD5vMzBSARERE75+AAw4cbK8i7uMCSJfDYY3D6tNmVZR8FIBEREQGgc2djZFjx4rBnj9E/aNcus6vKHgpAIiIiYlO/PuzYAVWrwtmzRkvQokVmV5X1FIBEREQkldKlYcsWY6LE69eNPkGRkflr+QwFIBEREbmDp6excGqfPsbrf/0LwsMhMdHcurKKApCIiIikyckJxo+HiRPB0RFmzTLmC7p40ezKHpwCkIiIiNxT797www9Gq9CmTcbyGX/8YXZVD0YBSERERO4rLAy2boUyZeDoUSMErVtndlWZpwAkIiIi6VKlirF8RoMGEBdndJKeMsXsqjJHAUhERETSrXhxiIqCLl0gORl69oQ33zR+zksUgERERCRDXFxg9mx4/33j9aefQps2cOWKqWVliAKQiIiIZJjFAu+9B998A66uRifpBg3gxAmzK0sfBSARERHJtOeeg40bjZXl9++HevVg2zazq7o/BSARERF5IPXqGctn1KwJ589Dkybw9ddmV3VvCkAiIiLywAIDYfNmaN3amC26UydjhfncunxGrghAkyZNonTp0ri6uhIcHMyOHTvueuz06dN57LHHKFKkCEWKFCE0NPSO47t3747FYkm1NW/ePLtvQ0RExK4VLAiLF8Nbbxmvhw0zVpi/ccPUstJkegBasGABERERDB06lN27d1OzZk3CwsI4f/58msdv2LCBTp06sX79erZu3UpgYCDNmjXjzJkzqY5r3rw5586ds23z58/PidsRERGxa46O8PHHMG2asZTG/PnwxBMQE2N2ZalZrFZzG6eCg4OpW7cuEydOBCAlJYXAwED69OnDgAED7nt+cnIyRYoUYeLEiXTt2hUwWoBiY2NZsmRJpmqKj4/Hy8uLuLg4PD09M3UNERERe/fjj8ZK8rGxUKoUfP89VKuWfZ+Xke9vU1uAkpKS2LVrF6GhobZ9Dg4OhIaGsnXr1nRd49q1a9y8eZOiRYum2r9hwwZ8fHyoWLEiPXv25NKlS1lau4iIiNzbE08YI8LKlzeGx9evDytWmF2VwdQAdPHiRZKTk/H19U2139fXl+jo6HRd49133yUgICBViGrevDmzZ88mKiqK0aNHs3HjRlq0aEHyXaapTExMJD4+PtUmIiIiD65iRSMENW5sTJTYurWxwrzZnaOdzP34B/Phhx/y9ddfs2HDBlxdXW37O3bsaPu5evXq1KhRg3LlyrFhwwaaNm16x3UiIyMZPnx4jtQsIiJib4oVgzVrjGUzZsyAvn3hzz/hk0/Mq8nUFiBvb28cHR2J+UfPqJiYGPz8/O557scff8yHH37ImjVrqFGjxj2PLVu2LN7e3hw5ciTN9wcOHEhcXJxtO3XqVMZuRERERO7J2Rk+/xzGjDFmka5d29x6TA1Azs7O1K5dm6ioKNu+lJQUoqKiCAkJuet5Y8aMYeTIkaxatYo6derc93NOnz7NpUuX8Pf3T/N9FxcXPD09U20iIiKStSwWePtt+M9/jMVUzWT6MPiIiAimT5/OrFmzOHDgAD179iQhIYHw8HAAunbtysCBA23Hjx49msGDBzNjxgxKly5NdHQ00dHRXL16FYCrV6/y9ttvs23bNo4fP05UVBRt2rShfPnyhIWFmXKPIiIi8rdKlcyuIBf0AerQoQMXLlxgyJAhREdHExQUxKpVq2wdo0+ePImDw985bfLkySQlJfHss8+mus7QoUMZNmwYjo6O7Nu3j1mzZhEbG0tAQADNmjVj5MiRuLi45Oi9iYiISO5k+jxAuZHmARIREcl78sw8QCIiIiJmUAASERERu6MAJCIiInZHAUhERETsjgKQiIiI2B0FIBEREbE7CkAiIiJidxSARERExO4oAImIiIjdUQASERERu6MAJCIiInbH9MVQc6Pby6PFx8ebXImIiIik1+3v7fQsc6oAlIYrV64AEBgYaHIlIiIiklFXrlzBy8vrnsdoNfg0pKSkcPbsWQoVKoTFYjG7nFwpPj6ewMBATp06dd8VdyX76feRu+j3kbvo95G7ZOfvw2q1cuXKFQICAnBwuHcvH7UApcHBwYGHHnrI7DLyBE9PT/2Dkovo95G76PeRu+j3kbtk1+/jfi0/t6kTtIiIiNgdBSARERGxOwpAkikuLi4MHToUFxcXs0sR9PvIbfT7yF30+8hdcsvvQ52gRURExO6oBUhERETsjgKQiIiI2B0FIBEREbE7CkAiIiJidxSAJN0iIyOpW7cuhQoVwsfHh7Zt23Lw4EGzy5L/+vDDD7FYLPTr18/sUuzamTNn6NKlC8WKFcPNzY3q1avzyy+/mF2WXUpOTmbw4MGUKVMGNzc3ypUrx8iRI9O1TpQ8uJ9++onWrVsTEBCAxWJhyZIlqd63Wq0MGTIEf39/3NzcCA0N5fDhwzlWnwKQpNvGjRvp3bs327ZtY+3atdy8eZNmzZqRkJBgdml2b+fOnUydOpUaNWqYXYpdu3z5Mg0aNKBAgQKsXLmS//znP4wdO5YiRYqYXZpdGj16NJMnT2bixIkcOHCA0aNHM2bMGCZMmGB2aXYhISGBmjVrMmnSpDTfHzNmDOPHj2fKlCls374dDw8PwsLCuHHjRo7Up2HwkmkXLlzAx8eHjRs30qhRI7PLsVtXr16lVq1afPbZZ7z//vsEBQXx6aefml2WXRowYAA///wzmzZtMrsUAZ566il8fX354osvbPvat2+Pm5sbc+fONbEy+2OxWFi8eDFt27YFjNafgIAA3nrrLfr37w9AXFwcvr6+zJw5k44dO2Z7TWoBkkyLi4sDoGjRoiZXYt969+5Nq1atCA0NNbsUu7ds2TLq1KnDc889h4+PD4888gjTp083uyy7Vb9+faKiojh06BAAv/76K5s3b6ZFixYmVyZ//vkn0dHRqf7d8vLyIjg4mK1bt+ZIDVoMVTIlJSWFfv360aBBA6pVq2Z2OXbr66+/Zvfu3ezcudPsUgQ4duwYkydPJiIign/961/s3LmTN954A2dnZ7p162Z2eXZnwIABxMfHU6lSJRwdHUlOTuaDDz6gc+fOZpdm96KjowHw9fVNtd/X19f2XnZTAJJM6d27N7/99hubN282uxS7derUKfr27cvatWtxdXU1uxzB+A+DOnXqMGrUKAAeeeQRfvvtN6ZMmaIAZIJvvvmGefPm8dVXX1G1alX27t1Lv379CAgI0O9D9AhMMu7111/n+++/Z/369Tz00ENml2O3du3axfnz56lVqxZOTk44OTmxceNGxo8fj5OTE8nJyWaXaHf8/f2pUqVKqn2VK1fm5MmTJlVk395++20GDBhAx44dqV69Oi+++CJvvvkmkZGRZpdm9/z8/ACIiYlJtT8mJsb2XnZTAJJ0s1qtvP766yxevJgff/yRMmXKmF2SXWvatCn79+9n7969tq1OnTp07tyZvXv34ujoaHaJdqdBgwZ3TA1x6NAhSpUqZVJF9u3atWs4OKT+mnN0dCQlJcWkiuS2MmXK4OfnR1RUlG1ffHw827dvJyQkJEdq0CMwSbfevXvz1VdfsXTpUgoVKmR7Tuvl5YWbm5vJ1dmfQoUK3dH/ysPDg2LFiqlflknefPNN6tevz6hRo3j++efZsWMH06ZNY9q0aWaXZpdat27NBx98QMmSJalatSp79uxh3LhxvPTSS2aXZheuXr3KkSNHbK///PNP9u7dS9GiRSlZsiT9+vXj/fff5+GHH6ZMmTIMHjyYgIAA20ixbGcVSScgze3LL780uzT5r8aNG1v79u1rdhl2bfny5dZq1apZXVxcrJUqVbJOmzbN7JLsVnx8vLVv377WkiVLWl1dXa1ly5a1vvfee9bExESzS7ML69evT/M7o1u3blar1WpNSUmxDh482Orr62t1cXGxNm3a1Hrw4MEcq0/zAImIiIjdUR8gERERsTsKQCIiImJ3FIBERETE7igAiYiIiN1RABIRERG7owAkIiIidkcBSEREROyOApCISDpYLBaWLFlidhkikkUUgEQk1+vevTsWi+WOrXnz5maXJiJ5lNYCE5E8oXnz5nz55Zep9rm4uJhUjYjkdWoBEpE8wcXFBT8/v1RbkSJFAOPx1OTJk2nRogVubm6ULVuWhQsXpjp///79PPHEE7i5uVGsWDF69OjB1atXUx0zY8YMqlatiouLC/7+/rz++uup3r948SLt2rXD3d2dhx9+mGXLlmXvTYtItlEAEpF8YfDgwbRv355ff/2Vzp0707FjRw4cOABAQkICYWFhFClShJ07d/Ltt9+ybt26VAFn8uTJ9O7dmx49erB//36WLVtG+fLlU33G8OHDef7559m3bx8tW7akc+fO/PXXXzl6nyKSRXJs2VURkUzq1q2b1dHR0erh4ZFq++CDD6xWq9UKWP/v//4v1TnBwcHWnj17Wq1Wq3XatGnWIkWKWK9evWp7/4cffrA6ODhYo6OjrVar1RoQEGB977337loDYB00aJDt9dWrV62AdeXKlVl2nyKSc9QHSETyhMcff5zJkyen2le0aFHbzyEhIaneCwkJYe/evQAcOHCAmjVr4uHhYXu/QYMGpKSkcPDgQSwWC2fPnqVp06b3rKFGjRq2nz08PPD09OT8+fOZvSURMZECkIjkCR4eHnc8ksoqbm5u6TquQIECqV5bLBZSUlKyoyQRyWbqAyQi+cK2bdvueF25cmUAKleuzK+//kpCQoLt/Z9//hkHBwcqVqxIoUKFKF26NFFRUTlas4iYRy1AIpInJCYmEh0dnWqfk5MT3t7eAHz77bfUqVOHhg0bMm/ePHbs2MEXX3wBQOfOnRk6dCjdunVj2LBhXLhwgT59+vDiiy/i6+sLwLBhw/i///s/fHx8aNGiBVeuXOHnn3+mT58+OXujIpIjFIBEJE9YtWoV/v7+qfZVrFiRP/74AzBGaH399df06tULf39/5s+fT5UqVQBwd3dn9erV9O3bl7p16+Lu7k779u0ZN26c7VrdunXjxo0bfPLJJ/Tv3x9vb2+effbZnLtBEclRFqvVajW7CBGRB2GxWFi8eDFt27Y1uxQRySPUB0hERETsjgKQiIiI2B31ARKRPE9P8kUko9QCJCIiInZHAUhERETsjgKQiIiI2B0FIBEREbE7CkAiIiJidxSARERExO4oAImIiIjdUQASERERu6MAJCIiInbn/wF2hTwpbp288gAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a list of epochs\n",
        "epochs = range(1, len(loss_hist) + 1)\n",
        "\n",
        "# Plot the training loss as a curve\n",
        "plt.plot(epochs, loss_hist, 'b-', label='Training loss')\n",
        "\n",
        "# Add a title and axis labels\n",
        "plt.title(\"Training Loss History\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'summary' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msummary\u001b[49m(model, (\u001b[38;5;241m160\u001b[39m,\u001b[38;5;241m216\u001b[39m), \u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'summary' is not defined"
          ]
        }
      ],
      "source": [
        "print(summary(model, (160,216), 64, \"cuda\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "aYojwtoSBPF6"
      },
      "outputs": [],
      "source": [
        "# https://drive.google.com/file/d/1--QAwB8w8GVWZ1wmpPOPaj6jNkGWDI3c/view?usp=sharing\n",
        "# !gdown --id 1--QAwB8w8GVWZ1wmpPOPaj6jNkGWDI3c\n",
        "# !unzip /content/both_tensors_160_216_validation_1.zip\n",
        "import torch \n",
        "# path warning!! Colab dir are different\n",
        "prog_tensors_validation = torch.load('./content/progressive_rock_songs_validation_tensor.pt')\n",
        "non_prog_tensors_validation = torch.load('./content/non_progressive_rock_songs_validation_tensor.pt')\n",
        "# non_prog_tensors_validation = non_prog_tensors_validation.reshape(1530, 216, 160)\n",
        "# prog_tensors_validation = prog_tensors_validation.reshape(1938, 216, 160)\n",
        "# prog_tensors_validation = prog_tensors_validation.float()\n",
        "# non_prog_tensors_validation = non_prog_tensors_validation.float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu2hNB4GY1Yt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "j_4ukuwaC9pq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 216, 160])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "non_prog_tensors_validation[0][0].shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB-KO4o8Y3Ma"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "PZpCheS0kAUL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28 2\n",
            "2 57\n",
            "1739 199\n",
            "206 1324\n"
          ]
        }
      ],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "## For the song\n",
        "device = get_device()\n",
        "model.to(\"cpu\")\n",
        "prog_true = 0\n",
        "prog_false = 0\n",
        "non_prog_true = 0\n",
        "non_prog_false = 0\n",
        "# for the individual snippets\n",
        "total_prog_true = 0\n",
        "total_prog_false = 0\n",
        "total_non_prog_true = 0\n",
        "total_non_prog_false = 0\n",
        "with torch.no_grad():  # Disable gradients during validation\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for obj in prog_tensors_validation:\n",
        "    total = len(obj)\n",
        "    it = 0\n",
        "    for i in obj:\n",
        "      i = i.reshape(1,160,216)\n",
        "      i = i.to(\"cpu\")\n",
        "      outputs = model(i)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      # print(predicted)\n",
        "      if(predicted.item() == 1):\n",
        "        it = it+1\n",
        "    \n",
        "    total_prog_true += it\n",
        "    total_prog_false += (total-it)\n",
        "    if(2*it >= total):\n",
        "      prog_true +=1\n",
        "    else:\n",
        "      prog_false +=1\n",
        "  for obj in non_prog_tensors_validation:\n",
        "    total = len(obj)\n",
        "    it = 0\n",
        "    for i in obj:\n",
        "      i = i.reshape(1,160,216)\n",
        "      outputs = model(i)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      # print(predicted)\n",
        "      if(predicted.item() == 0):\n",
        "        it = it+1\n",
        "    total_non_prog_true += it\n",
        "    total_non_prog_false += (total-it)\n",
        "    if(2*it >= total):\n",
        "      non_prog_true +=1\n",
        "    else:\n",
        "      non_prog_false +=1\n",
        "\n",
        "\n",
        "print(prog_true, prog_false)\n",
        "print(non_prog_false, non_prog_true)\n",
        "\n",
        "print(total_prog_true, total_prog_false)\n",
        "print(total_non_prog_false, total_non_prog_true)\n",
        "# with torch.no_grad():  # Disable gradients during validation\n",
        "#   correct = 0\n",
        "#   total = 0\n",
        "#   for inputs, labels in val_loader:\n",
        "#     inputs.shape\n",
        "#     outputs = model(inputs)\n",
        "#     _, predicted = torch.max(outputs.data, 1)\n",
        "#     total += labels.size(0)\n",
        "#     print('label',labels)\n",
        "#     print('predicted',predicted)\n",
        "#     correct += (predicted == labels).sum().item()\n",
        "#     print(correct / total)\n",
        "#   val_accuracy = correct / total\n",
        "#   print(f\"Validation accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch \n",
        "# path warning!! Colab dir are different\n",
        "prog_tensors_validation = torch.load('./test-tensors/progressive_rock_songs_test_tensor.pt')\n",
        "non_prog_tensors_validation = torch.load('./test-tensors/non_progressive_rock_songs_test_tensor.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model state dictionary keys:\n",
            "conv1.weight\n",
            "conv1.bias\n",
            "batchnorm1.weight\n",
            "batchnorm1.bias\n",
            "batchnorm1.running_mean\n",
            "batchnorm1.running_var\n",
            "batchnorm1.num_batches_tracked\n",
            "conv2.weight\n",
            "conv2.bias\n",
            "batchnorm2.weight\n",
            "batchnorm2.bias\n",
            "batchnorm2.running_mean\n",
            "batchnorm2.running_var\n",
            "batchnorm2.num_batches_tracked\n",
            "conv3.weight\n",
            "conv3.bias\n",
            "batchnorm3.weight\n",
            "batchnorm3.bias\n",
            "batchnorm3.running_mean\n",
            "batchnorm3.running_var\n",
            "batchnorm3.num_batches_tracked\n",
            "conv4.weight\n",
            "conv4.bias\n",
            "batchnorm4.weight\n",
            "batchnorm4.bias\n",
            "batchnorm4.running_mean\n",
            "batchnorm4.running_var\n",
            "batchnorm4.num_batches_tracked\n",
            "fc1.weight\n",
            "fc1.bias\n",
            "fc2.weight\n",
            "fc2.bias\n",
            "fc3.weight\n",
            "fc3.bias\n",
            "\n",
            "Model layer details:\n",
            "Layer: conv1.weight\n",
            " Shape: torch.Size([320, 160, 3])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: conv1.bias\n",
            " Shape: torch.Size([320])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm1.weight\n",
            " Shape: torch.Size([320])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm1.bias\n",
            " Shape: torch.Size([320])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm1.running_mean\n",
            " Shape: torch.Size([320])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm1.running_var\n",
            " Shape: torch.Size([320])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm1.num_batches_tracked\n",
            " Shape: torch.Size([])\n",
            " Data type: torch.int64\n",
            " Device: cpu\n",
            "Layer: conv2.weight\n",
            " Shape: torch.Size([640, 320, 5])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: conv2.bias\n",
            " Shape: torch.Size([640])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm2.weight\n",
            " Shape: torch.Size([640])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm2.bias\n",
            " Shape: torch.Size([640])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm2.running_mean\n",
            " Shape: torch.Size([640])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm2.running_var\n",
            " Shape: torch.Size([640])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm2.num_batches_tracked\n",
            " Shape: torch.Size([])\n",
            " Data type: torch.int64\n",
            " Device: cpu\n",
            "Layer: conv3.weight\n",
            " Shape: torch.Size([256, 640, 5])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: conv3.bias\n",
            " Shape: torch.Size([256])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm3.weight\n",
            " Shape: torch.Size([256])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm3.bias\n",
            " Shape: torch.Size([256])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm3.running_mean\n",
            " Shape: torch.Size([256])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm3.running_var\n",
            " Shape: torch.Size([256])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm3.num_batches_tracked\n",
            " Shape: torch.Size([])\n",
            " Data type: torch.int64\n",
            " Device: cpu\n",
            "Layer: conv4.weight\n",
            " Shape: torch.Size([128, 256, 3])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: conv4.bias\n",
            " Shape: torch.Size([128])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm4.weight\n",
            " Shape: torch.Size([128])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm4.bias\n",
            " Shape: torch.Size([128])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm4.running_mean\n",
            " Shape: torch.Size([128])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm4.running_var\n",
            " Shape: torch.Size([128])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: batchnorm4.num_batches_tracked\n",
            " Shape: torch.Size([])\n",
            " Data type: torch.int64\n",
            " Device: cpu\n",
            "Layer: fc1.weight\n",
            " Shape: torch.Size([200, 27136])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: fc1.bias\n",
            " Shape: torch.Size([200])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: fc2.weight\n",
            " Shape: torch.Size([20, 200])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: fc2.bias\n",
            " Shape: torch.Size([20])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: fc3.weight\n",
            " Shape: torch.Size([2, 20])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "Layer: fc3.bias\n",
            " Shape: torch.Size([2])\n",
            " Data type: torch.float32\n",
            " Device: cpu\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# model/model_conv1d_norm_1_large.pt\n",
        "model_path = \"./model/model_conv1d_norm_1_large.pt\"\n",
        "model_state_dict = torch.load(model_path)\n",
        "# Print model details\n",
        "print(\"Model state dictionary keys:\")\n",
        "for key in model_state_dict.keys():\n",
        "    print(key)\n",
        "print(\"\\nModel layer details:\")\n",
        "for key, value in model_state_dict.items():\n",
        "    print(f\"Layer: {key}\")\n",
        "    print(f\" Shape: {value.shape}\")\n",
        "    print(f\" Data type: {value.dtype}\")\n",
        "    print(f\" Device: {value.device}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m model_state_dict\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## For the song\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# device = get_device()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# model.to(\"cpu\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m prog_true \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
          ]
        }
      ],
      "source": [
        "model = model_state_dict\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "## For the song\n",
        "# device = get_device()\n",
        "# model.to(\"cpu\")\n",
        "prog_true = 0\n",
        "prog_false = 0\n",
        "non_prog_true = 0\n",
        "non_prog_false = 0\n",
        "# for the individual snippets\n",
        "total_prog_true = 0\n",
        "total_prog_false = 0\n",
        "total_non_prog_true = 0\n",
        "total_non_prog_false = 0\n",
        "with torch.no_grad():  # Disable gradients during validation\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for obj in prog_tensors_validation:\n",
        "    total = len(obj)\n",
        "    it = 0\n",
        "    for i in obj:\n",
        "      i = i.reshape(1,160,216)\n",
        "      i = i.to(\"cpu\")\n",
        "      outputs = model(i)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      # print(predicted)\n",
        "      if(predicted.item() == 1):\n",
        "        it = it+1\n",
        "    \n",
        "    total_prog_true += it\n",
        "    total_prog_false += (total-it)\n",
        "    if(2*it >= total):\n",
        "      prog_true +=1\n",
        "    else:\n",
        "      prog_false +=1\n",
        "  for obj in non_prog_tensors_validation:\n",
        "    total = len(obj)\n",
        "    it = 0\n",
        "    for i in obj:\n",
        "      i = i.reshape(1,160,216)\n",
        "      outputs = model(i)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      # print(predicted)\n",
        "      if(predicted.item() == 0):\n",
        "        it = it+1\n",
        "    total_non_prog_true += it\n",
        "    total_non_prog_false += (total-it)\n",
        "    if(2*it >= total):\n",
        "      non_prog_true +=1\n",
        "    else:\n",
        "      non_prog_false +=1\n",
        "\n",
        "\n",
        "print(prog_true, prog_false)\n",
        "print(non_prog_false, non_prog_true)\n",
        "\n",
        "print(total_prog_true, total_prog_false)\n",
        "print(total_non_prog_false, total_non_prog_true)\n",
        "# with torch.no_grad():  # Disable gradients during validation\n",
        "#   correct = 0\n",
        "#   total = 0\n",
        "#   for inputs, labels in val_loader:\n",
        "#     inputs.shape\n",
        "#     outputs = model(inputs)\n",
        "#     _, predicted = torch.max(outputs.data, 1)\n",
        "#     total += labels.size(0)\n",
        "#     print('label',labels)\n",
        "#     print('predicted',predicted)\n",
        "#     correct += (predicted == labels).sum().item()\n",
        "#     print(correct / total)\n",
        "#   val_accuracy = correct / total\n",
        "#   print(f\"Validation accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
